category,page,text,label
Finance,Portal:Banks,"The human back, also called the dorsum, is the large posterior area of the human body, rising from the top of the buttocks to the back of the neck. It is the surface of the body opposite from the chest and the abdomen. The vertebral column runs the length of the back and creates a central area of recession. The breadth of the back is created by the shoulders at the top and the pelvis at the bottom.
Back pain is a common medical condition, generally benign in origin.",3
Finance,Finance,"Finance is the study of money and assets. It is intertwined but not the same with economics, the study of production, distribution, and consumption of money, assets, goods and services. Finance activities take place in financial systems at various scopes, thus the field can be roughly divided into personal, corporate, and public finance. In a financial system, assets are bought, sold, and traded as financial instruments, such as currencies, loans, bonds, shares, stocks, options, futures, etc. Assets can also be banked, invested, and insured to maximize value and minimize loss. In practice, risks are always present in any financial action and entities.
Many subfields within finance are formed due to its wide scope. Asset, money, risk and investment management aim to maximize value and reduce volatility. Financial analysis is viability, stability, and profitability assessment of an action or entity. In some cases, theories in finance can be tested using the scientific method, covered by experimental finance. Some fields are multidisciplinary, such as mathematical finance, financial law, financial economics, financial engineering and financial technology. These fields are the foundation of business and accounting.
The history of finance may begin with the history of money, which is prehistoric. Ancient and medieval civilizations are known to have done basic finance functions, such as banking, trading, and accounting. Qualitative finance theories were first proposed in the 20th century, starting with Louis Bachelier's thesis. In the late 20th and early 21st century, the global financial system was formed.",3
Finance,Approved Publication Arrangement,"With MiFID II directive being in force in January 2018, Approved Publication Arrangements (APA) data should  increase transparency in the OTC markets by publishing quotes for pre-trade transparency, and trades for post-trade transparency. An APA is an organisation authorised to publish trade reports on behalf of investment firms according to Article (4)(1)(52) MiFID II.In finance, people usually use APA to refer to the data they provide, and not only the organization which provides the data.",3
Finance,Asset,"In financial accounting, an asset is any resource owned or controlled by a business or an economic entity. It is anything (tangible or intangible) that can be used to produce positive economic value. Assets represent value of ownership that can be converted into cash (although cash itself is also considered an asset).
The balance sheet of a firm records the monetary value of the assets owned by that firm. It covers money and other valuables belonging to an individual or to a business.Assets can be grouped into two major classes: tangible assets and intangible assets. Tangible assets contain various subclasses, including current assets and fixed assets. Current assets include cash, inventory, accounts receivable, while fixed assets include land, buildings and equipment.
Intangible assets are non-physical resources and rights that have a value to the firm because they give the firm an advantage in the marketplace. Intangible assets include goodwill, copyrights, trademarks, patents, computer programs, and financial assets, including financial investments, bonds, and stocks.

",3
Finance,Austerity,"Austerity is a set of political-economic policies that aim to reduce government budget deficits through spending cuts, tax increases, or a combination of both. There are three primary types of austerity measures: higher taxes to fund spending, raising taxes while cutting spending, and lower taxes and lower government spending. Austerity measures are often used by governments that find it difficult to borrow or meet their existing obligations to pay back loans. The measures are meant to reduce the budget deficit by bringing government revenues closer to expenditures. Proponents of these measures state that this reduces the amount of borrowing required and may also demonstrate a government's fiscal discipline to creditors and credit rating agencies and make borrowing easier and cheaper as a result.
In most macroeconomic models, austerity policies which reduce government spending lead to increased unemployment in the short term. These reductions in employment usually occur directly in the public sector and indirectly in the private sector. Where austerity policies are enacted using tax increases, these can reduce consumption by cutting household disposable income. Reduced government spending can reduce GDP growth in the short term as government expenditure is itself a component of GDP. In the longer term, reduced government spending can reduce GDP growth if, for example, cuts to education spending leave a country's workforce less able to do high-skilled jobs or if cuts to infrastructure investment impose greater costs on business than they saved through lower taxes. In both cases, if reduced government spending leads to reduced GDP growth, austerity may lead to a higher debt-to-GDP ratio than the alternative of the government running a higher budget deficit. In the aftermath of the Great Recession, austerity measures in many European countries were followed by rising unemployment and slower GDP growth. The result was increased debt-to-GDP ratios despite reductions in budget deficits.Theoretically in some cases, particularly when the output gap is low, austerity can have the opposite effect and stimulate economic growth. For example, when an economy is operating at or near capacity, higher short-term deficit spending (stimulus) can cause interest rates to rise, resulting in a reduction in private investment, which in turn reduces economic growth. Where there is excess capacity, the stimulus can result in an increase in employment and output. Alberto Alesina, Carlo Favero, and Francesco Giavazzi argue that austerity can be expansionary in situations where government reduction in spending is offset by greater increases in aggregate demand (private consumption, private investment, and exports).",3
Finance,Bal Krishen Rathore,"Bal Krishen Rathore is a UAE based businessman, investor and entrepreneur of Indian origin and the Chairman and CEO of Century Financial. He was listed in the top 100 inspiring leaders in the UAE by Arabian Business. He has been granted the Golden Visa by the government of UAE in 2021.

",3
Finance,Capital Markets Union,"The Capital Markets Union (CMU) is an economic policy initiative launched by the former president of the European Commission, Jean-Claude Junker in the initial exposition of his policy agenda on 15 July 2014. The main target was to create a single market for capital in the whole territory of the EU by the end of 2019. The reasoning behind the idea was to address the issue that corporate finance relies on debt (i.e. bank loans) and the fact that capitals markets in Europe were not sufficiently integrated so as to protect the EU and especially the Eurozone from future crisis. The  Five Presidents Report of June 2015 proposed the CMU in order to complement the Banking union of the European Union and eventually finish the Economic and Monetary Union (EMU) project. The CMU is supposed to attract 2000 billion dollars more on the European capital markets, on the long-term.The CMU was considered as the ""New frontier of Europe's single market"" by the Commission aiming at tackling the different problems surrounding capital markets in Europe such as: the reduction of market fragmentation, diversification of financial sources, cross-border capital flows with a special attention for Small and Medium-sized enterprises (SMEs). The project was also seen as the final step for the completion of the Economic and Monetary Union as it was complementary to the Banking union of the European Union that had been the stage for intense legislative activity since its launching in 2012. The CMU project meant centralisation and delegation of powers at the supranational level with the field of macroeconomic governance and banking supervision being the most affected.In order to address the goals and the objectives decided at the creation of the project, an Action Plan subject to a mid-term review was proposed consisting in several priority actions along with legislative proposals to harmonise rules and non-legislative proposals aiming at ensuring good practices between market operators and financial firms.The new European Commission under the leadership of Ursula von der Leyen has committed to take ahead and finalise the project started by its predecessor by working on a new long-term strategy and to address the problems the project has had in recent times following the mid-term review and the UK's exit from the EU. This is also highlighted in her bid for the presidency of the European Commission during the process of election as the main economic motto of her campaign was ""An economy that works for people"".",3
Finance,Decentralized finance,"Decentralized finance (DeFi) offers financial instruments without relying on intermediaries such as brokerages, exchanges, or banks by using smart contracts on a blockchain. DeFi platforms allow people to lend or borrow funds from others, speculate on price movements on assets using derivatives, trade cryptocurrencies, insure against risks, and earn interest in savings-like accounts. DeFi uses a layered architecture and highly composable building blocks. Some applications promote high interest rates but are subject to high risk. As of February 2022, the value of assets used in decentralized finance amounted to $200 billion.

",3
Finance,Designated Professional Body,"According to the  UK Financial Conduct Authority  (FCA), a dedicated professional body is one designated by the Treasury under section 326 of the Act (Designation of professional bodies) for the purposes of the Act (Provision of Financial Services by Members of the Professions).
The following professional bodies have been designated in the Financial Services and Markets Act 2000 (Designated Professional Bodies) Order 2001 (SI 2001/1226), the Financial Services and Markets Act 2000 (Designated Professional Bodies) (Amendment) Order 2004 (SI 2004/3352) and the Financial Services and Markets Act 2000 (Designated Professional Bodies) (Amendment) Order 2006 (SI 2006/58):

The Law Society of England & Wales;
The Law Society of Scotland;
The Law Society of Northern Ireland;
The Institute of Chartered Accountants in England and Wales;
The Institute of Chartered Accountants of Scotland;
The Institute of Chartered Accountants in Ireland;
The Association of Chartered Certified Accountants;
The Institute of Actuaries;
The Council for Licensed Conveyancers; and
The Royal Institution of Chartered Surveyors.Under Section 325(4) of the FSMA, Designated Professional Bodies are required to cooperate with the FCA in a number of ways, including information sharing, in order for the FCA to be able to perform its functions.",3
Finance,DreamAhead College Investment Plan,"DreamAhead College Investment Plan is a higher education savings program administered by the State of Washington. The plan was created in 2016 by the Washington State Legislature, and statutorily known as the Washington College Savings Plan (RCW 28B.95.032), and opened for nationwide participation in 2018. It is one of two 529 programs offered by the state, the other being the Guaranteed Education Tuition Program (known as GET), which is a prepaid program. The programs are supported by Washington College Savings Plans (WA529), a division of Washington Student Achievement Council.",3
Finance,Equivalence in financial services (European Union),"The principle of equivalence in financial services at the European Union (EU) level is one of the instruments the Commission has at its disposal to carry out its international strategy for financial services. The principle of equivalence is materialised through an equivalence decision issued by the European Commission to a targeted country that it judges fit to have access to the European Market in financial services. The decision is unilateral, non-reciprocal and affects the targeted third country in regard to particular activities or services to which the decision is intended.  The equivalence decision is issued through an assessment of the third country regulations in relation to particular services or activities in the EU. In order to do so, the Commission bases its decision on 40 provisions of EU law. Important to note, perhaps is the fact that not all have been availed but over 250 equivalence decisions were made targeting more than 30 countries worldwide.According to the European Commission, the aim of the equivalence decision is not to liberalise international trade in financial services, but a cross-border instrument to manage financial activity of market players in a way that respects standards of prudential rules as the EU does internationally.There is a recognition that equivalence has become a significant tool in the EU and one of the main powers of the Commission when it comes to financial services. In his speech, Valdis Dombrovskis, the Commissioner responsible for financial services, has highlighted the efficiency of the policy. In the occasion, he took the opportunity to talk about the new EU's comprehensive approach and legislative improvements for the granting of equivalence to third countries. In addition, it outlines the way the Commission and the European Supervisory Authorities monitor the situation in the countries to which an equivalence decision has been taken.",3
Finance,Financial Modeling World Cup,"The Financial Modeling World Cup (FMWC) is a financial modeling competition. The competition was started in September 2020. Contestants solve real-life financial problems, in the form of case studies, by building financial models in Microsoft Excel spreadsheet software. Stages held through the year contribute to a global leaderboard. The event is sponsored by financial services firm AG Capital and Microsoft.",3
Finance,Financial stability,"The Financial Stability Board (FSB) is an international body that monitors and makes recommendations about the global financial system. It was established after the G20 London summit in April 2009 as a successor to the Financial Stability Forum (FSF). The Board includes all G20 major economies, FSF members, and the European Commission. Hosted and funded by the Bank for International Settlements, the board is based in Basel, Switzerland, and is established as not-for-profit association under Swiss law.The FSB represented the G20 leaders' first major international institutional innovation. U.S. Treasury Secretary Tim Geithner has described it as ""in effect, a fourth pillar"" of the architecture of global economic governance. The FSB has been assigned a number of important tasks, working alongside the International Monetary Fund, World Bank, and the World Trade Organization.
Unlike most multilateral financial institutions, the FSB lacks a legal form and any formal power, given that its charter is an informal and nonbinding memorandum of understanding for cooperation adopted by its members.

",3
Finance,Finnfund,"Finnfund, Finnish Fund for Industrial Cooperation Ltd  (in Finnish Teollisen yhteistyön rahasto Oy) is a Finnish development financier and impact investor, that offers long-term investment loans and venture capital to private companies for projects in developing countries. Finnfund’s statutory duty is to promote economic and social development in developing countries.",3
Finance,Industrial and provident society,"An industrial and provident society (IPS) is a body corporate registered for carrying on any industries, businesses, or trades specified in or authorised by its rules.The members of a society benefit from the protection of limited liability much like other corporate forms, but unlike companies for example, each member will normally only have one vote at a General Meeting regardless of their shareholding. The governance of a society is therefore democratically oriented rather than financially oriented.
The legal form originated in the United Kingdom of Great Britain and Ireland and became the traditional legal form taken by trading organisations with democratic governance including:

co-operatives (which trade for the benefit of their members);
societies for the benefit of the community (which trade for the benefit of the broader community).In Great Britain the Co-operative and Community Benefit Societies Act 2014 has renamed these societies as co-operative or community benefit societies. 
The term industrial and provident society is still used in statute in New Zealand, the Republic of Ireland and within the UK in Northern Ireland.",3
Finance,Renu Sud Karnad,"Renu Sud Karnad is an Indian businesswoman and the managing director of India's largest private sector bank Housing Development Finance Corp. Ltd. Additionally she also holds seven other positions for companies like HDFC Property Ventures Ltd., HDFC Education & Development Services Pvt Ltd. (both are subsidiaries of HDFC Ltd.) and Non-Executive Chairman at GlaxoSmithKline Pharmaceuticals Ltd. She is also the Vice Chairman-Governing Council at Indraprastha Cancer Society and Research Centre and part of the board of 17 other companies.",3
Finance,Master of Applied Finance,"The Master of Finance is a master's degree awarded by universities or graduate schools preparing students for careers in finance. The degree is often titled Master in Finance (abbreviated M.Fin., MiF, MFin), or Master of Science in Finance (MSF in North America, and MSc in Finance in the UK and Europe).  In the U.S. and Canada the program may be positioned as a professional degree.  Particularly in Australia, the degree may be offered as a Master of Applied Finance (MAppFin). In some cases, the degree is offered as a Master of Management in Finance (MMF). More specifically focused and titled degrees are also offered.

",3
Finance,Master of Finance,"The Master of Finance is a master's degree awarded by universities or graduate schools preparing students for careers in finance. The degree is often titled Master in Finance (abbreviated M.Fin., MiF, MFin), or Master of Science in Finance (MSF in North America, and MSc in Finance in the UK and Europe).  In the U.S. and Canada the program may be positioned as a professional degree.  Particularly in Australia, the degree may be offered as a Master of Applied Finance (MAppFin). In some cases, the degree is offered as a Master of Management in Finance (MMF). More specifically focused and titled degrees are also offered.

",3
Finance,Morningstar Rating for Funds,"The Morningstar Rating for Funds, or the Star Rating, debuted in 1985, a year after Morningstar was founded. The 1- to 5-star system, ""looks at a fund's risk-adjusted return based on its performance over three, five and 10 years and on its volatility. The highest rating of five stars is bestowed on the 10 percent of funds that perform the best."" Funds need to be at least three years old to be rated.
Originally, funds were compared in four broad asset classes until the ratings methodology was revised in 2002 to rank and rate funds in 50 categories.  In 2006, the Morningstar Rating was applied to exchange-traded funds.",3
Finance,Morningstar Style Box,"The Morningstar Style Box is a grid of nine squares used to identify the investment style of stocks and mutual funds. Developed by Don Phillips and John Rekenthaler, the Style Box was launched in 1992.The vertical axis of the Style Box represents an investment's size category: small, mid and large. The horizontal axis depicts fund investment style categories such as ""value"" and ""growth,"" which are common to stocks and funds. The ""blend"" definition in the central column differs for stocks and funds. “For stocks, the central column of the Style Box will represent the core style (those for which neither value or growth characteristics dominate); for funds, it will represent the blend style (a mixture of growth and value stocks or mostly core stocks).”",3
Finance,Non-financial asset,"A financial asset is a non-physical asset whose value is derived from a contractual claim, such as bank deposits, bonds, and participations in companies' share capital. Financial assets are usually more liquid than other tangible assets, such as commodities or real estate.The opposite of financial assets is non-financial assets, which include both tangible property (sometimes also called real assets) such as land, real estate or commodities, and intangible assets such as intellectual property, including copyrights, patents, trademarks and data.",3
Finance,PERC Reporting Standard,"The PERC Standard for Reporting of Exploration Results, Mineral Resources and Mineral Reserves (the ‘PERC Reporting Standard’) sets out the minimum standards, as well as additional guidelines and recommendations for the Public Reporting of Exploration Results, Mineral Resources and Mineral Reserves within Europe.
It applies to all solid mineral raw materials for which Public Reporting of Exploration Results, Mineral Resources and Mineral Reserves is required by any relevant regulatory authority.
The PERC Reporting Standard is fully aligned with the CRIRSCO International Reporting Template.The CRIRSCO International Reporting Template is a document that represents the best of the CRIRSCO-style codes for the public reporting of Exploration Results, Mineral Resources and Mineral Reserves that are recognised and adopted world-wide for market-related reporting and financial investment.",3
Finance,Private Market Assets,Private market assets refer to investments in equity (shares) and debt issued by privately owned (non listed) companies – as opposed to ‘public’ (listed) corporations. These markets include private equity (PE) and venture capital (VC); real estate (property); infrastructure; farmland and forestry.,3
Finance,Public commercial assets,"Public Commercial Assets are the assets owned by the public sector able to generate income if managed professionally.Public Commercial Assets are a sub-sector of the asset side of the Public Sector Balance Sheet, that reports the totals of assets and liabilities that the government controls.
According to IMF research, total public sector assets have a value equivalent to 2×GDP globally. Net worth (assets minus liabilities) would be equivalent to some 21% of GDP.Real estate is the single largest segment of all assets, globally. According to research from McKinsey Global Institute, Global net worth has risen as interest rates have fallen, since 2000 mainly due to the prices of real estate triple in value between 2000 to 2020. Most governments do not keep a complete record of all the real estate it owns, thus making it difficult to value, manage or develop and put these assets to their most productive uses.

",3
Finance,Public Financial Management,"Public Financial Management (PFM), provides a framework for parliamentary authorisation and scrutiny of the Government's expenditure proposals and the Government's management of its assets and liabilities, including:
establish lines of responsibility for effective and efficient management and use of public financial resources
specify the principles for responsible fiscal management in the conduct of fiscal policy and require regular reporting on the extent to which the Government's fiscal policy is consistent with those principles
specify the minimum financial and non-financial reporting obligations of Ministers, departments, Offices of Parliament and certain other agencies
provide for the application of financial management incentives and for the accountability of specified central government organisations
safeguard public assets by providing statutory authority and control for the borrowing of money, issuing of securities, use of derivative transactions, investment of funds, operation of bank accounts and giving of guarantees and indemnitiesPublic sector performance management approach emphasises, among other things, clear objectives and clear lines of responsibility, greater freedom to manage, and a corresponding expectation of greater accountability for results. Such a system requires good measures of performance that interested external parties can trust. This requires that public sector entities to prepare financial information that:
uses accrual accounting concepts and statements
is in accordance with financial reporting standards approved by an independent standard setter, and
in the case of annual financial statements, complies with generally accepted accounting practice (IPSASB) and is audited by an independent auditor.",3
Finance,Public sector balance sheet,"A Public Sector Balance Sheet, like a balance sheet in the corporate world, reports comprehensively on what a government owns and owes, as well as its own capital. As such, it is a critical element of a system of Public Financial Management. A balance sheet, or statement of financial position, recognises and discloses the assets, liabilities, and net worth at a given point in time, for a government entity, a government or the whole public sector. An important metric for the fiscal position of the whole public sector is public sector net worth.
For a government at any level, local, regional or national, the balance sheet offers greater fiscal transparency, being more comprehensive than the conventional metrics of debt and deficits. 
The quality of the financial statements depends on the quality of data used and what basis of accounting is used to compose the balance sheet and the other financial statements.
Willem Buiter and the IMF argued in 1983 for the use of public sector balance sheets to improve public financial management.Following a financial crisis, the New Zealand government passed its Public Finance Act (PFA) in 1989, introducing accrual budgeting, appropriations and accounting, publishing the world's first public sector balance sheet based on audited accounting records rather than statistical estimates.The IMF shifted its Government Finance Statistics Manual from a cash to an accrual basis in 2001. It is more recently emphasising the importance of government balance sheets on a global basis, albeit using statistical rather than accounting data.IFAC and CIPFA predict that in 2025, almost half the world's governments will adopt accrual-based accounting.  Far fewer, however, will be using accrual information at the heart of their financial management and budget systems. For example, the UK's Whole of Government Accounts, which reports its public sector real estate assets, does not have a mandate to assign a fair market value to the assets, and its financial management framework pays very little attention to net worth creation.The use of proper public sector balance sheets rests upon the use of accrual accounting throughout the public financial management system, making it integral to financial and budgetary decision-making.
Level 1
At level 1, governments focus their financial decision-making and reporting almost entirely on cash flows and levels of debt, though information on debt is usually constrained to the nominal value of the debt. This results in assets, especially non-financial assets, being poorly managed, along with insurance obligations.
Level 2
At level 2, budgeting and appropriations focus on cash flows, while ex post reporting covers the full spectrum of assets and liabilities. Because decision-making within the budgeting system does not utilize accrual information, it is sub-optimal. While transparency is improved with accrual reporting, the failure to use the information for management purposes reduces both its value and the incentives to produce it in a timely manner.
Level 3
At level 3, financial decision-making utilizes a more comprehensive set of information and should result in greater attention being placed on asset utilization and management, and on the incurrence and management of non-debt liabilities. However, because the appropriations, which are legally binding, are cash measures, there remain strong incentives to focus on cash flows in management decision-making and control.
Level 4
At level 4, the financial system is fully based on a comprehensive set of information covering all assets and liabilities, revenues, and expenses. This enables better informed decision-making that reflects all resources and flows, including cash flows. The measures, signals and incentives generated by the system are consistent in encouraging optimal use of all resources.",3
Finance,Public sector net worth,"The change in public sector net worth in any given forecast year is largely driven by the operating balance and property, plant and equipment revaluations.Research suggests that the main fiscal factor driving bond yields hence appears to be government net worth.Focusing on net worth as the most comprehensive measure of fiscal position incentivizes the public sector to invest the proceeds of borrowing in productive investments rather than use debt to finance consumption spending. Net worth also provides a tool for assessing whether government policy is fair to future generations from a financial point of view; negative, or declining, net worth indicates that past or present consumption will need to be funded by future taxation.

",3
Finance,Public Wealth Fund,"A sovereign wealth fund (SWF), sovereign investment fund, or social wealth fund is a state-owned investment fund that invests in real and financial assets such as stocks, bonds, real estate, precious metals, or in alternative investments such as private equity fund or hedge funds. Sovereign wealth funds invest globally. Most SWFs are funded by revenues from commodity exports or from foreign-exchange reserves held by the central bank.  
Some sovereign wealth funds may be held by a central bank, which accumulates the funds in the course of its management of a nation's banking system; this type of fund is usually of major economic and fiscal importance.  Other sovereign wealth funds are simply the state savings that are invested by various entities for the purposes of investment return, and that may not have a significant role in fiscal management.
The accumulated funds may have their origin in, or may represent, foreign currency deposits, gold, special drawing rights (SDRs) and International Monetary Fund (IMF) reserve positions held by central banks and monetary authorities, along with other national assets such as pension investments, oil funds, or other industrial and financial holdings. These are assets of the sovereign nations that are typically held in domestic and different reserve currencies (such as the dollar, euro, pound, and yen). Such investment management entities may be set up as official investment companies, state pension funds, or sovereign funds, among others.
There have been attempts to distinguish funds held by sovereign entities from foreign-exchange reserves held by central banks. Sovereign wealth funds can be characterized as maximizing long-term return, with foreign exchange reserves serving short-term ""currency stabilization"", and liquidity management. Many central banks in recent years possess reserves massively in excess of needs for liquidity or foreign exchange management.  Moreover, it is widely believed most have diversified hugely into assets other than short-term, highly liquid monetary ones, though almost no data is publicly available to back up this assertion.

",3
Finance,Real bills doctrine,"The real bills doctrine says that as long as bankers lend to businessmen only against the security (collateral) of short-term 30-, 60-, or 90-day commercial paper representing claims to real goods in the process of production, the loans will be just sufficient to finance the production of goods. The doctrine seeks to have real output determine its own means of purchase without affecting prices. Under the real bills doctrine, there is only one policy role for the central bank: lending commercial banks the necessary reserves against real customer bills, which the banks offer as collateral. The term ""real bills doctrine"" was coined by Lloyd Mints in his 1945 book, A History of Banking Theory. The doctrine was previously known as ""the commercial loan theory of banking"".
Moreover, as bank loans are granted to businessmen in the form either of new bank notes or of additions to their checking deposits, which deposits constitute the main component of the money stock, the doctrine assures that the volume of money created will be just enough to allow purchasers to buy the finished goods off the market as final product without affecting prices. From their sales receipts, businessmen then pay off their real bills bank loans. Banks retire the returned money from circulation until the next batch of goods need financing.
The doctrine has roots in some statements of Adam Smith. John Law (1671–1729) in his Money and Trade Considered: With a Proposal for Supplying a Nation with Money (1705) originated the basic idea of the real bills doctrine, the concept of an ""output-governed currency secured to real property and responding to the needs of trade"". Law sought to limit monetary expansion and maintain price stability, by using land as a measure of, and collateral for, real activity. Smith then substituted short-term self-liquidating commercial paper for Law's production proxy, land, and so the real bills doctrine was born.
The British banker, parliamentarian, philanthropist, anti-slavery activist, and monetary theorist Henry Thornton (1760–1815) was an early critic of the real bills doctrine. He noted one of the doctrine's three main flaws, namely that by linking money not to real output as the original intention was, but to the price times quantity—or nominal dollar value—of real output, it set up a positive feedback loop running from price to money to price. When the monetary authority holds the market (loan) rate of interest, below the profit rate on capital, this feedback loop can generate continuing inflation.Doctrinal historians have noted the real bills doctrine's place as one factor contributing to the instability of the U.S. money supply precipitating the Great Depression. Adhering to the doctrine's second flaw, namely that speculative activity/paper can be sharply distinguished from purely productive activity/paper (as if production motivated by uncertain expected future profits does not involve a speculative element), long-time Fed Board member Adolph C. Miller in 1929 launched his Direct Pressure initiative. It required all member banks seeking Federal Reserve discount window assistance to affirm that they had never made speculative loans, especially of the stock-market variety. No self-respecting banker seeking to borrow emergency reserves from the Fed was willing to undergo such interrogation, especially given that a ""hard-boiled"" Federal Reserve was unlikely to grant such aid. Instead, the banks chose to fail (and the Federal Reserve let them), which they did in large numbers, almost 9000 of them. These failures led to the 1⁄3 contraction of the money stock, which, according to Friedman and Schwartz, caused the Great Depression. The result was a decade-long fall of real output and prices, which by the needs-of-trade logic of the real bills doctrine justified shrinkage of the money stock.
Here was the doctrine's third flaw. It calls for pro-cyclical contractions and expansions of the money stock when correct stabilization policy calls for counter-cyclical ones. The doctrine fell into disuse in the late 1930s, but its legacy still influences banking policy from time to time.

",3
Finance,Renting,"Renting, also known as hiring or letting, is an agreement where a payment is made for the temporary use of a good, service or property owned by another. A gross lease is when the tenant pays a flat rental amount and the landlord pays for all property charges regularly incurred by the ownership. An example of renting is equipment rental. Renting can be an example of the sharing economy.",3
Finance,Request for quote,"A Request for Quote (RfQ) is a financial term for certain way to ask a bank for an offer of a given financial instrument from a bank, made available by so-called Approved Publication Arrangement (APA) by the stock markets itself or by Financial data vendors as required in Europe by MiFID II and in effect since January 2018. A RFQ contains at least the ISIN to uniquely identify the financial product, the type (buy/ sell), the amount, a currency, and the volume (
  
    
      
        
          
            amount
          
        
        ×
        
          
            market price
          
        
      
    
    {\displaystyle {\hbox{amount}}\times {\hbox{market price}}}
   in given currency).",3
Finance,Shadow banking in China,"Chinese shadow banking refers to underground financial activity that takes place outside of traditional banking regulations and systems. China has one of the largest shadow banking industries with approximately 40% of the country's outstanding loans tied up in shadow banking activities. Shadow banking in China arose after the People's Bank of China became the central bank in 1983. This encouraged commercial enterprises and private investors to place more of their money in financial products, causing the banking industry to grow.",3
Finance,Trade exchange,"A local exchange trading system (also local employment and trading system or local energy transfer system; abbreviated LETS) is a locally initiated, democratically organised, not-for-profit community enterprise that provides a community information service and records transactions of members exchanging goods and services by using locally created currency. LETS allow people to negotiate the value of their own hours or services, and to keep wealth in the locality where it is created.Similar trading systems around the world are also known as Community Exchange Systems, Mutual Credit trading systems, Clearing Circles, Trade Exchanges or Time Banks. These all use 'metric currencies' – currencies that measure, as opposed to the fiat currencies used in conventional value exchange. These are all a type of alternative or complementary currency.In the 21st century, the internet-based networks haves been used to link individual LETS systems, into national or global networks.",3
Finance,Tredefina,"Treuhandverwaltung für das Deutsch-Niederländische Finanzabkommen GmbH (Tredefina) was a public law institution originally formed in 1920 by the Weimar Republic which during the Nazi occupation of Europe was used by CEO Alexander Kreuter to purchase Aryanized companies. Originally created to administer a revolving loan from the Netherlands of 140 million Dutch guilders, Tredefina remained active until the early 21st century.",3
Finance,Valuation (finance),"In finance, valuation is the process of determining the present value (PV) of an asset. In a business context, it is often the hypothetical price that a third party would pay for a given asset. Valuations can be done on assets (for example, investments in marketable securities such as companies' shares and related rights, business enterprises, or intangible assets such as patents, data and trademarks) or on liabilities (e.g., bonds issued by a company). Valuations are needed for many reasons such as investment analysis, capital budgeting, merger and acquisition transactions, financial reporting, taxable events to determine the proper tax liability.",3
Finance,Accretion/dilution analysis,"Accretion/dilution analysis is a type of M&A financial modelling performed in the pre-deal phase to evaluate the effect of the transaction on shareholder value and to check whether EPS for buying shareholders will increase or decrease post-deal. Generally, shareholders do not prefer dilutive transactions; however, if the deal may generate enough value to become 
 accretive in a reasonable time, a proposed combination is justified. 
Aside is a simplified example. A real-life accretion/dilution analysis may be much more complex if the deal is structured as cash-and-stock-for-stock, if preferred shares and dilutive instruments are involved, if debt and transaction fees are substantial, and so on. Generally, if the buying company has a higher P/E multiple than that of the target, the deal is likely to be accretive. The reverse is true for a dilutive transaction.

",3
Finance,Adjusted present value,"Adjusted present value (APV) is a valuation method introduced in 1974 by Stewart Myers. The idea is to value the project as if it were all equity financed (""unleveraged""), and to then add the present value of the tax shield of debt – and other side effects.Technically, an APV valuation model looks similar to a standard DCF model. However, instead of WACC, cash flows would be discounted at the unlevered cost of equity, and tax shields at either the cost of debt (Myers) or following later academics also with the unlevered cost of equity. APV and the standard DCF approaches should give the identical result if the capital structure remains stable.According to Myers, the value of the levered firm (Value levered, Vl) is equal to the value of the firm with no debt (Value unlevered, Vu) plus the present value of the tax savings due to the tax deductibility of interest payments, the so-called value of the tax shield (VTS). Myers proposes calculating the VTS by discounting the tax savings at the cost of debt (Kd). The argument is that the risk of the tax saving arising from the use of debt is the same as the risk of the debt.The method is to calculate the NPV of the project as if it is all-equity financed (so called ""base case""). Then the base-case NPV is adjusted for the benefits of financing. Usually, the main benefit is a tax shield resulted from tax deductibility of interest payments. Another benefit can be a subsidized borrowing at sub-market rates. The APV method is especially effective when a leveraged buyout case is considered since the company is loaded with an extreme amount of debt, so the tax shield is substantial.

",3
Finance,The Appraisal Foundation,"The Appraisal Foundation (TAF) is the United States organization responsible for setting standards for the real estate valuation profession.  The organization sets the congressionally-authorized standards and qualifications for real estate appraisers, and provides voluntary guidance on recognized valuation methods and techniques for all valuation professionals. The aim is to ensure appraisals are impartial, objective and independent, are conducted without bias and are performed in an ethical and competent manner.
The Appraisal Foundation is a non-profit organization established in 1987 by the largest valuation organizations in North America.  The Foundation was congressionally-authorized to develop standards and qualifications for real estate appraisers under Title XI of the Financial Institutions Reform, Recovery, and Enforcement Act of 1989. Under this legislation, the Foundation is overseen by the Appraisal Subcommittee (ASC), a subcommittee of the Federal Financial Institutions Examination Council (FFIEC). The ASC is charged with filing a report to Congress each year, on the activities of TAF.
Real Estate Appraisal practice in the US is regulated by the various states and territories. TAF's Appraisal Standards Board (ASB) promulgates and updates best practices as codified in the Uniform Standards of Professional Appraisal Practice (USPAP), and TAF's Appraisal Qualifications Board (AQB) promulgates minimum criteria for appraiser certification and licensing. The federal government does not regulate appraisers directly, the states are responsible for regulating the appraisal profession.  The Appraisal Subcommittee (ASC), with members from the major Federal Lending Regulators, monitors and reviews the activities of TAF and oversees the state licensing agencies.  If ASC finds that a particular state's appraiser regulation and certification program is inadequate (i.e., it doesn't meet ASB and AQB standards), it can impose penalties including prohibition of a state's licensed appraisers from performing appraisals for federally related lending transactions. Banks make widespread use of appraisals in their real estate lending activity mortgage loans and mortgage-backed securities.",3
Finance,Appraisal Institute,"The Appraisal Institute (AI), headquartered in Chicago, Illinois, is an international association of professional real estate appraisers. It was founded in January 1991 when the American Institute of Real Estate Appraisers (AIREA) and the Society of Residential Appraisers merged. The AIREA and the Society were respectively founded in 1932 and 1935. Real estate appraisal emerged as a profession at this point in response to the crash of home values as a result of the Great Depression, building on the intellectual frameworks developed over the course of the 1920s by land value theorists like Ernest McKinley Fisher, Frederick Babcock, Homer Hoyt, and Richard T. Ely. As of February 2007, the Appraisal Institute has more than 21,000 members and 99 chapters throughout the United States, Canada, and overseas.
The group publishes the Appraisal Journal.",3
Finance,Asset management,"Asset management is a systematic approach to the governance and realization of value from the things that a group or entity is responsible for, over their whole life cycles. It may apply both to tangible assets (physical objects such as buildings or equipment) and to intangible assets (such as human capital, intellectual property, goodwill or financial assets). Asset management is a systematic process of developing, operating, maintaining, upgrading, and disposing of assets in the most cost-effective manner (including all costs, risks, and performance attributes).
The term is commonly used in the financial sector to describe people and companies who manage investments on behalf of others. Those include, for example, investment managers that manage the assets of a pension fund.
It is also increasingly used in both the business world and public infrastructure sectors to ensure a coordinated approach to the optimization of costs, risks, service/performance, and sustainability.
The International Standard, ISO 55000, provides an introduction and requirements specification for a management system for asset management.",3
Finance,Beneish M-score,"The Beneish model is a statistical model that uses financial ratios calculated with accounting data of a specific company in order to check if it is likely (high probability) that the reported earnings of the company have been manipulated.

",3
Finance,Benjamin Graham formula,"The  Benjamin Graham formula is a formula proposed by investor and professor of Columbia University, Benjamin Graham, often referred to as the ""father of value investing"". Published in his book, The Intelligent Investor, Graham devised the formula for lay investors to help them with the valuation of growth stocks in vogue at the time of the formula's publication.
He did however caution that the use of this equation was not appropriate for companies ""below-par"" debt position: ""My advice to analysts would be to limit your appraisals to enterprises of investment quality, excluding from that category such as do not meet specific criteria of financial strength"".",3
Finance,Book value,"In accounting, book value  is the value of an asset according to its balance sheet account balance. For assets, the value is based on the original cost of the asset less any depreciation, amortization or impairment costs made against the asset. Traditionally, a company's book value is its total assets minus intangible assets and liabilities. However, in practice, depending on the source of the calculation, book value may variably include goodwill, intangible assets, or both. The value inherent in its workforce, part of the intellectual capital of a company, is always ignored.  When intangible assets and goodwill are explicitly excluded, the metric is often specified to be ""tangible book value"".
In the United Kingdom, the term net asset value may refer to the book value of a company.",3
Finance,Buffett indicator,"The Buffett indicator (or the Buffett metric, or the Market capitalization-to-GDP ratio) is a valuation multiple used to assess how expensive or cheap the aggregate stock market is at a given point in time.  It was proposed as a metric by investor Warren Buffett in 2001, who called it ""probably the best single measure of where valuations stand at any given moment"", and its modern form compares the capitalization of the US Wilshire 5000 index to US GDP.  It is widely followed by the financial media as a valuation measure for the US market in both its absolute, and de-trended forms.The indicator set an all-time high during the so-called ""everything bubble"", crossing the 200% level in February 2021; a level that Buffett warned if crossed, was ""playing with fire"".",3
Finance,Business valuation,"Business valuation is a process and a set of procedures used to estimate the economic value of an owner's interest in a business.  Here various valuation techniques are used by financial market participants to determine the price they are willing to pay or receive to effect a sale of the business.
In addition to estimating the selling price of a business, the same valuation tools are often used by business appraisers to resolve disputes related to estate and gift taxation, divorce litigation, allocate business purchase price among business assets, establish a formula for estimating the value of partners' ownership interest for buy-sell agreements, and many other business and legal purposes such as in shareholders deadlock, divorce litigation and estate contest.Specialized business valuation credentials include the Chartered Business Valuator (CBV) offered by the CBV Institute, ASA and CEIV from the American Society of Appraisers, and the CVA by the National Association of Certified Valuators and Analysts. 
In some cases, the court would appoint a forensic accountant as the joint-expert doing the business valuation.  Here, attorneys should always be prepared to have their expert's report withstand the scrutiny of cross-examination and criticism. Business valuation is distinct from stock valuation, which is about calculating theoretical values of listed companies and their stocks, for the purposes of share trading and investment management. 
This distinction extends to the use of the results: stock investors intend to profit from price movement, whereas a business owner is focused on the enterprise as a total, going concern.
A further distinction is re corporate finance: 
the transactions here are generally handled by a business broker; 
whereas where two corporates are involved, the transaction and valuation is within the realm of ""mergers and acquisitions"", and is handled by an investment bank; see Mergers and acquisitions § Business valuation and Corporate finance § Investment and project valuation.",3
Finance,Business valuation standard,"Business Valuation Standards (BVS) are codes of practice that are used in business valuation.  The major business appraisal standards are these:

CICBV Practice Standards. Published by the CBV Institute.
Uniform Standards of Professional Appraisal Practice (USPAP). Standards 9 and 10 cover business valuation and reporting standards. Published by the Appraisal Foundation.
International Valuation Standards. Published by the International Valuation Standards Council.
Statement on Standards for Valuation Services (SSVS No 1). Published by the American Institute of CPAs.In addition, each of the three major United States valuation societies — the American Society of Appraisers (ASA), American Institute of Certified Public Accountants (CPA/ABV), and the National Association of Certified Valuation Analysts (NACVA) — has its own set of Business Valuation Guidelines, which it requires all of its accredited members to adhere to.  The AICPA's standards are published as Statement on Standards for Valuation Services No.1 and the ASA's guidelines are published as the ASA Business Valuation Guidelines, which largely follow the USPAP Standard requirements.  All AICPA members are required to follow SSVS1.  Additionally, the majority of the State Accountancy Boards have adopted SSVS1 for CPAs licensed in their state.",3
Finance,Cann v Willson,"Cann v Willson (1888) 39 Ch D 39, is an English tort law case, concerning negligent valuation.",3
Finance,Capital appreciation,"Capital appreciation is an increase in the price or value of assets. It may refer to appreciation of company stocks or bonds held by an investor, an increase in land valuation,  or other upward revaluation of fixed assets.
Capital appreciation may occur passively and gradually, without the investor taking any action. It is distinguished from a capital gain which is the profit achieved by selling an asset. Capital appreciation may or may not be shown in financial statements; if it is shown, by revaluation of the asset, the increase is said to be ""recognized"". Once the asset is sold, the appreciation since the date of initially buying the asset becomes a ""realized"" gain.
When the term is used in reference to stock valuation, capital appreciation is the goal of an investor seeking long term growth. It is growth in the principal amount invested, but not necessarily an increase in the current income from the asset.
In the context of investment in a mutual fund, capital appreciation refers to a rise in the value of the securities in a portfolio which contributes to the growth in net asset value. A capital appreciation fund is a fund for which it is its primary goal, and accordingly invests in growth stocks.

",3
Finance,Cash value added,"Cash value added (CVA) is a measure of business profitability defined as the EBITDA generated by the business, less tax, less its required return. 
The required return is an annuity based on the purchase price of the assets in use in the business, inflated to today's value of money, the weighted average cost of capital (WACC) and the economic life of the assets.
CVA can also be expressed as an index, where the CVA is divided by the required return. An index of more than 1.0 will indicate profitability while an index below 1.0 will indicate value destruction.",3
Finance,Channel check,"In financial analysis, a channel check is third-party research on a company's business based on collecting information from the distribution channels of the company. It may be conducted in order to value the company, to perform due diligence in various contexts, and the like. Industries where channel checks are more often conducted include retail, technology, commodities, etc.
It is a practice performed by third party researchers and financial analysts in order to collect information about a company's business. The Channel Check process includes interviewing people within other organizations connected to the company's supply and distribution channels. These interviews usually occur without the target company's knowledge. For example, a channel check could include one or multiple conversations with a store manager to understand their targeted customer. Analysts generally look for top products, customer buying patterns and past performance.
Analysts could also contact one or several suppliers or vendors to obtain information about the targeted company. In these interviews analysts are looking for quantity of materials being demanded and prices. Suppliers could also help Analysts to see the “Bigger Picture” of a company's production plans, new products and more. Suppliers may also give an indication of the raw material availability, finished product inventory levels, promotion plans to the Analysts.
Channel checks can give insights complementary to balance sheet analysis, such as distributor and retailer attitudes towards a product and its competitors, seasonal and geographic variation, inventory levels (notably channel stuffing), and so on.",3
Finance,Chepakovich valuation model,"The Chepakovich valuation model is a specialized discounted cash flow valuation model, originally designed for the valuation of “growth stocks” (ordinary/common shares of companies experiencing high revenue growth rates), and subsequently applied to the valuation of high-tech companies - even those that are (currently) unprofitable. 
Relatedly, it is a general valuation model and can also be applied to no-growth or negative growth companies. 
In fact, in the limiting case of no growth in revenues, the model yields similar (but not identical) results to a regular discounted cash flow to equity model. 
The model was developed by Alexander Chepakovich in 2000 and enhanced in subsequent years.",3
Finance,Clean surplus accounting,"The clean surplus accounting method provides elements of a forecasting model that yields price as a function of earnings, expected returns, and change in book value.

The theory's primary use is to estimate the value of a company’s shares (instead of discounted dividend/cash flow approaches). The secondary use is to estimate the cost of capital, as an alternative to e.g. the CAPM. 
The ""clean surplus"" is calculated by not including transactions with shareholders (such as dividends, share repurchases or share offerings) when calculating returns; whereas standard accounting for financial statements requires that the change in book value equal earnings minus dividends (net of capital changes).",3
Finance,Contingent claim valuation,"In finance, a contingent claim is a derivative whose future payoff depends on the value of another “underlying” asset, or more generally, that is dependent on the realization of some uncertain future event. 
These are so named, since there is only a payoff under certain contingencies. 
Any derivative instrument that is not a contingent claim is called a forward commitment.The prototypical contingent claim is an option, the right to buy or sell the underlying asset at a specified exercise price by a certain expiration date;  whereas (vanilla) swaps, forwards, and futures are forward commitments, since these grant no such optionality.Contingent claims are applied under financial economics in developing models and theory, and in corporate finance as a valuation framework.
This approach originates with Robert C. Merton,
  
decomposing the value of a corporate into a set of options in his ""Merton model"" of credit risk.

",3
Finance,Contingent value rights,"In corporate finance, 
Contingent Value Rights (CVR) are rights granted by an acquirer to a company’s shareholders,  facilitating the transaction where some uncertainty is inherent.
CVRs may be separately tradeable  securities; they are occasionally acquired (or shorted) by specialized hedge funds.",3
Finance,Convention of conservatism,"In  accounting, the convention of conservatism, also known as the doctrine of prudence, is a policy of anticipating possible future losses but not future gains. This policy tends to understate rather than overstate net assets and net income, and therefore lead companies to ""play safe"". When given a choice between several outcomes where the probabilities of occurrence are equally likely, you should recognize that transaction resulting in the lower amount of profit, or at least the deferral of a profit.In accounting, it states that when choosing between two solutions, the one that will be least likely to overstate assets and income should be selected. Essentially, ""expected losses are losses but expected gains are not gains"".
The conservatism principle is the foundation for the lower of cost or market rule, which states that you should record inventory at the lower of either its acquisition cost or its current market value.
Conservatism plays an important role in a number of accounting rules, including the allowance for doubtful debts and the lower of cost or market rule.",3
Finance,Corporate Finance Institute,"Corporate Finance Institute (CFI) is an online training and education platform for finance and investment professionals, providing courses and certifications in financial modeling, valuation, and other corporate finance topics. These are the skills CFI deems important for modern finance - such as Microsoft Excel, presentation and visuals - as well as underlying knowledge of accounting and business strategy. 
CFI is a provider of certification programs including Financial Modeling & Valuation Analyst (FMVA), Commercial Banking & Credit Analyst (CBCA), Capital Markets and Securities Analyst (CMSA), and Business Intelligence and Data Analyst (BIDA). All CFI courses are delivered online. 
The organization was incorporated as a joint venture with MDA Training (MDA), a London-based financial training company founded in 1988 by Professor Walter Reid, who was one of the founding professors of the London Business School (LBS).In 2021, Corporate Finance Institute acquired Macabacus, a Microsoft Office Add-In for finance professionals, with tens of thousands of monthly and annual subscribers.",3
Finance,Customer equity,"Customer equity is the total combined customer lifetime values of all of the company’s customers. 

",3
Finance,Cyclically adjusted price-to-earnings ratio,"The price-earnings ratio, also known as P/E ratio, P/E, or PER, is the ratio of a company's share (stock) price to the company's earnings per share. The ratio is used for valuing companies and to find out whether they are overvalued or undervalued. 

  
    
      
        P
        
          /
        
        E
        =
        
          
            Share Price
            Earnings per Share
          
        
      
    
    {\displaystyle P/E={\frac {\text{Share Price}}{\text{Earnings per Share}}}}
  As an example, if share A is trading at $24 and the earnings per share for the most recent 12-month period is $3, then share A has a P/E ratio of $24/($3 per year) = 8. Put another way, the purchaser of the share is investing $8 for every dollar of annual earnings; or, if earnings stayed constant it would take 8 years to recoup the share price. Companies with losses (negative earnings) or no profit have an undefined P/E ratio (usually shown as ""not applicable"" or ""N/A""); sometimes, however, a negative P/E ratio may be shown.",3
Finance,Depreciation (economics),"In economics, depreciation is the gradual decrease in the economic value of the capital stock of a firm, nation or other entity, either through physical depreciation, obsolescence or changes in the demand for the services of the capital in question. If the capital stock is 
  
    
      
        
          K
          
            t
          
        
      
    
    {\displaystyle K_{t}}
   in one period 
  
    
      
        t
      
    
    {\displaystyle t}
  , gross (total)  investment spending on newly produced capital is 
  
    
      
        
          I
          
            t
          
        
      
    
    {\displaystyle I_{t}}
   and depreciation is 
  
    
      
        
          D
          
            t
          
        
      
    
    {\displaystyle D_{t}}
  , the capital stock in the next period, 
  
    
      
        
          K
          
            t
            +
            1
          
        
      
    
    {\displaystyle K_{t+1}}
  , is 
  
    
      
        
          K
          
            t
          
        
        +
        
          I
          
            t
          
        
        −
        
          D
          
            t
          
        
      
    
    {\displaystyle K_{t}+I_{t}-D_{t}}
  . The net increment to the capital stock is the difference between gross investment and depreciation, and is called net investment.",3
Finance,Deprival value,"Deprival value 
is a concept used in accounting theory to determine the appropriate measurement basis for assets.  It is an alternative to historical cost and fair value or mark to market accounting.  Some writers prefer terms such as 'value to the owner' or 'value to the firm'.  Deprival value is also sometimes advocated for liabilities, in which case another term such as 'Relief value' may be used.
The deprival value of an asset is the extent to which the entity is ""better off"" because it holds the asset. This may be thought of as the answer to the following questions, all of which are equivalent: - What amount would just compensate the entity for the loss of the asset? - What loss would the entity sustain if deprived of the asset? - How much would the entity rationally pay to acquire the asset (if it did not already hold it)?",3
Finance,Diminution in value,"Diminution in value is a legal term of art used when calculating damages in a legal dispute, and describes a measure of value lost due to a circumstance or set of circumstances that caused the loss. Specifically, it measures the value of something before and after the causative act or omission creating the lost value in order to calculate compensatory damages.In legal damages theories, diminution in value is often calculated for compensatory special damages when a loss is monetarily quantifiable, and for restitution or disgorgement damages when the loss has unfairly enriched a wrongdoer.",3
Finance,Discounted cash flow,"In finance, discounted cash flow (DCF) analysis is a method of valuing a security, project, company, or asset using the concepts of the time value of money. 
Discounted cash flow analysis is widely used in investment finance, real estate development, corporate financial management and patent valuation. It was used in industry as early as the 1700s or 1800s, widely discussed in financial economics in the 1960s, and became widely used in U.S. courts in the 1980s and 1990s.",3
Finance,Dividend discount model,"In finance and investing, the dividend discount model (DDM) is a method of valuing the price of a company's stock based on the fact that its stock is worth the sum of all of its future dividend payments, discounted back to their present value. In other words, DDM is used to value stocks based on the net present value of the future dividends. The constant-growth form of the DDM is sometimes referred to as the Gordon growth model (GGM), after Myron J. Gordon of the Massachusetts Institute of Technology, the University of Rochester, and the University of Toronto, who published it along with Eli Shapiro in 1956 and made reference to it in 1959. Their work borrowed heavily from the theoretical and mathematical ideas found in John Burr Williams 1938 book ""The Theory of Investment Value,"" which put forth the dividend discount model 18 years before Gordon and Shapiro.
When dividends are assumed to grow at a constant rate, the variables are: 
  
    
      
        P
      
    
    {\displaystyle P}
   is the current stock price. 
  
    
      
        g
      
    
    {\displaystyle g}
   is the constant growth rate in perpetuity expected for the dividends. 
  
    
      
        r
      
    
    {\displaystyle r}
   is the constant cost of equity capital for that company. 
  
    
      
        
          D
          
            1
          
        
      
    
    {\displaystyle D_{1}}
   is the value of dividends at the end of the first period.

  
    
      
        P
        =
        
          
            
              D
              
                1
              
            
            
              r
              −
              g
            
          
        
      
    
    {\displaystyle P={\frac {D_{1}}{r-g}}}",3
Finance,Dividend puzzle,"The dividend puzzle is a concept in finance in which companies that pay dividends are rewarded by investors with higher valuations, even though, according to many economists, it should not matter to investors whether a firm pays dividends or not. The reasoning goes that dividends, from the investor’s point of view, should have no effect on the process of valuing equity because the investor already owns the firm and, thus, he/she should be indifferent to either getting the dividends or having them re-invested in the firm. Another reason for economists to be puzzled is that equity holders pay a higher tax rate on dividend payouts compared to capital gains from the firm repurchasing shares as an alternative payout policy. 
The puzzle evolved from the Modigliani–Miller theorems of 1959 and 1961.
The reasons for the dividend puzzle have been attributed to a wide range of factors, including uncertainties, psychological/behavioral economics issues, tax-related matters, and asymmetric information.",3
Finance,Drawdown cover ratio,"Drawdown cover ratio is one of the key terms in project finance funding agreements.
It compares the projected maximum debt outstanding with the forecast net present value of the project cash flows during the term of the loan.",3
Finance,Economic value added,"In corporate finance, as part of fundamental analysis, economic value added is an estimate of a firm's economic profit, or the value created in excess of the required return of the company's shareholders. EVA is the net profit less the capital charge ($) for raising the firm's capital. The idea is that value is created when the return on the firm's economic capital employed exceeds the cost of that capital. This amount can be determined by making adjustments to GAAP accounting. There are potentially over 160 adjustments but in practice, only several key ones are made, depending on the company and its industry.

",3
Finance,EV/Ebitda,"Enterprise value/EBITDA (more commonly referred to by the acronym EV/EBITDA) is a popular valuation multiple used to determine the fair market value of a company. By contrast to the more widely available P/E ratio (price-earnings ratio) it includes debt as part of the value of the company in the numerator and excludes costs such as the need to replace depreciating plant, interest on debt, and taxes owed from the earnings or denominator. It is the most widely used valuation multiple based on  enterprise value and is often used as an alternative to the P/E ratio when valuing companies believed to be in a high-growth phase, and thus credits enterprises with higher startup costs, high debt relative to equity, and lower realised earnings.

",3
Finance,Event study,"An event study is a statistical method to assess the impact of an event on the value of a firm. For example, the announcement of a merger between two business entities can be analyzed to see whether investors believe the merger will create or destroy value.  The basic idea is to find the abnormal return attributable to the event being studied by adjusting for the return that stems from the price fluctuation of the market as a whole.  The event study was invented by Ball and Brown (1968).As the event methodology can be used to elicit the effects of any type of event on the direction and magnitude of stock price changes, it is very versatile. Event studies are thus common to various research areas, such as accounting and finance, management, economics, marketing, information technology, law, political science, operations and supply chain management.One aspect often used to structure the overall body of event studies is the breadth of the studied event types. On the one hand, there is research investigating the stock market responses to economy-wide events (i.e., market shocks, such as regulatory changes, or catastrophic events). On the other hand, event studies are used to investigate the stock market responses to corporate events, such as mergers and acquisitions, earnings announcements, debt or equity issues, corporate reorganisations, investment decisions and corporate social responsibility (MacKinlay 1997; McWilliams & Siegel, 1997).",3
Finance,Expected commercial value,"Expected commercial value (ECV; also Estimated commercial value) 

is a prospect-weighted value for a ""project"" with unclear conclusions; it is similar to expected net existing value (ENPV). 
In general ECV is used as a supplementary capital budgeting technique, in that it allows an analyst to compare each project's expected value against its net present value as usually calculated, i.e. using planned and contracted costs.
The company can thereby maximize the value and worth of its portfolio of projects, while working within its budget constraints.
As with ENPV, developments are defined to represent different project outcomes, with each scenario being assigned a possibility. A project value is computed for each scenario, and the expected commercial value is obtained by multiplying each situation's value by the scenario odds and adding the results.  
Depending on the procedures used to estimate the value of the project under each scenario, ECV can be a useful way to address project uncertainties. However, as indicated below, the technique often involves explanations that may or may not be appropriate.
Several techniques are used to estimate the probabilities and cashflows of the scenarios; often, the project may be broken down into stages which are represented in a decision tree. In reality, technical and commercial successes are not definite outcomes. There are changeable degrees of technical success and, assuming the product is launched, commercial sales could be anywhere within a variety of possibilities. Still, depending on the assertion, the simple formula may provide a satisfactory calculation. More generally, because ECV is a simplified version of ENPV, it has the limitations of the more general approach (including omission of non-financial sources of project value, and the potential for insufficient treatment of risk).",3
Finance,Expertization,"Expertization is the process of authentication of an object, usually of a sort that is collected, by an individual authority or a committee of authorities. The expert, or expert committee, examines the collectible and issues a certificate typically including:

A statement of:
Whether or not the item is authentic
Identification of any damage to the item
Identification of any repairs to the item
Identification of any forgery or faked parts of the item
A photo of the itemSome experts apply a mark or signature to the item attesting its genuineness.
Expertization is particularly common with valuable philatelic items, some of which are so often forged that they may be unsaleable without it.",3
Finance,Fair value,"In accounting and in most schools of economic thought, fair value is a rational and unbiased estimate of the potential market price of a good, service, or asset. The derivation takes into account such objective factors as the costs associated with production or replacement, market conditions and matters of supply and demand.  Subjective factors may also be considered such as the risk characteristics, the cost of and return on capital, and individually perceived utility.",3
Finance,Fair value accounting and the subprime mortgage crisis,"The United States subprime mortgage crisis was a multinational financial crisis that occurred between 2007 and 2010 that contributed to the 2007–2008 global financial crisis. It was triggered by a large decline in US home prices after the collapse of a housing bubble, leading to mortgage delinquencies, foreclosures, and the devaluation of housing-related securities. Declines in residential investment preceded the Great Recession and were followed by reductions in household spending and then business investment. Spending reductions were more significant in areas with a combination of high household debt and larger housing price declines.The housing bubble preceding the crisis was financed with mortgage-backed securities (MBSes) and collateralized debt obligations (CDOs), which initially offered higher interest rates (i.e. better returns) than government securities, along with attractive risk ratings from rating agencies. While elements of the crisis first became more visible during 2007, several major financial institutions collapsed in September 2008, with significant disruption in the flow of credit to businesses and consumers and the onset of a severe global recession.There were many causes of the crisis, with commentators assigning different levels of blame to financial institutions, regulators, credit agencies, government housing policies, and consumers, among others. Two proximate causes were the rise in subprime lending and the increase in housing speculation. The percentage of lower-quality subprime mortgages originated during a given year rose from the historical 8% or lower range to approximately 20% from 2004 to 2006, with much higher ratios in some parts of the U.S. A high percentage of these subprime mortgages, over 90% in 2006 for example, had an interest rate that increased over time. Housing speculation also increased, with the share of mortgage originations to investors (i.e. those owning homes other than primary residences) rising significantly from around 20% in 2000 to around 35% in 2006–2007. Investors, even those with prime credit ratings, were much more likely to default than non-investors when prices fell. These changes were part of a broader trend of lowered lending standards and higher-risk mortgage products, which contributed to U.S. households becoming increasingly indebted. The ratio of household debt to disposable personal income rose from 77% in 1990 to 127% by the end of 2007.When U.S. home prices declined steeply after peaking in mid-2006, it became more difficult for borrowers to refinance their loans. As adjustable-rate mortgages began to reset at higher interest rates (causing higher monthly payments), mortgage delinquencies soared. Securities backed with mortgages, including subprime mortgages, widely held by financial firms globally, lost most of their value. Global investors also drastically reduced purchases of mortgage-backed debt and other securities as part of a decline in the capacity and willingness of the private financial system to support lending. Concerns about the soundness of U.S. credit and financial markets led to tightening credit around the world and slowing economic growth in the U.S. and Europe.
The crisis had severe, long-lasting consequences for the U.S. and European economies. The U.S. entered a deep recession, with nearly 9 million jobs lost during 2008 and 2009, roughly 6% of the workforce. The number of jobs did not return to the December 2007 pre-crisis peak until May 2014. U.S. household net worth declined by nearly $13 trillion (20%) from its Q2 2007 pre-crisis peak, recovering by Q4 2012. U.S. housing prices fell nearly 30% on average and the U.S. stock market fell approximately 50% by early 2009, with stocks regaining their December 2007 level during September 2012. One estimate of lost output and income from the crisis comes to ""at least 40% of 2007 gross domestic product"". Europe also continued to struggle with its own economic crisis, with elevated unemployment and severe banking impairments estimated at €940 billion between 2008 and 2012. As of January 2018, U.S. bailout funds had been fully recovered by the government, when interest on loans is taken into consideration. A total of $626B was invested, loaned, or granted due to various bailout measures, while $390B had been returned to the Treasury. The Treasury had earned another $323B in interest on bailout loans, resulting in an $109B profit as of January 2021.",3
Finance,Fed model,"The ""Fed model"" or ""Fed Stock Valuation Model"" (FSVM), is a disputed theory of equity valuation that compares the stock market's forward earnings yield to the nominal yield on long-term government bonds, and that the stock market – as a whole – is fairly valued, when the one-year forward-looking I/B/E/S earnings yield equals the 10-year nominal Treasury yield; deviations suggest over-or-under valuation.The relationship has only held in the United States, and only for two main periods: 1921 to 1928 and from 1987 to 2000.  It has been shown to be flawed on a theoretical basis, fails to hold in long-term analysis of data (both in the United States, and international markets), and has poor predictive power for future returns on a 1, 5 and 10-year basis. The relationship can breakdown completely at very low real yields (from natural forces, or where yields are artificially suppressed by quantitative easing); in such circumstances, without additional central bank support for the stock market (e.g. use of the Greenspan put by the Fed in 2020, or the Bank of Japan's purchase of equities post-2013), the relationship collapses.The Fed model is used by Wall Street sales desks as it almost always gives a ""buy signal"", and has rarely signaled stocks are overvalued.  Some academics say the relationship, when it appears, is driven by the allocation of the Fed's balance sheet to Wall Street banks via repurchase agreements as part of Fed put stimulus (i.e. the relationship reflects the investment strategy these banks follow using borrowed Fed funds when the Fed is stimulating asset prices, e.g. Wall Street banks lending to Long-Term Capital Management-type vehicles being a noted example).The term was coined in 1997–99 by Deutsche Bank analyst Dr. Edward Yardeni commenting on a report on the July 1997 Humphrey-Hawkins testimony by the then-Fed Chair, Alan Greenspan on equity valuations.  In 2014, Yardeni noted that the predictive power of the Fed model stopped working almost as soon as he noted the relationship. The term was never formally endorsed by the Fed, however, Greenspan made further references to the relationship.  In December 2020, the Fed Chair Jerome Powell, invoked the relationship to justify stock market valuations that were approaching levels not seen since the 1999–2000 Dot-com bubble or the 1929 market bubble, due to exceptional monetary looseness by the Fed.",3
Finance,Film finance,"Film finance is an aspect of film production that occurs during the development stage prior to pre-production, and is concerned with determining the potential value of a proposed film. 
In the United States, the value is typically based on a forecast of revenues (generally 10 years for films and 20 years for television shows), beginning with theatrical release, and including DVD sales, and release to cable broadcast television networks both domestic and international and inflight airline licensing.",3
Finance,Financial analysis,"Financial analysis (also referred to as financial statement analysis or accounting analysis or Analysis of finance) refers to an assessment of the viability, stability, and profitability of a business, sub-business or project. 
It is performed by professionals who prepare reports using ratios and other techniques, that make use of information taken from financial statements and other reports. These reports are usually presented to top management as one of their bases in making business decisions.  
Financial analysis may determine if a business will:

Continue or discontinue its main operation or part of its business;
Make or purchase certain materials in the manufacture of its product;
Acquire or rent/lease certain machineries and equipment in the production of its goods;
Issue  shares or negotiate for a bank loan to increase its working capital;
Make decisions regarding investing or lending capital;
Make other decisions that allow management to make an informed selection on various alternatives in the conduct of its business.",3
Finance,Financial analyst,"Financial analysis (also referred to as financial statement analysis or accounting analysis or Analysis of finance) refers to an assessment of the viability, stability, and profitability of a business, sub-business or project. 
It is performed by professionals who prepare reports using ratios and other techniques, that make use of information taken from financial statements and other reports. These reports are usually presented to top management as one of their bases in making business decisions.  
Financial analysis may determine if a business will:

Continue or discontinue its main operation or part of its business;
Make or purchase certain materials in the manufacture of its product;
Acquire or rent/lease certain machineries and equipment in the production of its goods;
Issue  shares or negotiate for a bank loan to increase its working capital;
Make decisions regarding investing or lending capital;
Make other decisions that allow management to make an informed selection on various alternatives in the conduct of its business.",3
Finance,First Chicago Method,"The First Chicago Method or Venture Capital Method is a business valuation approach used by venture capital and private equity investors that combines elements of both a multiples-based valuation and a discounted cash flow (DCF) valuation approach. The First Chicago Method was first developed by, and consequently named for, the venture capital arm of the First Chicago bank, the predecessor of private equity firms Madison Dearborn Partners and GTCR.

It was first discussed academically in 1987.",3
Finance,Forecast period (finance),"In corporate finance, in the context of discounted cash flow valuation, the forecast period is the time period during which individual yearly cash flows are input to the valuation-formula. 
Cash flows after the forecast period are represented by a fixed number, the ""terminal value"", determined using assumptions relating to the sustainable compound annual growth rate or exit multiple. 
There are no fixed rules for determining the duration of the forecast period.  However, choosing a forecast period of 10 years, for example, will not be meaningful when individual cash flows can only reasonably be modeled for four years; see Cashflow forecast. 
The number of forecasting years is therefore to be limited by the ""meaningfulness"" of the individual yearly cash flows ahead.  
Addressing this, there are three typical methods of determining the forecast period.

Based on company positioning:  The forecast period corresponds to the years where an excess return is achievable.  In the years chosen the company should expect to generate a return on new investments greater than its cost of capital. This will be based on its expected competitiveness, coupled with known barriers to entry. See: Porter's five forces, a well known tool for analyzing the competition of a business; and Sustainable growth rate  § From a financial perspective for discussion re the economic argument here.
Based on exit strategy:  The number of years after which an ""exit"" is planned.  An exit can either be positive (merger, acquisition, initial public offering) or negative (bankruptcy). This method is typically used by investors in venture capital and private equity, planning a positive exit. See: Private equity  § Investment timescales; Venture capital  § Financing stages.
Based on market characteristics: Determine a forecast period by choosing a number of years based on the characteristics of the market. Companies in established and well known markets are better suited towards longer forecasting periods than those opening up a new market, or startups.",3
Finance,"Furniture, fixtures and equipment (accounting)","Furniture, fixtures, and equipment (or FF&E) (sometimes Furniture, furnishings, and equipment) is an accounting term used in valuing, selling, or liquidating a company or a building.
FF&E are movable furniture, fixtures, or other equipment that have no permanent connection to the structure of a building or utilities. These items depreciate substantially but are important costs to consider when valuing a company, especially in liquidation.
Examples of FF&E include desks, chairs, computers, electronic equipment, tables, bookcases, and partitions.
Sometimes the term FF&A is used (furniture, fixtures, and accessories).",3
Finance,German income approach,"The German income approach (German: Ertragswertverfahren, abbr. EWV) is the standard approach used in Germany for the valuing of property that produces  a stream of future cash flows.",3
Finance,Graham number,"Graham's number is an immense number that arose as an upper bound on the answer of a problem in the mathematical field of Ramsey theory. It is much larger than many other large numbers such as Skewes's number and Moser's number, both of which are in turn much larger than a googolplex. As with these, it is so large that the observable universe is far too small to contain an ordinary digital representation of Graham's number, assuming that each digit occupies one Planck volume, possibly the smallest measurable space. But even the number of digits in this digital representation of Graham's number would itself be a number so large that its digital representation cannot be represented in the observable universe.  Nor even can the number of digits of that number—and so forth, for a number of times far exceeding the total number of Planck volumes in the observable universe. Thus Graham's number cannot be expressed even by physical universe-scale power towers of the form 
  
    
      
        
          a
          
            
              b
              
                
                  c
                  
                    
                      ⋅
                      
                        
                          ⋅
                          
                            ⋅
                          
                        
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle a^{b^{c^{\cdot ^{\cdot ^{\cdot }}}}}}
  .
However, Graham's number can be explicitly given by computable recursive formulas using Knuth's up-arrow notation or equivalent, as was done by Graham.  As there is a recursive formula to define it, it is much smaller than typical busy beaver numbers.  Though too large to be computed in full, the sequence of digits of Graham's number can be computed explicitly through simple algorithms; the last thirteen digits are ...7262464195387.  With Knuth's up-arrow notation, Graham's number is 
  
    
      
        
          g
          
            64
          
        
      
    
    {\displaystyle g_{64}}
  , where

Graham's number is named after the American mathematician Ronald Graham, who used the number in conversations with popular science writer Martin Gardner as a simplified explanation of the upper bounds of the problem he was working on. In 1977, Gardner described the number in Scientific American, introducing it to the general public.  At the time of its introduction, it was the largest specific positive integer ever to have been used in a published mathematical proof.  The number was described in the 1980 Guinness Book of World Records, adding to its popular interest.  Other specific integers (such as TREE(3)) known to be far larger than Graham's number have since appeared in many serious mathematical proofs, for example in connection with Harvey Friedman's various finite forms of Kruskal's theorem. Additionally, smaller upper bounds on the Ramsey theory problem from which Graham's number derived have since been proven to be valid.

",3
Finance,The Growth X,"The Growth X is commonly used to describe the pattern created when plotting the market value of a successful growth company against its relative valuation over time.  Early on in a company's life cycle (particularly with internet investments), companies can demand high valuations despite little or negative earnings.  When the company breaks even, its valuation seems inflated from a relative valuation standpoint (market value divided by the company's earnings, EBITDA, or cash flow).  Over time as the earnings power grows, the relative valuation begins to fall as earnings often growth faster than the market value (stock price).This pattern can be observed in most successful companies in recent history (AAPL, GOOG, FB).  The Growth X is commonly used among growth and venture capital investors, and is believed to be coined by Spencer Walsh and Vidur Singhal, two prominent Silicon Valley internet investors.Much like other technical indicators, The Growth X is often disputed and depends on a company's ability to continually grow earnings.  If investors bid up the market value of the company faster than the earnings growth rate, then The Growth X will not form, and the relative valuation will stay flat or continue to rise.  Eventually relative valuations fall as growth prospects diminish, but this can often occur quickly and cause a step function down in valuation multiples.",3
Finance,Hamada's equation,"In corporate finance, Hamada’s equation, named after Robert Hamada, is used to separate the financial risk of a levered firm from its business risk. The equation combines the Modigliani–Miller theorem with the capital asset pricing model. It is used to help determine the levered beta and, through this, the optimal capital structure of firms.
Hamada’s equation relates the beta of a levered firm (a firm financed by both debt and equity) to that of its unlevered (i.e., a firm which has no debt) counterpart. It has proved useful in several areas of finance, including capital structuring, portfolio management and risk management, to name just a few. This formula is commonly taught in MBA Corporate Finance and Valuation classes. It is used to determine the cost of capital of a levered firm based on the cost of capital of comparable firms. Here, the comparable firms would be the ones having similar business risk and, thus, similar unlevered betas as the firm of interest.

",3
Finance,Herfindahl–Hirschman index,"The Herfindahl index (also known as Herfindahl–Hirschman Index, HHI, or sometimes HHI-score) is a measure of the size of firms in relation to the industry they are in and is an indicator of the amount of competition among them.  Named after economists Orris C. Herfindahl and Albert O. Hirschman, it is an economic concept widely applied in competition law, antitrust and also technology management. HHI has continued to be used by antitrust authorities, primarily to evaluate and understand how mergers will affect their associated markets. HHI is calculated by squaring the market share of each competing firm in the industry and then summing the resulting numbers,(sometimes limited to the 50 largest firms), market shares are expressed as either fractions, decimals, or whole numbers. The result is proportional to the average market share, weighted by market share. As such, it can range from 0 to 1.0, moving from a huge number of very small firms to a single monopolistic producer. Increases in the Herfindahl index generally indicate a decrease in competition and an increase of market power, whereas decreases indicate the opposite. Alternatively, if whole percentages are used, the index ranges from 0 to 10,000 ""points"".  For example, an index of .25 is the same as 2,500 points.
The major benefit of the Herfindahl index in relationship to such measures as the concentration ratio is that it gives more weight to larger firms. Other benefits of the Herfindahl index includes its simple calculation method and the small amount of easily obtainable data required for the calculation.The measure is the same formula as the Simpson diversity index, which is a diversity index used in ecology; the inverse participation ratio (IPR) in physics; and the effective number of parties index in politics.",3
Finance,Income approach,"The income approach is one of three major groups of methodologies, called valuation approaches, used by appraisers. It is particularly common in commercial real estate appraisal and in business appraisal. The fundamental math is similar to the methods used for financial valuation, securities analysis, or bond pricing. However, there are some significant and important modifications when used in real estate or business valuation.
While there are quite a few acceptable methods under the rubric of the income approach, most of these methods fall into three categories: direct capitalization, discounted cash flow, and gross income multiplier.",3
Finance,Institution of Valuers,"The Institution of Valuers (IOV) is a national valuation body for licensing and regulation of the valuation professionals in India specialized in various disciplines and various kinds of assets. It is under the ownership of Ministry of Corporate Affairs, Government of India. It was founded in 1968 by Shri P. C. Goel, also named as the Father of the Indian Valuer, and presently has a membership of over 20,000 valuers. The IOV serves recommendations on valuation procedures and related disputes to governmental as well as non governmental organizations, imparts training to valuation professionals, and develops standards in asset valuation.IOV is amongst the top five valuers association of the world on the basis of registered valuation professionals. The Institution of Valuers brings into its fold valuers of immovable property, agricultural lands, coffee estates, stocks, shares and debentures of companies, shares of a partner in a partnership, business assets including goodwill, jewellery, precious stones and ornaments, works of art, life interest reversions and interests expectancy, tea estates, standing forests, mines and quarries, machinery, electrical equipments, industry etc.",3
Finance,Intellectual property valuation,"Valuation is considered one of the most critical areas in finance; it plays a key role in many areas of finance such as buy/sell, solvency, merger and acquisition.",3
Finance,Intermediated research,"In finance, intermediated research is a type of fundamental analysis or investment analysis of a business to establish its value for investors that attempts to avoid commercial pressure or influence.
Many banks and brokers provide research to the investment community, however this form of research suffers from a real or perceived lack of objectivity. For example, this research may be subject to influence from within a bank from, say, investment bankers keen to win company’s IPO mandate.
An alternative form is known as independent research or alternative research, this is research that is not provided by a bank or broker. However, someone has to fund the costs of conducting the research, and when the funder is the company being researched, this form of independent research is termed “sponsored” or “company-paid” research. Unfortunately, with the subject company paying directly for these services, this form of research suffers from a real or perceived lack of objectivity, which reduces its value to investors and hence to the companies themselves.
Intermediated research improves significantly upon the sponsored research model, by introducing important safeguards into the commercial and research process framework. The key structure is that the company has no choice in its allocation to a research provider; once matched with the research provider, the company is under a long term contract in order to minimize commercial pressure or influence; and the fees paid to the research provider are not sufficiently material for the company to be able to exert any influence over the content or conclusions of the research report. Collectively, these measures enhance the value of the report to investors and consequently, to the company itself.
The intermediated research framework includes the further protections; ensuring that the independent research firms involved have no investment banking or brokerage operations in their business models; conduct no traditional company-paid research; deliver a standard template for analysis of every company so that all standard topics are covered; and avoiding ratings and target prices in the reports, since these can risk becoming a focus of pressure exerted on the research provider by the subject company.",3
Finance,International Valuation Standards Council,"The International Valuation Standards Council (IVSC) is an independent, not-for-profit, private sector standards organisation incorporated in the United States and with its operational headquarters in London, UK. IVSC develops international technical and ethical standards for valuations on which investors and others rely.
IVSC is responsible for developing the International Valuation Standards and associated technical guidance. To ensure that the public interest is effectively protected it also engages with other bodies active in the regulation of the financial markets to ensure that valuation issues are properly understood and reflected. IVSC works cooperatively with national professional valuation institutes, users and preparers of valuations, governments, regulators and academic bodies, all of whom can become members of IVSC and play a role in advising the Boards on their agenda priorities.
In developing its standards and technical guidance, IVSC follows a process of issuing discussion papers and exposure drafts for public comment.
The IVSC is recognised by the United Nations Department of Economic and Social Affairs.",3
Finance,Investment theory,"Parental investment, in evolutionary biology and evolutionary psychology, is any parental expenditure (e.g. time, energy, resources) that benefits offspring. Parental investment may be performed by both males and females (biparental care), females alone (exclusive maternal care) or males alone (exclusive paternal care). Care can be provided at any stage of the offspring's life, from pre-natal (e.g. egg guarding and incubation in birds, and placental nourishment in mammals) to post-natal (e.g. food provisioning and protection of offspring).
Parental investment theory, a term coined by Robert Trivers in 1972, predicts that the sex that invests more in its offspring will be more selective when choosing a mate, and the less-investing sex will have intra-sexual competition for access to mates. This theory has been influential in explaining sex differences in sexual selection and mate preferences, throughout the animal kingdom and in humans.",3
Finance,Investment value,"Investment value is the value of a property to a particular investor.  In the U.S. and U.K., it is equal to market value for the investor who has the capacity to put the property to good use—its highest-and-best-use, its most valuable use. For other investors with limited capacity or vision, investment value is lower because they cannot put the property to use in a way that is maximally productive.",3
Finance,Land value tax,"A land value tax (LVT) is a levy on the value of land without regard to buildings, personal property and other improvements. It is also known as a location value tax, a site valuation tax, split rate tax, or a site-value rating,
Land value taxes are generally favored by economists as they do not cause economic inefficiency, and reduce inequality. A land value tax is a progressive tax, in that the tax burden falls on land owners, because land ownership  is correlated with wealth and income. The land value tax has been referred to as ""the perfect tax"" and the economic efficiency of a land value tax has been accepted since the eighteenth century. Economists since Adam Smith and David Ricardo have advocated this tax because it does not hurt economic activity or discourage or subsidize development.
LVT is associated with Henry George, whose ideology became known as Georgism. George argued that taxing the land value is most logical source of public revenue because the supply of land is fixed and because public infrastructure improvements would be reflected in (and thus paid for) by increased land values.Land value taxation is currently implemented throughout Denmark, Estonia, Lithuania, Russia, Singapore, and Taiwan; it has also been applied to lesser extents in parts of Australia, Mexico (Mexicali), and the United States (e.g., Pennsylvania).",3
Finance,Liquidation value,"Liquidation value is the likely price of an asset when it is allowed insufficient time to sell on the open market, thereby reducing its exposure to potential buyers. Liquidation value is typically lower than fair market value. Unlike cash or securities, certain illiquid assets, like real estate, often require a period of several months in order to obtain their fair market value in a sale, and will generally sell for a significantly lower price if a sale is forced to occur in a shorter time period. The liquidation value may be either the result of a forced liquidation or an orderly liquidation. Either value assumes that the sale is consummated by a seller who is compelled to sell and assumes an exposure period which is less than market normal.
The most common definition used by real estate appraisers is as followsThe most probable price that a specified interest in real property is
likely to bring under all of the following conditions:

Consummation of a sale will occur within a severely limited future marketing period specified by the client.
The actual market conditions currently prevailing are those to which the appraised property interest is subject.
The buyer is acting prudently and knowledgeably.
The seller is under extreme compulsion to sell.
The buyer is typically motivated.
The buyer is acting in what he or she considers his or her best interest.
A limited marketing effort and time will be allowed for the completion of a sale.
Payment will be made in cash in U.S. dollars or in terms of financial arrangements comparable thereto.
The price represents the normal consideration for the property sold, unaffected by special or creative financing or sales concessions granted by anyone associated with the sale.Note that this definition differs from the most commonly used definitions of market value or fair market value.

",3
Finance,Magic formula investing,Magic formula investing is an investment technique outlined by Joel Greenblatt that uses the principles of value investing.,3
Finance,Mark to model,"Mark-to-Model refers to the practice of pricing a position or portfolio at prices determined by financial models, in contrast to allowing the market to determine the price. Often the use of models is necessary where a market for the financial product is not available, such as with 
complex financial instruments. One shortcoming of Mark-to-Model is that it gives an artificial illusion of liquidity, and the actual price of the product depends on the accuracy of the financial models
used to estimate the price.

On the other hand it is argued that Asset managers and Custodians have a real problem valuing illiquid assets in their portfolios even though many of these assets are perfectly sound and the asset manager has no intention of selling them. Assets should be valued at mark to market prices as required by the Basel rules. However mark to market prices should not be used in isolation, but rather compared to model prices to test their validity. Models should be improved to take into account the greater amount of market data available. New methods and new data are available to help improve models and these should be used. In the end all prices start off from a model.",3
Finance,Mark-to-market accounting,"Mark-to-market (MTM or M2M) or fair value accounting refers to accounting for the ""fair value"" of an asset or liability based on the current market price, or the price for similar assets and liabilities, or based on another objectively assessed ""fair"" value.  Fair value accounting has been a part of Generally Accepted Accounting Principles (GAAP) in the United States since the early 1990s, and is now regarded as the ""gold standard"" in some circles. Failure to use it is viewed as the cause of the Orange County Bankruptcy, even though its use is considered to be one of the reasons for the Enron scandal and the eventual bankruptcy of the company, as well as the closure of the accounting firm Arthur Andersen.Mark-to-market accounting can change values on the balance sheet as market conditions change.  In contrast, historical cost accounting, based on the past transactions, is simpler, more stable, and easier to perform, but does not represent current market value. It summarizes past transactions instead. Mark-to-market accounting can become volatile if market prices fluctuate greatly or change unpredictably. Buyers and sellers may claim a number of specific instances when this is the case, including inability to value the future income and expenses both accurately and collectively, often due to unreliable information, or over-optimistic or over-pessimistic expectations of cash flow and earnings.",3
Finance,Mid-year adjustment,"Valuation using discounted cash flows (DCF valuation) is a method of estimating the current value of a company based on projected future cash flows adjusted for the time value of money.
The cash flows are made up of those within the “explicit” forecast period, together with a continuing or terminal value that represents the cash flow stream after the forecast period.
In several contexts, DCF valuation is referred to as the ""income approach"".
Discounted cash flow valuation was used in industry as early as the 1700s or 1800s; it was explicated by John Burr Williams in his The Theory of Investment Value in 1938; it was widely discussed in financial economics in the 1960s; and became widely used in U.S. courts in the 1980s and 1990s.
This article details the mechanics of the valuation, via a worked example, including modifications typical for startups, private equity and venture capital, corporate finance ""projects"", and mergers and acquisitions.
See Discounted cash flow for further discussion, and Valuation (finance) § Valuation overview for context.",3
Finance,Mismarking,"Mismarking in securities valuation takes place when the value that is assigned to securities does not reflect what the securities are actually worth, due to intentional fraudulent mispricing. Mismarking misleads investors and fund executives about how much the securities in a securities portfolio managed by a trader are worth (the securities' net asset value, or NAV), and thus misrepresents performance.  When a trader engages in mismarking, it allows him to obtain a higher bonus from the financial firm for which he works, where his bonus is calculated by the performance of the securities portfolio that he is managing.Mismarking is an element of operational risk. The trader engaging in mismarking is sometimes referred to as a ""rogue trader.""During market downturns, determining the value of illiquid securities held in portfolios becomes especially challenging, in part because of the amount of debt associated with these securities and in part because of fewer mechanisms for price discovery. As a result, during such periods illiquid securities are especially susceptible to fraudulent mismarking.",3
Finance,Mortgage modification,"Mortgage modification is a process where the terms of a mortgage are modified outside the original terms of the contract agreed to by the lender and borrower (i.e. mortgagee and mortgagor in mortgage states; Trustee and Trustor in Trust Deed states). In general, any loan can be modified, and the process is referred to as loan modification or debt rescheduling.",3
Finance,Net present value,"The net present value (NPV) or net present worth (NPW) applies to a series of cash flows occurring at different times. The present value of a cash flow depends on the interval of time between now and the cash flow. It also depends on the discount rate. NPV accounts for the time value of money. It provides a method for evaluating and comparing capital projects or financial products with cash flows spread over time, as in loans, investments, payouts from insurance contracts plus many other applications.
Time value of money dictates that time affects the value of cash flows. For example, a lender may offer 99 cents for the promise of receiving $1.00 a month from now, but the promise to receive that same dollar 20 years in the future would be worth much less today to that same person (lender), even if the payback in both cases was equally certain. This decrease in the current value of future cash flows is based on a chosen rate of return (or discount rate).  If for example there exists a time series of identical cash flows, the cash flow in the present is the most valuable, with each future cash flow becoming less valuable than the previous cash flow. A cash flow today is more valuable than an identical cash flow in the future because a present flow can be invested immediately and begin earning returns, while a future flow cannot.
NPV is determined by calculating the costs (negative cash flows) and benefits (positive cash flows) for each period of an investment. After the cash flow for each period is calculated, the present value (PV) of each one is achieved by discounting its future value (see Formula) at a periodic rate of return (the rate of return dictated by the market). NPV is the sum of all the discounted future cash flows.  
Because of its simplicity, NPV is a useful tool to determine whether a project or investment will result in a net profit or a loss. A positive NPV results in profit, while a negative NPV results in a loss. The NPV measures the excess or shortfall of cash flows, in present value terms, above the cost of funds. In a theoretical situation of unlimited capital budgeting, a company should pursue every investment with a positive NPV. However, in practical terms a company's capital constraints limit investments to projects with the highest NPV whose cost cash flows, or initial cash investment, do not exceed the company's capital. NPV is a central tool in discounted cash flow (DCF) analysis and is a standard method for using the time value of money to appraise long-term projects. It is widely used throughout economics, financial analysis, and financial accounting.
In the case when all future cash flows are positive, or incoming (such as the principal and coupon payment of a bond) the only outflow of cash is the purchase price, the NPV is simply the PV of future cash flows minus the purchase price (which is its own PV). NPV can be described as the ""difference amount"" between the sums of discounted cash inflows and cash outflows. It compares the present value of money today to the present value of money in the future, taking inflation and returns into account.
The NPV of a sequence of cash flows takes as input the cash flows and a discount rate or discount curve and outputs a present value, which is the current fair price. The converse process in discounted cash flow (DCF) analysis takes a sequence of cash flows and a price as input and as output the discount rate, or internal rate of return (IRR) which would yield the given price as NPV. This rate, called the yield, is widely used in bond trading.
Many computer-based spreadsheet programs have built-in formulae for PV and NPV.",3
Finance,Overtrading,"Overtrading is a term in financial statement analysis. Overtrading often occurs when companies expand their own operations too quickly (aggressively).  Overtraded companies enter a negative cycle, where an increase in interest expenses negatively impacts the net profit, which leads to lesser working capital, and that leads to increased borrowings, which in turn leads to interest expenses and the cycle continues.  Overtraded companies eventually face liquidity problems and can run out of working capital.",3
Finance,Owner earnings,"Owner earnings is a valuation method detailed by Warren Buffett in Berkshire Hathaway's annual report in 1986. He stated that the value of a company is simply the total of the net cash flows (owner earnings) expected to occur over the life of the business, minus any reinvestment of earnings.Buffett defined owner earnings as follows:

""These represent (a) reported earnings plus (b) depreciation, depletion, amortization, and certain other non-cash charges ... less (c) the average annual amount of capitalized expenditures for plant and equipment, etc. that the business requires to fully maintain its long-term competitive position and its unit volume ... Our owner-earnings equation does not yield the deceptively precise figures provided by GAAP, since (c) must be a guess - and one sometimes very difficult to make. Despite this problem, we consider the owner earnings figure, not the GAAP figure, to be the relevant item for valuation purposes ... All of this points up the absurdity of the 'cash flow' numbers that are often set forth in Wall Street reports. These numbers routinely include (a) plus (b) - but do not subtract (c).""",3
Finance,Paper valuation,"Paper Valuation is the value of privately held shares that is not directly tradeable at an exchange.
This notional value, though, is as yet untested on real buyers. 
The opposite of paper value is exchangeable value, and is the value that is directly monetizable as long as there is a willing buyer and a willing seller.
Thus, if the aside exchange was made as an ""Exchange Valuation"" this new company valuation would be tradeable directly on the stock exchange. One problem with Paper Valuation is that is not that easy to monetize in a short time period.
This valuation concept is a cornerstone in the stock exchange world. Value exchange is paramount to its existence.",3
Finance,Par value,"In null-hypothesis significance testing, the p-value is the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct. A very small p-value means that such an extreme observed outcome would be very unlikely under the null hypothesis. Reporting p-values of statistical tests is common practice in academic publications of many quantitative fields. Since the precise meaning of p-value is hard to grasp, misuse is widespread and has been a major topic in metascience.",3
Finance,Penalized present value,"The Penalized Present Value (PPV) is a method of capital budgeting under risk developed by Fernando Gómez-Bezares in the 1980s, where the value of the investment is ""penalized"" as a function of its volatility.",3
Finance,Period of financial distress,"A period of financial distress occurs when the price of a company or an asset or an index of a set of assets in a market is declining with the danger of a sudden crash of value occurring, either because the company is experiencing increasing problems of cash flow or a deteriorating credit balance or because the price had become too high as a result of a speculative bubble that has now peaked.",3
Finance,Piotroski F-score,"Piotroski F-score is a number between 0 and 9 which is used to assess strength of company's financial position. The score is used by financial investors in order to find the best value stocks (nine being the best). The score is named after Stanford accounting professor Joseph Piotroski.

",3
Finance,Post-money valuation,"Post-money valuation is a way of expressing the value of a company after an investment has been made. This value is equal to the sum of the pre-money valuation and the amount of new equity.These valuations are used to express how much ownership external investors, such as venture capitalists and angel investors, receive when they make a cash injection into a company.  The amount external investors invest into a company is equal to the company's post-money valuation multiplied by the fraction of the company those investors own after the investment. Equivalently, the implied post-money valuation is calculated as the dollar amount of investment divided by the equity stake gained in an investment.
More specifically, the post-money valuation of a financial investment deal is given by the formula 
  
    
      
        P
        M
        V
        =
        N
        ×
        P
      
    
    {\textstyle PMV=N\times P}
  , where PMV is the post-money valuation, N is the number of shares the company has after the investment, and P is the price per share at which the investment was made. This formula is similar to the market capitalization formula used to express the value of public companies.

",3
Finance,Pre-money valuation,"A pre-money valuation is a term widely used in the private equity and venture capital industries. It refers to the valuation of a company or asset prior to an investment or financing. If an investment adds cash to a company, the company will have a valuation after the investment that is equal to the pre-money valuation plus the cash amount. That is, the pre-money valuation refers to the company's valuation before the investment. It is used by equity investors in the primary market, such as venture capitalists, private equity investors, corporate investors and angel investors. They may use it to determine how much equity they should be issued in return for their investment in the company. This is calculated on a fully diluted basis. For example, all warrants and options issued are taken into account.
Startups and venture capital-backed companies usually receive multiple rounds of financing rather than a big lump sum. This is in order to decrease the risk for investors and to motivate entrepreneurs. These rounds are conventionally named Round A, Round B, Round C, etc. Pre-money and post-money valuation concepts apply to each round.",3
Finance,Present value of growth opportunities,"In corporate finance, the present value of growth opportunities (PVGO) is a valuation measure applied to growth stocks.
It represents the component of the company’s stock value that corresponds to (expected) growth in earnings. 
It thus allows an analyst to assess the extent to which the share price represents the current business, and to what extent it reflects assumptions about the future.
As a proportion of market cap, PVGO can then also be used in relative valuation, i.e. when comparing between two investments  (see similar re PEG ratio).
PVGO is calculated as follows:

PVGO = share price - earnings per share ÷ cost of capital.This arises by thinking of the value of a company as inhering two components:
(i) the present value of existing earnings, i.e. the company continuing as if under a ""no-growth policy"";
and (ii) the present value of the company's growth opportunities.
PVGO can then simply be calculated as the difference between the stock price and the present value of its zero-growth-earnings; this second term uses the formula for a perpetuity (see Dividend discount model § Some properties of the model).",3
Finance,Project finance model,"A project finance model is a specialized financial model, the purpose of which is to assess the economic feasibility of the project in question. The model's output can also be used in structuring, or ""sculpting"", the project finance deal.
The context: project finance is the long-term financing of infrastructure and industrial projects based upon the projected cash flows of the project - rather than the balance sheets of its sponsors. The project is therefore only feasible when the project is capable of producing enough cash to cover all operating and debt-servicing expenses over the whole tenor of the debt. 
Most importantly, therefore, the model is used to determine the maximum amount of debt the project company (Special-purpose entity) can maintain - and the corresponding debt repayment profile; there are several related metrics here, the most important of which is arguably the Debt Service Coverage Ratio (DSCR).",3
Finance,Quantitative analysis (finance),"Quantitative analysis is the use of mathematical and statistical methods in finance and investment management. Those working in the field are quantitative analysts (quants). Quants tend to specialize in specific areas which may include derivative structuring or pricing, risk management, algorithmic trading and investment management. The occupation is similar to those in industrial mathematics in other industries. The process usually consists of searching vast databases for patterns, such as correlations among liquid assets or price-movement patterns (trend following or mean reversion). The resulting strategies may involve high-frequency trading. 
Although the original quantitative analysts were ""sell side quants"" from market maker firms, concerned with derivatives pricing and risk management, the meaning of the term has expanded over time to include those individuals involved in almost any application of mathematical finance, including the buy side. Applied quantitative analysis is commonly associated with quantitative investment management which includes a variety of methods such as statistical arbitrage, algorithmic trading and electronic trading.
Some of the larger investment managers using quantitative analysis include Renaissance Technologies, D. E. Shaw & Co., and AQR Capital Management.

",3
Finance,Quantitative analyst,"Quantitative analysis is the use of mathematical and statistical methods in finance and investment management. Those working in the field are quantitative analysts (quants). Quants tend to specialize in specific areas which may include derivative structuring or pricing, risk management, algorithmic trading and investment management. The occupation is similar to those in industrial mathematics in other industries. The process usually consists of searching vast databases for patterns, such as correlations among liquid assets or price-movement patterns (trend following or mean reversion). The resulting strategies may involve high-frequency trading. 
Although the original quantitative analysts were ""sell side quants"" from market maker firms, concerned with derivatives pricing and risk management, the meaning of the term has expanded over time to include those individuals involved in almost any application of mathematical finance, including the buy side. Applied quantitative analysis is commonly associated with quantitative investment management which includes a variety of methods such as statistical arbitrage, algorithmic trading and electronic trading.
Some of the larger investment managers using quantitative analysis include Renaissance Technologies, D. E. Shaw & Co., and AQR Capital Management.

",3
Finance,Quantum valebant,"Quantum valebant is a Latin phrase meaning ""as much as they were worth"". It is sometimes used in its singular form, quantum valebat, meaning “as much as it was worth"". It is a common count at law very similar to quantum meruit. The two legal actions differ only in that quantum meruit is used to recover the reasonable value of services rendered, while quantum valebant is used to recover the reasonable value of goods sold and delivered. This count is considered a type of assumpsit.",3
Finance,Real versus nominal value (economics),"In economics, nominal value is measured in terms of money, whereas real value is measured against goods or services. A real value is one which has been adjusted for inflation, enabling comparison of quantities as if the prices of goods had not changed on average. Changes in value in real terms therefore exclude the effect of inflation. In contrast with a real value, a nominal value has not been adjusted for inflation, and so changes in nominal value reflect at least in part the effect of inflation.",3
Finance,Relative valuation,"Relative valuation also called valuation using multiples is the notion of comparing the price of an asset to the market value of similar assets. In the field of securities investment, the idea has led to important practical tools, which could presumably spot pricing anomalies. These tools have subsequently become instrumental in enabling analysts and investors to make vital decisions on asset allocation.",3
Finance,Required rate of return,"In finance, discounted cash flow (DCF) analysis is a method of valuing a security, project, company, or asset using the concepts of the time value of money. 
Discounted cash flow analysis is widely used in investment finance, real estate development, corporate financial management and patent valuation. It was used in industry as early as the 1700s or 1800s, widely discussed in financial economics in the 1960s, and became widely used in U.S. courts in the 1980s and 1990s.",3
Finance,Residual income valuation,"Residual income valuation (RIV; also, residual income model and residual income method, RIM) is an approach to equity valuation that formally accounts for the cost of equity capital.  Here, ""residual"" means in excess of any opportunity costs measured relative to the book value of shareholders' equity; residual income (RI) is then the income generated by a firm after accounting for the true cost of capital. The approach is largely analogous to the EVA/MVA based approach, with similar logic and advantages. Residual Income valuation has its origins in Edwards & Bell (1961), Peasnell (1982), and Ohlson (1995).

",3
Finance,Revaluation of fixed assets,"In finance, a revaluation of fixed assets is an action that may be required to accurately describe the true value of the capital goods a business owns. This should be distinguished from planned depreciation, where the recorded decline in value of an asset is tied to its age.
Fixed assets are held by an enterprise for the purpose of producing goods or rendering services, as opposed to being held for resale for the normal course of business. An example, machines, buildings, patents, or licenses can be fixed assets of a business.
The purpose of a revaluation is to bring into the books the fair market value of fixed assets. This may be helpful in order to decide whether to invest in another business. If a company wants to sell one of its assets, it is revalued in preparation for sales negotiations.",3
Finance,Smith v Eric S Bush,"Smith v Eric S Bush [1990] UKHL 1 is an English tort law and contract law case, heard by the House of Lords. First, it concerned the existence of a duty of care in tort for negligent misstatements, not made directly to someone relying on the statement. Second, it concerned the reasonableness of a term excluding liability under the Unfair Contract Terms Act 1977, s 2(2) and s 11.",3
Finance,Sociology of valuation,"The sociology of valuation (sometimes ""valuation studies"") is an emerging area of study focusing on the tools, models, processes, politics, cultural differences and other inputs and outcomes of valuation.",3
Finance,Stock valuation,"In financial markets, stock valuation is the method of calculating theoretical values of companies and their stocks. The main use of these methods is to predict future market prices, or more generally, potential market prices, and thus to profit from price movement – stocks that are judged undervalued (with respect to their theoretical value) are bought, while stocks that are judged overvalued are sold, in the expectation that undervalued stocks will overall rise in value, while overvalued stocks will generally decrease in value.
In the view of fundamental analysis, stock valuation based on fundamentals aims to give an estimate of the intrinsic value of a stock, based on predictions of the future cash flows and profitability of the business. Fundamental analysis may be replaced or augmented by market criteria – what the market will pay for the stock, disregarding intrinsic value. These can be combined as ""predictions of future cash flows/profits (fundamental)"", together with ""what will the market pay for these profits?"" These can be seen as ""supply and demand"" sides – what underlies the supply (of stock), and what drives the (market) demand for stock?
Stock valuation is distinct from business valuation, which is about calculating the economic value of an owner's interest in a business, used to determine the price interested parties would be willing to pay or receive to effect a sale of the business.
Re. valuation in cases where both parties are corporations, see under Mergers and acquisitions and Corporate finance.",3
Finance,Store of value,"A store of value is any commodity or asset that would normally retain purchasing power into the future and is the function of the asset that can be saved, retrieved and exchanged at a later time, and be predictably useful when retrieved.The most common store of value in modern times has been money, currency, or a commodity like a precious metal or financial capital. The point of any store of value is risk management due to a stable demand for the underlying asset.

",3
Finance,Sum of perpetuities method,"The sum of perpetuities method (SPM)   is a way of valuing a business assuming that investors discount the future earnings of a firm regardless of whether earnings are paid as dividends or retained.  SPM is an alternative to the Gordon growth model (GGM)  and can be applied to business or stock valuation if the business is assumed to have constant earnings and/or dividend growth.  The variables are:

  
    
      
        P
      
    
    {\displaystyle P}
   is the value of the stock or business

  
    
      
        E
      
    
    {\displaystyle E}
   is a company's earnings

  
    
      
        G
      
    
    {\displaystyle G}
   is the company's constant growth rate

  
    
      
        K
      
    
    {\displaystyle K}
   is the company's risk adjusted discount rate

  
    
      
        D
      
    
    {\displaystyle D}
   is the company's dividend payment
  
    
      
        P
        =
        (
        
          
            
              E
              ∗
              G
            
            
              K
              
                2
              
            
          
        
        )
        +
        (
        
          
            D
            K
          
        
        )
      
    
    {\displaystyle P=({\frac {E*G}{K^{2}}})+({\frac {D}{K}})}",3
Finance,Tax amortization benefit,"In accounting, tax amortization benefit (or tax amortisation benefit) refers to the present value of income tax savings resulting from the tax deduction generated by the amortization of an intangible asset.",3
Finance,Terminal value (finance),"In finance, the terminal value (also known as “continuing value” or “horizon value” or ""TV"") of a security is the present value at a future point in time of all future cash flows when we expect stable growth rate forever. It is most often used in multi-stage discounted cash flow analysis, and allows for the limitation of cash flow projections to a several-year period; see Forecast period (finance). 
Forecasting results beyond such a period is impractical and exposes such projections to a variety of risks limiting their validity, primarily the great uncertainty involved in predicting industry and macroeconomic conditions beyond a few years.
Thus, the terminal value allows for the inclusion of the value of future cash flows occurring beyond a several-year projection period while satisfactorily mitigating many of the problems of valuing such cash flows. 
The terminal value is calculated in accordance with a stream of projected future free cash flows in discounted cash flow analysis. 
For whole-company valuation purposes, there are two methodologies used to calculate the Terminal Value.",3
Finance,The Theory of Investment Value,"John Burr Williams (November 27, 1900 – September 15, 1989) was an American economist, recognized as an important figure in the field of fundamental analysis, and for his analysis of stock prices as reflecting their ""intrinsic value"".He is best known for his 1938 text The Theory of Investment Value, based on his PhD thesis, in which he articulated the theory of discounted cash flow (DCF) based valuation, and in particular, dividend based valuation.

",3
Finance,Total Beta,"Business valuation is a process and a set of procedures used to estimate the economic value of an owner's interest in a business.  Here various valuation techniques are used by financial market participants to determine the price they are willing to pay or receive to effect a sale of the business.
In addition to estimating the selling price of a business, the same valuation tools are often used by business appraisers to resolve disputes related to estate and gift taxation, divorce litigation, allocate business purchase price among business assets, establish a formula for estimating the value of partners' ownership interest for buy-sell agreements, and many other business and legal purposes such as in shareholders deadlock, divorce litigation and estate contest.Specialized business valuation credentials include the Chartered Business Valuator (CBV) offered by the CBV Institute, ASA and CEIV from the American Society of Appraisers, and the CVA by the National Association of Certified Valuators and Analysts. 
In some cases, the court would appoint a forensic accountant as the joint-expert doing the business valuation.  Here, attorneys should always be prepared to have their expert's report withstand the scrutiny of cross-examination and criticism. Business valuation is distinct from stock valuation, which is about calculating theoretical values of listed companies and their stocks, for the purposes of share trading and investment management. 
This distinction extends to the use of the results: stock investors intend to profit from price movement, whereas a business owner is focused on the enterprise as a total, going concern.
A further distinction is re corporate finance: 
the transactions here are generally handled by a business broker; 
whereas where two corporates are involved, the transaction and valuation is within the realm of ""mergers and acquisitions"", and is handled by an investment bank; see Mergers and acquisitions § Business valuation and Corporate finance § Investment and project valuation.",3
Finance,Total value of ownership,"Total value of ownership (TVO) or total value of opportunity, is a methodology of measuring and analyzing the business value of IT investments. Gartner Group designed this methodology in 2003.TVO differs from total cost of ownership (TCO) in that TVO considers the benefits of alternative investments. It is a comparative measurement that evaluates the TCO and any additional benefits, such as the mobility of laptops when compared to desktop computers.",3
Finance,Transactional Asset Pricing Approach (TAPA),"In the valuation theory department of economics, the Transactional Asset Pricing Approach (TAPA) is a general reconstruction of asset pricing theory developed in 2000s by a collaboration of Russian and Israeli economists Vladimir B. Michaletz and Andrey I. Artemenkov. It provides a basis for reconstructing the discounted cash flow (DCF) analysis and the resulting income capitalization techniques, such as the Gordon growth formula (see dividend discount model ), from a transactional perspective relying, in the process, on a formulated dynamic principle of transactional equity-in-exchange.",3
Finance,Turnaround stock,A turnaround stock is a share with a high P/E ratio but low price-to-book ratio.,3
Finance,Undervalued stock,"An undervalued stock is defined as a stock that is selling at a price significantly below what is assumed to be its intrinsic value. For example, if a stock is selling for $50, but it is worth $100 based on predictable future cash flows, then it is an undervalued stock. The undervalued stock has the intrinsic value below the investment's true intrinsic value.  
Numerous popular books discuss undervalued stocks. Examples are The Intelligent Investor by Benjamin Graham, also known as ""The Dean of Wall Street,"" and The Warren Buffett Way by Robert Hagstrom. The Intelligent Investor puts forth Graham's principles that are based on mathematical calculations such as the price/earning ratio. He was less concerned with the qualitative aspects of a business such as the nature of a business, its growth potential and its management. For example, Amazon, Facebook, Netflix and Tesla in 2016, although they had a promising future, would not have appealed to Graham, since their price-earnings ratios were too high. Graham's ideas had a significant influence on the young Warren Buffett, who later became a famous US billionaire.

",3
Finance,Unicorn (finance),"In business, a unicorn is a privately held startup company valued at over US$1 billion.: 1270  The term was first popularised in 2013 by venture capitalist Aileen Lee, choosing the mythical animal to represent the statistical rarity of such successful ventures.According to CB Insights, there are more than 803 unicorns as of August 2021. The largest unicorns included ByteDance, SpaceX and Stripe. Also according to CB Insights, there are now 30 unicorns with over $10 billion valuation in the world, including SpaceX, Getir, Goto, J&T Express, Stripe, and Klarna. They have been given the name ""decacorn"".

",3
Finance,Valuation risk,"Valuation risk is the risk that an entity suffers a loss when trading an asset or a liability due to a difference between the accounting value and the price effectively obtained in the trade. 
In other words, valuation risk is the uncertainty about the difference between the value reported in the balance sheet for an asset or a liability and the price that the entity could obtain if it effectively sold the asset or transferred the liability (the so-called “exit price”). 
This risk is especially significant for financial instruments with complex features and limited liquidity, that are valued using internally developed pricing models. Valuation errors can result for instance from missing consideration of risk factors, inaccurate modeling of risk factors, or inaccurate modeling of the sensitivity of instrument prices to risk factors. Errors are more likely when models use inputs that are unobservable or for which little information is available, and when financial instruments are illiquid so that the accuracy of pricing models cannot be verified with regular market trades.",3
Finance,Valuation using discounted cash flows,"Valuation using discounted cash flows (DCF valuation) is a method of estimating the current value of a company based on projected future cash flows adjusted for the time value of money.
The cash flows are made up of those within the “explicit” forecast period, together with a continuing or terminal value that represents the cash flow stream after the forecast period.
In several contexts, DCF valuation is referred to as the ""income approach"".
Discounted cash flow valuation was used in industry as early as the 1700s or 1800s; it was explicated by John Burr Williams in his The Theory of Investment Value in 1938; it was widely discussed in financial economics in the 1960s; and became widely used in U.S. courts in the 1980s and 1990s.
This article details the mechanics of the valuation, via a worked example, including modifications typical for startups, private equity and venture capital, corporate finance ""projects"", and mergers and acquisitions.
See Discounted cash flow for further discussion, and Valuation (finance) § Valuation overview for context.",3
Finance,Valuation using multiples,"In economics, valuation using multiples, or “relative valuation”, is a process that consists of:

identifying comparable assets (the peer group) and obtaining market values for these assets.
converting these market values into standardized values relative to a key statistic, since the absolute prices cannot be compared. This process of standardizing creates valuation multiples.
applying the valuation multiple to the key statistic of the asset being valued, controlling for any differences between asset and the peer group that might affect the multiple.Multiples analysis is one of the oldest methods of analysis.  It was well understood in the 1800s and widely used by U.S. courts during the 20th century, although it has recently declined as Discounted Cash Flow and more direct market-based methods have become more popular.
""Comparable company analysis"",  closely related, was introduced by economists at Harvard Business School in the 1930's.",3
Finance,Valuation using the Market Penetration Model,"Valuation using the Market Penetration Model (MPM) or the growth potential of a company is a method of estimating the value of a company by calculating the depth of its market penetration as evidenced by its customer base and industry niche.
The process consists of:

profiling a company's type of customer and analysing which complementary companies share these customers.
valuing the barriers to entry into the industry niche that the company operates in.",3
Finance,Value added,"A value-added tax (VAT), known in some countries as a goods and services tax (GST), is a type of tax that is assessed incrementally. It is levied on the price of a product or service at each stage of production, distribution, or sale to the end consumer. If the ultimate consumer is a business that collects and pays to the government VAT on its products or services, it can reclaim the tax paid. It is similar to, and is often compared with, a sales tax. VAT is an indirect tax because the person who ultimately pays the tax is not necessarily the same person as the one who pays the tax to the tax authorities.
VAT essentially compensates for the shared service and infrastructure provided in a certain locality by a state and funded by its taxpayers that were used in the provision of that product or service. Not all localities require VAT to be charged, and exports are often exempt. VAT is usually implemented as a destination-based tax, where the tax rate is based on the location of the consumer and applied to the sales price. The terms VAT, GST, and the more general consumption tax are sometimes used interchangeably. VAT raises about a fifth of total tax revenues both worldwide and among the members of the Organisation for Economic Co-operation and Development (OECD).: 14  As of 2018, 166 of the 193 countries with full UN membership employ a VAT, including all OECD members except the United States,: 14  where many states use a sales tax system instead.
There are two main methods of calculating VAT: the credit-invoice or invoice-based method and the subtraction or accounts-based method. In the credit-invoice method, sales transactions are taxed, the customer is informed of the VAT on the transaction, and businesses may receive a credit for the VAT paid on input materials and services. The credit-invoice method is by far the more common and is used by all national VATs except for Japan. In the subtraction method, a business at the end of a reporting period, calculates the value of all taxable sales, subtracts the sum of all taxable purchases, and applies the VAT rate to the difference. The subtraction method VAT is currently used only by Japan although it, often by using the name ""flat tax,"" has been part of many recent tax reform proposals by US politicians. With both methods, there are exceptions in the calculation method for certain goods and transactions that are created to help collection or to counter tax fraud and evasion.",3
Finance,Value date,"Value date, in finance, is the date when the value of an asset that fluctuates in price is determined. The value date is used when there is a possibility for discrepancies due to differences in the timing of asset valuation. It usually applies to forward currency contracts, options and other derivatives, interest payable or receivable.
The value date can also mean:

the date when the entry to an account is considered effective in accounting.
the delivery date of funds traded in banking. For spot transactions it is the future date on which the trade is settled. In the case of a spot foreign exchange trade it is normally two days after a transaction is agreed upon.
the date the tax payment would coincide with the payment date in online banking, and retail payment gateways online.",3
Finance,Value investing,"Value investing is an investment paradigm that  involves buying securities that appear underpriced by some form of fundamental analysis. The various forms of value investing derive from the investment philosophy first taught by Benjamin Graham and David Dodd at Columbia Business School in 1928, and subsequently developed in their 1934 text Security Analysis.
The early value opportunities identified by Graham and Dodd included stock in public companies trading at discounts to book value or tangible book value, those with high dividend yields, and those having low price-to-earning multiples, or low price-to-book ratios.
High-profile proponents of value investing, including Berkshire Hathaway chairman Warren Buffett, have argued that the essence of value investing is buying stocks at less than their intrinsic value. The discount of the market price to the intrinsic value is what Benjamin Graham called the ""margin of safety"". For the last 25 years, under the influence of Charlie Munger, Buffett expanded the value investing concept with a focus on ""finding an outstanding company at a sensible price"" rather than generic companies at a bargain price. Hedge fund manager Seth Klarman has described value investing as rooted in a rejection of the efficient-market hypothesis (EMH). While the EMH proposes that securities are accurately priced based on all available data, value investing proposes that some equities are not accurately priced.Graham never used the phrase value investing – the term was coined later to help describe his ideas and has resulted in significant misinterpretation of his principles, the foremost being that Graham simply recommended cheap stocks. The Heilbrunn Center at Columbia Business School is the current home of the Value Investing Program.",3
Finance,Value of control,"The value of control is a quantitative measure of the value of controlling the outcome of an uncertain variable. Decision analysis provides a means for calculating the value of both perfect and imperfect control. The former value, informally known as the value of wizardry, is an upper bound for the latter. Obtaining meaningful value-of-control measurements requires an awareness of important restrictions (concerning the nature of free will and the meaning of counterfactual statements) on the validity of this kind of analysis.",3
Finance,Value of information,Value of information (VOI or VoI) is the amount a decision maker would be willing to pay for information prior to making a decision.,3
Finance,Value of life,"The value of life is an economic value used to quantify the benefit of avoiding a fatality. It is also referred to as the cost of life, value of preventing a fatality (VPF), implied cost of averting a fatality (ICAF), and value of a statistical life (VSL). In social and political sciences, it is the marginal cost of death prevention in a certain class of circumstances. In many studies the value also includes the quality of life, the expected life time remaining, as well as the earning potential of a given person especially for an after-the-fact payment in a wrongful death claim lawsuit.
As such, it is a statistical term, the cost of reducing the average number of deaths by one. It is an important issue in a wide range of disciplines including economics, health care, adoption, political economy, insurance, worker safety, environmental impact assessment, and globalization.The motivation for placing a monetary value on life is to enable policy and regulatory analysts to allocate the limited supply of resources, infrastructure, labor, and tax revenue. Estimates for the value of a life are used to compare the life-saving and risk-reduction benefits of new policies, regulations, and projects against a variety of other factors.Estimates for the statistical value of life are published and used in practice by various government agencies. In Western countries and other liberal democracies, estimates for the value of a statistical life typically range from US$1 million—US$10 million; for example, the United States FEMA estimated the value of a statistical life at US$7.5 million in 2020.",3
Finance,Value-in-use,"Use value (German: Gebrauchswert) or value in use is a concept in Marxist economics. It refers to the tangible features of a commodity (a tradeable object) which can satisfy some human requirement, want or need, or which serves a useful purpose. In Karl Marx's critique of political economy, any product has a labor-value and a use-value, and if it is traded as a commodity in markets, it additionally has an exchange value, most often expressed as a money-price.Marx acknowledges that commodities being traded also have a general utility, implied by the fact that people want them, but he argues that this by itself says nothing about the specific character of the economy in which they are produced and sold.",3
Finance,Accountant,"Accounting, also known as accountancy, is the measurement, processing, and communication of financial and non financial information about economic entities such as businesses and corporations. Accounting, which has been called the ""language of business"", measures the results of an organization's economic activities and conveys this information to a variety of stakeholders, including investors, creditors, management, and regulators.  Practitioners of accounting are known as accountants. The terms ""accounting"" and ""financial reporting"" are often used as synonyms.
Accounting can be divided into several fields including financial accounting, management accounting, tax accounting and cost accounting. Financial accounting focuses on the reporting of an organization's financial information, including the preparation of financial statements, to the external users of the information, such as investors, regulators and suppliers; and management accounting focuses on the measurement, analysis and reporting of information for internal use by management. The recording of financial transactions, so that summaries of the financials may be presented in financial reports, is known as bookkeeping, of which double-entry bookkeeping is the most common system. Accounting information systems are designed to support accounting functions and related activities.
Accounting has existed in various forms and levels of sophistication throughout human history. The double-entry accounting system in use today was developed in medieval Europe, particularly in Venice, and is usually attributed to the Italian mathematician and Franciscan friar Luca Pacioli.  Today, accounting is facilitated by accounting organizations such as standard-setters, accounting firms and professional bodies. Financial statements are usually audited by accounting firms, and are prepared in accordance with generally accepted accounting principles (GAAP). GAAP is set by various standard-setting organizations such as the Financial Accounting Standards Board (FASB) in the United States and the Financial Reporting Council in the United Kingdom. As of 2012, ""all major economies"" have plans to converge towards or adopt the International Financial Reporting Standards (IFRS).",3
Finance,Consignment,"Consignment involves selling one's personal goods (clothing, furniture, etc.) through a third-party vendor such as a consignment store or online thrift store. The owner of the goods pays the third-party a portion of the sale for facilitating the sale. Consignors maintain the rights to their property until the item is sold or abandoned. Many consignment shops and online consignment platforms have a set day limit before an item expires for sale (usually 60–90 days).
The consignment stock is stock legally owned by one party, but held by another, meaning that the risk and rewards regarding to the said stock remains with the first party while the second party is responsible for distribution or retail operations.The verb ""consign"" means ""to send"" and therefore the noun ""consignment"" means ""sending goods to another person"". In the case of ""retail consignment"" or ""sales consignment"" (often just referred to as a ""consignment""), goods are sent to the agent for the purpose of sale. The ownership of these goods remains with the sender. The agent sells the goods on behalf of the sender according to instructions. The sender of goods is known as the ""consignor"" and the agent entrusted with the custody and care of the goods is known as the ""consignee"".",3
Finance,Outline of accounting,"The following outline is provided as an overview of and topical guide to accounting:
Accounting – measurement, statement or provision of assurance about financial information primarily used by managers, investors, tax authorities and other decision makers to make resource allocation decisions within companies, organizations, and public agencies. The terms derive from the use of financial accounts.",3
Finance,Index of accounting articles,"This page is an index of accounting topics.

",3
Finance,Accounting entity,"An Accounting Entity is simply an Entity for which accounting records are to be kept.
The main requirements for something to be considered an ""accounting entity"" are:

It can own property the value of which can be measured in financial terms
It can incur debts or liabilities which can also be measured in financial terms
It can therefore be assigned a value for its net worth or solvency which is the difference between the twoExamples of accounting entities include corporations, clubs, trusts, partnerships and individuals.",3
Finance,Accounting reform,"Accounting reform is an expansion of accounting rules that goes beyond financial measures for both individual economic entities and national economies.  It is advocated by those who consider the focus of the current standards and practices wholly inadequate to the task of measuring and reporting the activity, success, and failure of the modern enterprise, including government.
The real debate concerns concepts such as whether to report transactions, such as asset acquisitions, at their cost or their current market values. The former, traditional approach, appeals for its reliability but can quickly lose its relevance due to inflation and other factors; the latter, an increasingly common approach, is appealing for its relevance but is less reliable due to the need to use subjective measures. Accounting standards setters such as the International Accounting Standards Board attempt to balance relevance and reliability.",3
Finance,Audit technology,"An information technology audit, or information systems audit, is an examination of the management controls within an Information technology (IT) infrastructure and business applications. The evaluation of evidence obtained determines if the information systems are safeguarding assets, maintaining data integrity, and operating effectively to achieve the organization's goals or objectives. These reviews may be performed in conjunction with a financial statement audit, internal audit, or other form of attestation engagement.
IT audits are also known as automated data processing audits (ADP audits) and computer audits.
They were formerly called electronic data processing audits (EDP audits).

",3
Finance,BAS (accounting),"The Swedish BAS chart of accounts (Basic chart), represents the Swedish accounting generally accepted accounting principles (GAAP) and is an open to use chart of accounts for accounting in Sweden available in Swedish, English and German language texts. Very similar chart of accounts are commonly used in neighbouring countries and applicable internationally.
It was developed by the Swedish industry in the 1960s and is today owned by its own foundation. The BAS chart was reformed in the late 1990s to adapt the IFRS chart layout directives and is known then as BAS2000, but today in general as just the BAS chart.
A nested group of organisations with BAS in the center is forming a common Swedish GAAP and common routines in accounting and domestic reporting standards.",3
Finance,Bookkeeping,"Bookkeeping is the recording of financial transactions, and is part of the process of accounting in business and other organizations. It involves preparing source documents for all transactions, operations, and other events of a business. Transactions include purchases, sales, receipts and payments by an individual person or an organization/corporation. There are several standard methods of bookkeeping, including the single-entry and double-entry bookkeeping systems. While these may be viewed as ""real"" bookkeeping, any process for recording financial transactions is a bookkeeping process.
The person in an organisation who is employed to perform bookkeeping functions is usually called the bookkeeper (or book-keeper). They usually write the daybooks (which contain records of sales, purchases, receipts, and payments), and document each financial transaction, whether cash or credit, into the correct daybook—that is, petty cash book, suppliers ledger, customer ledger, etc.—and the general ledger. Thereafter, an accountant can create financial reports from the information recorded by the bookkeeper. The bookkeeper brings the books to the trial balance stage, from which an accountant may prepare financial reports  for the organisation, such as the income statement and balance sheet.",3
Finance,Continuous accounting,"Continuous accounting is an approach to managing the accounting cycle that can be the key to achieving a more strategic finance and accounting function in a corporation. It is designed to be a practical approach that addresses the tactical issues that prevent finance departments from being more strategic. Continuous accounting embraces three main principles:
The first is the need to automate mechanical, repetitive accounting processes in a continuous, end-to-end fashion. Managing processes in a controlled, end-to-end fashion improves efficiency and it ensures data integrity. Ensuring data integrity is crucial because the absence of data integrity is the root cause of a lot of time-consuming work that departments perform that adds little value to the rest of the company.The second is optimizing the accounting calendar by distributing workloads continuously and evenly over the accounting period (the month, quarter or year) to eliminate bottlenecks and optimize when and in which order accounting tasks are performed.The third is establishing a culture of continuous improvement in managing the accounting cycle. A continuous improvement culture sets increasingly rigorous objectives, reviews performance to those objectives frequently and makes addressing performance shortcomings a departmental priority.Continuous accounting requires the effective use of information (data) and technology (software) to automate accounting and finance processes in an end-to-end fashion. This does not mean that every step is entirely hands-off, but processes are designed to automate the execution of as many steps as is practical. It also involves automating all the hand-offs that take place between people in performing the process as well as any managing all administrative and approval steps that must take place. End-to-end automation has two objectives. One is to cut down on the time employees spend doing work that computers can do faster and more consistently. The second is to improve the quality of the data in a company's financial records to ensure greater accuracy, consistency and control of financial data. In this respect, continuous accounting supports the COSO Framework.
Automation improves data quality because eliminating human intervention in the handling of data substantially reduces the potential for errors in a well-controlled IT environment. Consequently, by design, end-to-end financial process automation strengthens financial controls and facilitates auditing. Spreadsheets and manual calculations create the need for time-consuming reconciliations and checks to spot errors and identify their source.  
An important design objective for end-to-end automation is to ensure that there is a single authoritative source for the data that is used in accounting processes. Technology exists to ensures that even with multiple accounting and financial management systems and data stores, there is no duplication of data that requires checks and reconciliations.
To second component of continuous accounting focuses on optimizing workloads. Increasingly, financial software gives companies greater flexibility. The classic accounting calendar with its monthly, quarterly and annual cycles developed over centuries as a practical approach to dealing with the limitations of the paper based systems and manual calculations. Their frequency represents a trade-off aimed at balancing efficiency and control: waiting until there are enough entries to justify performing the work, but not waiting too long to identify fraud, procedural issues and accounting errors. The monthly accounting close developed for many accounting tasks because a weekly cadence proved to be too short a period to be efficient while quarterly was too long to wait. Other accounting processes are typically handled on a quarterly or semi-annual basis because this longer frequency reflects a more appropriate trade-off of efficiency and control.
Even when computers were first used to automate bookkeeping and accounting, the old accounting calendars persisted. The limitations of early business computing technology forced system designers to use batch processing. From an accounting process standpoint, batch systems do not offer much of an improvement over paper-based ones in changing the timing of finance and accounting processes. They impose the same need to balance efficiency and control in taking computing systems off-line to handle computations and summarizations in a way that does not interfere with transactions processing. Only within the past decade has information technology reached a performance threshold that eliminates the need for such batch processing.  Corporations that use batch-less accounting systems have greater freedom to schedule their accounting cycle.  
The third principle of continuous accounting is continuous (or continual) improvement. Decades ago, continuous improvement revolutionized manufacturing worldwide and culture of manufacturing departments. Continuous improvement can be used as a management approach to overcome organizational inertia. This is particularly difficult in accounting departments because accounting is a discipline that requires consistency, which is necessary for the faithful presentation of a company's financial performance and health. But performing accounting processes the same way may not be the best way.
Continuous improvement requires ongoing assessments to identify issues and how to address them, as well as a mindset that accepts that these sorts of changes will be made.",3
Finance,Customer Profitability Analysis,"Customer Profitability Analysis (in short CPA) is a management accounting and a credit underwriting method, allowing businesses and lenders to determine the profitability of each customer or segments of customers, by attributing profits and costs to each customer separately. CPA can be applied at the individual customer level (more time consuming, but providing a better understanding of business situation) or at the level of customer aggregates / groups (e.g. grouped by number of transactions, revenues, average transaction size, time since starting business with the customer, distribution channels, etc.).
CPA is a ""retrospective"" method, which means it analyses past events of different customers, in order to calculate customer profitability for each customer. Equally, research suggests that that credit score does not necessarily impact the lenders' profitability.

",3
Finance,Data valuation,"In computer science, data validation is the process of ensuring data has undergone data cleansing to ensure they have data quality, that is, that they are both correct and useful. It uses routines, often called ""validation rules"", ""validation constraints"", or ""check routines"", that check for correctness, meaningfulness, and security of data that are input to the system. The rules may be implemented through the automated facilities of a data dictionary, or by the inclusion of explicit application program validation logic of the computer and its application.
This is distinct from formal verification, which attempts to prove or disprove the correctness of algorithms for implementing a specification or property.",3
Finance,Depreciation,"In several fields, especially computing, deprecation is the discouragement of use of some terminology, feature, design, or practice, typically because it has been superseded or is no longer considered efficient or safe, without completely removing it or prohibiting its use. Typically, deprecated materials are not completely removed to ensure legacy compatibility or back up practice in case new methods are not functional in an odd scenario.
It can also imply that a feature, design, or practice will be removed or discontinued entirely in the future.",3
Finance,Direct costs,"Direct costs are costs which are directly accountable to a cost object (such as a particular project, facility, function or product). Direct cost is the nomenclature used in accounting. The equivalent nomenclature in economics is specific cost. By contrast, a joint cost is a cost incurred in the production or delivery of multiple products or product lines. For instance, in civil aviation, substantial costs of a flight (pilots, fuel, wear and tear on the plane, landing and takeoff fees) are a joint cost between carrying passengers and carrying freight, and underlie economies of scope across passenger and freight services. By contrast, some costs are specific to the services, for instance, meals and flight attendants are specific costs of carrying passengers.
Direct costs are directly attributable to the object. In construction, the costs of materials, labor, equipment, etc., and all directly involved efforts or expenses for the cost object are direct costs. In manufacturing or other non-construction industries, the portion of operating costs which is directly assignable to a specific product or process is a direct cost. Direct costs are those for activities or services that benefit specific projects, for example salaries for project staff and materials required for a particular project. Because these activities are easily traced to projects, their costs are usually charged to projects on an item-by-item basis.
The economics term for the same concept is specific cost.Direct costs typically include:

Direct materials used in manufacturing
Direct labour
Direct expenses, e.g. a royalty payment to a patent holder for a specific production process",3
Finance,DIRTI 5,"In accounting and economics the DIRTI 5 is an acronym for ""Depreciation, Interest, Repairs, Taxes, and Insurance"". Total fixed cost includes the DIRTI 5, which are unavoidable for any capital asset of significant value.",3
Finance,Equivalence number method,"The equivalence number method is a cost calculation method for co-production in cost and activity accounting. The resulting costs of the input factors are allocated to the individual products according to a weighting key, the so-called equivalence numbers.",3
Finance,Financial close management,"Financial close management (FCM) is a recurring process in management accounting by which accounting teams verify and adjust account balances at the end of a designated period in order to produce financial reports representative of the company's true financial position to inform stakeholders such as management, investors, lenders, and regulatory agencies.

",3
Finance,Fiscal pedaling,"Fiscal pedaling (a calque from Brazilian Portuguese: pedalada fiscal, or simply pedaladas) is a governmental creative accounting technique involving the use of state-owned banks to front funds required for paying general government obligations without officially declaring a loan, thus hiding these transfers from public scrutiny and delaying repayment from the Treasury to these banks.  As such it is a kind of ""overdraft"" implying a positive balance sheet that does not really exist.  Sometimes the term fiscal backpedaling is used.The term gained popularity with the Brazilian Presidential election of 2014, in which President Rousseff was reelected. She was later accused of fiscal pedaling during the campaign, for allowing this delay in repayment to government-owned banks by the Tesouro Nacional (Brazilian Treasury) which is also the entity which oversees these banks. Her opponents argued that this amounted to undeclared loans by these banks to the Treasury, which is prohibited by the Brazilian Constitution and a violation of the Fiscal Responsibility Law.  This led to the impeachment of Dilma Rousseff beginning in 2015 on multiple charges including fiscal pedaling, and her subsequent removal from office a year later..
One possible motivation for fiscal pedaling is political advantage, in that it permits a government to conceal the true extent of its fiscal obligations during a political campaign. President Rousseff's government was accused of using these accounting techniques during the bitterly fought campaign of 2014, with the funding thus obtained allegedly used to support programs for the poor, credits to farmers, and subsidies for low-income housing. Supporters claimed this was standard practice in Brazil and had been engaged in by previous Presidents, and that the opposition to her was purely political.",3
Finance,Flotation cost,"Flotation cost is the total cost incurred by a company in offering its securities to the public. They arise from expenses such as underwriting fees, legal fees and registration fees. Firms are well advised to consider the magnitude of these fees as they also impact how much capital they can raise from an initial public offering. The more is the flotation cost the less viable is the source.",3
Finance,General account,"A general account generally refers to the combined or aggregate investments and other assets of an insurance company available to pay claims and benefits to which insured entities or policyholders are entitled.  The general account may also be considered everything that is not represented by a separate accounts of the firm, if such separate account has been established by the company.  Should a firm have no separate accounts, then its only account is the general account.  The term should not be thought of narrowly in terms of a bank account or general ledger account, but rather the broader concept introduced in the first sentence.
Policyholders of insurance policies (that are not associated with separate accounts) do not have a legal or other direct interest or right to the assets or investments of the insurance company's general account but rather these obligations for benefits or claims are general obligations of the company.  In this case, policyholders are subject to credit risk of the insurance company-- that is, should the insurance company fail or go bankrupt, the claims or cash values of policies are not directly backed or collateralized by the company's investments and other assets.  In the U.S., state insurance departments examine (audit) insurance companies to evaluate many things, principally to see if the company is sound and policyholder interests are protected. A.M. Best is an example of an insurance rating agency who evaluates and rates many companies on various factors such as financial strength, claims-paying and other policyholder servicing experiences.",3
Finance,Interaction cost,"Interaction cost can comprise work, costs, and other expenses, required to complete a task or interaction. This applies to several categories, including:

Economy: the interaction cost of a purchase includes the requirements to complete it, and differ in costs for customers and vendors. Method of payment offered may factor into both transaction cost and interaction cost. Reducing steps for customers can be a service offered by the vendor. Interaction cost should be considered when clients choose vendors. Customers prefer to have choice about their interactions cost. In self-checkout, work is moved to the customer.
Politics: Specific interaction cost can be increased by law for political gains.
User interface: In a computer menu with a graphical user interface, some designs require more clicks from the user in order to make a selection. With a dropdown menu, one click (or touch or hover) may reveal a hidden menu (sub menu), with a second click required to select the menu option. If the entire menu were displayed all along, as in a navigation bar, only one click would be required, but the menu would occupy more screen space. Sub-menus require even more care from the user to make a desired selection.

",3
Finance,International Ethics Standards Board for Accountants,The International Ethics Standards Board for Accountants (IESBA) develops and promotes the International Code of Ethics for Professional Accountants (including International Independence Standards). The IESBA also supports debate on issues related to accounting ethics and auditor independence.,3
Finance,Maker-checker,"Maker-checker (or Maker and Checker, or 4-Eyes) is one of the central principles of authorization in the information systems of financial organizations. The principle of maker and checker means that for each transaction, there must be at least two individuals necessary for its completion. While one individual may create a transaction, the other individual should be involved in confirmation/authorization of the same. Here the segregation of duties play an important role. In this way, strict control is kept over system software and data, keeping in mind functional division of labor between all classes of employees.",3
Finance,Multilateral exchange,"A multilateral exchange is a transaction, or forum for transactions, which involve more than two parties.
For example, Alice gives Bob an apple in exchange for an orange, that is a bilateral exchange.
A multilateral exchange would involve a third party, for example:
Alice gives an apple to Bob who gives an orange to Charles, who gives a pear to Alice.
In the real world, such transactions are spread over time, and involved items of different values, and involve many more parties. A special type of accounting is used for this, called mutual credit, or credit clearing.",3
Finance,OpenTuition,"OpenTuition.com is an online learning site, providing free online training in accountancy and financial services. Founded by John Moffat in 2008, it is based in Riga, Latvia.
OpenTuition has over 500,000 registered students both in the UK and overseas who are studying for the professional accountancy qualifications:  ACCA,  CIMA  and AAT.OpenTuition publishes free electronic text books and streams free lectures for the following ACCA examinations: Fundamental Level: BT  Business and Technology, MA Management Accounting, FA Financial Accounting, LW Corporate and Business Law, PM Performance Management, TX Taxation, FR Financial Reporting, AA Audit and Assurance, and FM Financial Management.
The Professional level: Essentials (compulsory) exams: SBL Strategic Business Leader; SBR Strategic Business Reporting; Options (two papers required): AFM Advanced Financial Management; APM Advanced Performance Management; ATX Advanced Taxation; AAA Advanced Audit and Assurance.
OpenTuition is a registered CIMA tuition provider, free e-books, tests and lectures are published for the CIMA Certificate in Business Accounting and CIMA Professional Qualification.
OpenTuition study resources include such subjects as: financial accounting, management accounting, financial reporting, taxation, company law, audit and assurance and financial management.
OpenTuition have discussion forums for all ACCA papers, OBU, CIMA, FIA, AAT and others.
OpenTuition received international recognition among accountancy professionals in London, winning two prizes, the first in 2010 as the best accountancy learning site and in 2011 for the best accountancy study resource.

",3
Finance,Accounting period,"An accounting period, in bookkeeping, is the period with reference to which management accounts and financial statements are prepared. 
In management accounting the accounting period varies widely and is determined by management. Monthly accounting periods are common.
In financial accounting the accounting period is determined by regulation and is usually 12 months. The beginning of the accounting period differs according to jurisdiction. For example, one entity may follow the calendar year, January to December, while another may follow April to March as the accounting period.
The International Financial Reporting Standards allow a period of 52 weeks as an accounting period instead of 12 months.  This method is known as the 4-4-5 calendar in British and Commonwealth usage and the 52–53-week fiscal year in the United States.  In the United States the method is permitted by generally accepted accounting principles, as well as by US Internal Revenue Code Regulation 1.441-2  (IRS Publication 538).In some of the ERP tools there are more than 12 accounting periods in a financial year. They put one accounting period as ""Year Open"" period where all the carried over balances from last financial year are cleared and one period as ""Year Close"" where all the transactions for closed for the same financial year. Older systems sometimes called these periods ""Month 0"" and ""Month 13"".",3
Finance,Risk assurance,"Risk assurance is often associated with accounting practices and is a growing industry whereby internal processes are developed to create a ""checks and balances"" system. These checks predominantly identify differences between risk appetite and real risk .Business risk refers to factors that can affect the company, both internally and externally. There are various types of business risks: strategic, compliance, financial and operational.  Risk assurance aims to mitigate any of these areas. As such, companies can pre-analyse the industry to scout for potential risks or if a risk has already occurred, managers can analyse the problem in an attempt to mitigate the effects. 
Risk assurance involves tiers of internal processes including management and internal controls, financial control and security, inspection, compliance, internal audit and leadership teams that are aware of the companies internal and external risks. Following internal processes, assurance requires an external audit team who examines the internal processes effectiveness and reports to senior management with successes and areas for redevelopment.Auditors in risk assurance auditing filter information technology general controls (ITGCs) and completing a system and organisation control (SOC 1) report.Internal control is a large component of risk assurance whereby an entity's management design processes to provide reasonable assurance regarding the achievement of operational objectives, reporting and compliance. 
Internal control's 5 components include:
1.     Control environment
2.     Risk assessment
3.     Control activities
4.     Information and communication
5.     Monitoring activitiesPhysical internal control are accounting procedures that prevent fraud and ensure operational efficiency such as CCTV, passwords, and security locks. Internal audits are another internal control and play a role in corporate governance. These audits evaluate the effectiveness of a businesses' internal control. Another internal control is having different employees delegated to different tasks in a transaction.",3
Finance,Daniel Roche,"Daniel Peter Roche ( ROHSH; born 14 October 1999) is an English actor, known for playing Ben Brockman in the BBC One sitcom Outnumbered.

",3
Finance,Shrinkage (accounting),"In accounting, inventory shrinkage (sometimes shortened to shrinkage or shrink) occurs when a retailer has fewer items in stock than in the inventory list due to clerical error, goods being damaged, lost, or stolen between the point of manufacture (or purchase from a supplier) and the point of sale. This affects profit: if shrinkage is large, profits decrease. This leads retailers to increase prices to make up for losses, passing the cost of shrinkage onto customers.In 2008, the retail industry in the United States experienced shrinkage rates of around 1.52% of sales. During the same year, retailers in Europe and Asia Pacific reported average shrinkage of about 1.27% and 1.20% of sales, respectively.",3
Finance,Sum of Digits depreciation,"In accountancy, depreciation refers to two aspects of the same concept: first, the actual decrease of fair value of an asset, such as the decrease in value of factory equipment each year as it is used and wear, and second, the allocation in accounting statements of the original cost of the assets to periods in which the assets are used (depreciation with the matching principle).Depreciation is thus the decrease in the value of assets and the method used to reallocate, or ""write down"" the cost of a tangible asset (such as equipment) over its useful life span. Businesses depreciate long-term assets for both accounting and tax purposes. The decrease in value of the asset affects the balance sheet of a business or entity, and the method of depreciating the asset, accounting-wise, affects the net income, and thus the income statement that they report. Generally, the cost is allocated as depreciation expense among the periods in which the asset is expected to be used.
Methods of computing depreciation, and the periods over which assets are depreciated, may vary between asset types within the same business and may vary for tax purposes. These may be specified by law or accounting standards, which may vary by country. There are several standard methods of computing depreciation expense, including fixed percentage, straight line, and declining balance methods. Depreciation expense generally begins when the asset is placed in service. For example, a depreciation expense of 100 per year for five years may be recognized for an asset costing 500.
Depreciation has been defined as the diminution in the utility or value of an asset and is a non-cash expense. It does not result in any cash outflow; it just means that the asset is not worth as much as it used to be. Causes of depreciation are natural wear and tear.",3
Finance,Tone at the top,"""Tone at the top"" is a term that originated in the field of accounting and is used to describe an organization's general ethical climate, as established by its board of directors, audit committee, and senior management. Having good tone at the top is believed by business ethics experts to help prevent fraud and other unethical practices. The very same idea is expressed in negative terms by the old saying ""A fish rots from the head down"".",3
Technology,AUA Framework,"AUA framework Software projects is require constant changes and updates.This software was made in 2018 in Iran and Tehran.

",2
Technology,Software engineering,"Software engineering is a systematic engineering approach to software development.A software engineer is a person who applies the principles of software engineering to design, develop, maintain, test, and evaluate computer software. The term programmer is sometimes used as a synonym, but may also lack connotations of engineering education or skills.
Engineering techniques are used to inform the software development process which involves the definition, implementation, assessment, measurement, management, change, and improvement of the software life cycle process itself. It heavily uses software configuration management which is about systematically controlling changes to the configuration, and maintaining the integrity and traceability of the configuration and code throughout the system life cycle. Modern processes use software versioning.",2
Technology,Outline of software engineering,"The following outline is provided as an overview of and topical guide to software engineering:
Software engineering – application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software; that is the application of engineering to software.The ACM Computing Classification system is a poly-hierarchical ontology that organizes the topics of the field and can be used in semantic web applications and as a defacto standard classification system for the field.   The major section ""Software and its Engineering"" provides an outline and ontology for software engineering.",2
Technology,Index of software engineering articles,This is an alphabetical list of articles pertaining specifically to software engineering.,2
Technology,Agile software development,"In software development, agile (sometimes written Agile) practices include requirements discovery and solutions improvement through the collaborative effort of self-organizing and cross-functional teams with their customer(s)/end user(s), adaptive planning, evolutionary development, early delivery, continual improvement, and flexible responses to changes in requirements, capacity, and understanding of the problems to be solved.Popularized in the 2001 Manifesto for Agile Software Development, these values and principles were derived from and underpin a broad range of software development frameworks, including Scrum and Kanban.While there is much anecdotal evidence that adopting agile practices and values improves the effectiveness of software professionals, teams and organizations, the empirical evidence is mixed and hard to find.",2
Technology,Brownout (software engineering),Brownout in software engineering is a technique that involves disabling certain features of an application.,2
Technology,Certified Software Development Professional,"Certified Software Development Professional (CSDP) is a vendor-neutral professional certification in software engineering developed by the IEEE Computer Society for experienced software engineering professionals. This certification was offered globally since 2001 through Dec. 2014.
The certification program constituted an element of the Computer Society's major efforts in the area of Software engineering professionalism, along with the IEEE-CS and ACM Software Engineering 2004 (SE2004) Undergraduate Curricula Recommendations, and The Guide to the Software Engineering Body of Knowledge (SWEBOK Guide 2004), completed two years later.
As a further development of these elements, to facilitate the global portability of the software engineering certification, since 2005 through 2008 the International Standard ISO/IEC 24773:2008 ""Software engineering -- Certification of software engineering professionals -- Comparison framework""

has been developed. (Please, see an overview of this ISO/IEC JTC1 and IEEE standardization effort in the article published by Stephen B. Seidman, CSDP.

) The standard was formulated in such a way, that it allowed to recognize the CSDP certification scheme as basically aligned with it, soon after the standard's release date, 2008-09-01. Several later revisions of the CSDP certification were undertaken with the aim of making the alignment more complete. In 2019, ISO/IEC 24773:2008 has been withdrawn and revised (by ISO/IEC 24773-1:2019 ).
The certification was initially offered by the IEEE Computer Society to experienced software engineering and software development practitioners globally in 2001 in the course of the certification examination beta-testing. The CSDP certification program has been officially approved in 2002.

After December 2014 this certification program has been discontinued, all issued certificates are recognized as valid forever.
 
A number of new similar certifications were introduced by the IEEE Computer Society, including the Professional Software Engineering Master (PSEM) and Professional Software Engineering Process Master (PSEPM) Certifications (the later soon discontinued).
To become a Certified Software Development Professional (CSDP) candidates had to have four years (initially six years) of professional software engineering experience, pass a three-and-half-hour, 180-question examination on various knowledge areas of software engineering, and possess at least a bachelor's degree in Computer Science or Software Engineering. The CSDP examination tested candidates' proficiency in internationally accepted, industry-standard software engineering principles and practices. CSDP credential holders are also obligated to adhere to the IEEE/ACM's Software Engineering Code of Ethics and Professional Practice. As of 2021, the IEEE-CS offer which is a successor to CSDP is the Professional Software Engineering Master (PSEM) certification. The exam is three hours, is proctored remotely, and consists of 160 questions over the 11 SWEBOK knowledge areas: Software Requirements, Software Design, Software Construction, Software Testing, Software Maintenance, Software Configuration Management, Software Engineering Management, Software Engineering Process, Software Engineering Models and Methods, Software Quality, Software Engineering Economics.(There is also the Professional Software Developer (PSD) certification, which covers only 4 knowledge areas: software requirements, software design, software construction, and software testing. The similarity of the name of this certification to the CSDP is confusing, it is a reputable credential but NOT an equivalent of CSDP.)

",2
Technology,Empirical software engineering,"Empirical software engineering (ESE) is a subfield of software engineering (SE) research that uses empirical research methods to study and evaluate an SE phenomenon of interest. The phenomenon may refer to software development tools/technology, practices, processes, policies, or other human and organizational aspects.
ESE has roots in experimental software engineering, but as the field has matured the need and acceptance for both quantitative and qualitative research has grown. Today, common research methods used in ESE for primary and secondary research are the following:
Primary research (experimentation, case study research, survey research, simulations in particular software Process simulation)
Secondary research methods (Systematic reviews, Systematic mapping studies, rapid reviews, tertiary review)

",2
Technology,History of software engineering,"The history of software engineering begins in the 1960s. Writing software has evolved into a profession concerned with how best to maximize the quality of software and of how to create it. Quality can refer to how maintainable software is, to its stability, speed, usability, testability, readability, size, cost, security, and number of flaws or ""bugs"", as well as to less measurable qualities like elegance, conciseness, and customer satisfaction, among many other attributes. How best to create high quality software is a separate and controversial problem covering software design principles, so-called ""best practices"" for writing code, as well as broader management issues such as optimal team size, process, how best to deliver software on time and as quickly as possible, work-place ""culture"", hiring practices, and so forth. All this falls under the broad rubric of software engineering.",2
Technology,Mining software repositories,"Within software engineering, the mining software repositories (MSR) field  analyzes the rich data available in software repositories, such as version control repositories, mailing list archives, bug tracking systems, issue tracking systems, etc.  to uncover interesting and actionable information about software systems, projects and software engineering.

",2
Technology,Mixed criticality,"A mixed criticality system is a system containing computer hardware and software that can execute several applications of different criticality, such as safety-critical and non-safety critical, or of different Safety Integrity Level (SIL). Different criticality applications are engineered to different levels of assurance, with high criticality applications being the most costly to design and verify. These kinds of systems are typically embedded in a machine such as an aircraft whose safety must be ensured.",2
Technology,Allison Parrish,"Allison Parrish is an American poet, software engineer, creative coder, and game designer, notable as one of the most prominent early makers of creative, literary Twitter bots. She was named ""Best Maker of Poetry Bots"" by The Village Voice in 2016. Parrish has produced a textbook introduction to creative coding in Python, more specifically Processing.py. Parrish holds a BA in Linguistics from UC Berkeley, and a Master of Professional Studies from the Interactive Telecommunications Program (ITP), NYU. She has been a Writer-in-Residence in the English Department of Fordham University, 2014–16, and an Assistant Arts Professor at the ITP since 2016.",2
Technology,Protocol engineering,"Protocol engineering is the application of systematic methods to the development of communication protocols. It uses many of the principles of software engineering, but it is specific to the development of distributed systems.",2
Technology,Research software engineering,"Research software engineering is the use of software engineering practices in research applications. The term was proposed in a research paper in 2010 in response to an empirical survey on tools used for software development in research projects. It started to be used in United Kingdom in 2012, when it was needed to define the type of software development needed in research. This focuses on reproducibility, reusability, and accuracy  of data analysis and applications created for research.",2
Technology,Service-oriented software engineering,"Service-oriented Software Engineering (SOSE), also referred to as service engineering, is a software engineering methodology focused on the development of software systems by composition of reusable services (service-orientation) often provided by other service providers. Since it involves composition, it shares many characteristics of component-based software engineering, the composition of software systems from reusable components, but it adds the ability to dynamically locate necessary services at run-time. These services may be provided by others as web services, but the essential element is the dynamic nature of the connection between the service users and the service providers.",2
Technology,Site reliability engineering,"Site reliability engineering (SRE) is a set of principles and practices that incorporates aspects of software engineering and applies them to infrastructure and operations problems. The main goals are to create scalable and highly reliable software systems. Site reliability engineering is closely related to DevOps, a set of practices that combine software development and IT operations, and SRE has also been described as a specific implementation of DevOps.",2
Technology,Social software engineering,"Social software engineering (SSE) is a branch of software engineering that is concerned with the social aspects of software development and the developed software.
SSE focuses on the socialness of both software engineering and developed software. On the one hand, the consideration of social factors in software engineering activities, processes and CASE tools is deemed to be useful to improve the quality of both development process and produced software. Examples include the role of situational awareness and multi-cultural factors in collaborative software development. On the other hand, the dynamicity of the social contexts in which software could operate (e.g., in a cloud environment) calls for engineering social adaptability as a runtime iterative activity. Examples include approaches which enable software to gather users' quality feedback and use it to adapt autonomously or semi-autonomously.
SSE studies and builds socially-oriented tools to support collaboration and knowledge sharing in software engineering. SSE also investigates the adaptability of software to the dynamic social contexts in which it could operate and the involvement of clients and end-users in shaping software adaptation decisions at runtime. Social context includes norms, culture, roles and responsibilities, stakeholder's goals and interdependencies, end-users perception of the quality and appropriateness of each software behaviour, etc.
The participants of the 1st International Workshop on Social Software Engineering and Applications (SoSEA 2008) proposed the following characterization:

Community-centered: Software is produced and consumed by and/or for a community rather than focusing on individuals
Collaboration/collectiveness: Exploiting the collaborative and collective capacity of human beings
Companionship/relationship: Making explicit the various associations among people
Human/social activities: Software is designed consciously to support human activities and to address social problems
Social inclusion: Software should enable social inclusion enforcing links and trust in communitiesThus, SSE can be defined as ""the application of processes, methods, and tools to enable community-driven creation, management, deployment, and use of software in online environments"".One of the main observations in the field of SSE is that the concepts, principles, and technologies made for social software applications are applicable to software development itself as software engineering is inherently a social activity. SSE is not limited to specific activities of software development. Accordingly, tools have been proposed supporting different parts of SSE, for instance, social system design or social requirements engineering. 
Consequently vertical market software, such as software development tools, engineering tools, marketing tools or software that helps users in a decision making process can profit from social components. Such vertical social software differentiates strongly in its user-base from traditional social software such as Yammer.

",2
Technology,Software bot,"A software bot is a type of software agent in the service of software project management and software engineering. A software bot has an identity and potentially personified aspects in order to serve their stakeholders. Software bots often compose software services and provide an alternative user interface, which is sometimes, but not necessarily conversational.
Software bots are typically used to execute tasks, suggest actions, engage in dialogue, and promote social and cultural aspects of a software project.The term bot is derived from robot. However, robots act in the physical world and software bots act only in digital spaces. Some software bots are designed and behave as chatbots, but not all chatbots are software bots.  Erlenhov et al. discuss the past and future of software bots and show that software bots have been adopted for many years.",2
Technology,Software configuration management,"In software engineering, software configuration management (SCM or S/W CM) is the task of tracking and controlling changes in the software, part of the larger cross-disciplinary field of configuration management.  SCM practices include revision control and the establishment of baselines.  If something goes wrong, SCM can determine what was changed and who changed it.  If a configuration is working well, SCM can determine how to replicate it across many hosts.
The acronym ""SCM"" is also expanded as source configuration management process and software change and configuration management.  However, ""configuration"" is generally understood to cover changes typically made by a system administrator.",2
Technology,Software construction,"Software construction is a software engineering discipline. It is the detailed creation of working meaningful software through a combination of coding, verification, unit testing, integration testing, and debugging. It is linked to all the other software engineering disciplines, most strongly to software design and software testing.",2
Technology,Software development process,"In software engineering, a software development process is a process of dividing software development work into smaller, parallel, or sequential steps or sub-processes to improve design, product management.  It is also known as a software development life cycle (SDLC).  The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.Most modern development processes can be vaguely described as agile. Other methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, and extreme programming.
A life-cycle ""model"" is sometimes considered a more general term for a category of methodologies and a software development ""process"" a more specific term to refer to a specific process chosen by a specific organization. For example, there are many specific software development processes that fit the spiral life-cycle model. The field is often considered a subset of the systems development life cycle.

",2
Technology,Software diagnosis,"Software diagnosis (also: software diagnostics) refers to concepts, techniques, and tools that allow for obtaining findings, conclusions, and evaluations about software systems and their implementation, composition, behaviour, and evolution. It serves as means to monitor, steer, observe and optimize software development, software maintenance, and software re-engineering in the sense of a business intelligence approach specific to software systems. It is generally based on the automatic extraction, analysis, and visualization of corresponding information sources of the software system. It can also be manually done and not automatic.

",2
Technology,Software diversity,"Software diversity is a research field about the comprehension and engineering of diversity in the context of software.

",2
Technology,Software durability,"In software engineering, software durability means the solution ability of serviceability of software and to meet user's needs for a relatively long time. Software durability is important for user's satisfaction. For a software security to be durable, it must allow an organization to adjust the software to business needs that are constantly evolving, often in impulsive ways.Durability of software depends on four characteristics mainly; i.e. software trustworthiness, Human Trust for Serviceability, software dependability and software usability.",2
Technology,Software engine,"A software engine is a core component of a complex software system. Alternate phrases include ""software core"" and ""software core engine"", or just ""core engine"". 
The word ""engine"" is a metaphor of a car's engine. Thus a software engine is a complex subsystem.
There is no formal guideline for what should be called an engine, but the term has become entrenched in the software industry. Notable examples are database engine, graphics engine, physics engine, search engine, plotting engine, and game engine. Moreover, a web browser actually has two components referred to as engines: the browser engine and JavaScript engine.
Classically an engine is something packaged as a library, such as a "".sa"", "".so"", "".dll"", that provides functionality to the software that loads or embeds it. Engines may produce graphics, such as the Python matplotlib or the Objective-C Core Plot. But engines do not in and of themselves generally have standalone user interfaces or ""main"", they are not applications. Thus, a distinguishing characteristic of an engine is its presentation as an API. 
Engines may be used to produce higher level services that are applications, and the application developers or the management may choose to call the service an ""engine"". As in all definitions, context is critical. In the context of the packaging of software components, ""engine"" means one thing. In the context of advertising an online service, ""engine"" may mean something entirely different. In the arena of ""core software development"", an engine is a software module that might be included in other software by means of a package manager such as NuGet for C#, Pipenv for Python, and Swift Package Manager for the Swift language.
One seeming outlier is a search engine, such as Google Search, because it is a stand-alone service provided to end users. However, for the search provider, the engine is part of a distributed computing system that can encompass many data centers throughout the world. The word ""engine"" is evolving along with the evolution of computing as it expands into the arena of services offered via the Internet. It is important to note that there is a difference between Google the end-user application and Google the search engine. As an end user, search is done via a user interface, generally a browser, which talks to the ""engine"". This is but one way of interacting with the engine. Others include a wide range of Google APIs, which are more akin to the classic notion of engine (where an engine module presents via an API, only). There is also an overlapping software evolution, a service/application style known as microservices.
Prior to the Google online search service, there had been multiple search engines that were indeed packaged as software modules. Long before Google, there were online dialup services that used third party search engines, such as Congressional Quarterly's Washington Alert II service. Before that there were many desktop products that included third party search engines, especially CD-ROM based encyclopedias from Grollier, Comptons, Bertelsmann, and many others. Mac OS 9 for a long time used a third party search library (Personal Library Software's CPL). Most of the early search engine companies, such as Personal Library Software and their CPL product, are long gone. One of the earliest Web search services, perhaps the first, was WebCrawler. It was based on the CPL search engine library from Personal Library Software. The CPL engine is long gone, as it was withdrawn from the market when AOL acquired Personal Library Software, and apparently only exists as archival pages in the Internet Archive Wayback Machine.
For a software developer, probably the most useful notion of ""engine"" is that of a module you can use in your own code, a module that provides significant functionality in a focussed domain. One might call the C standard library an ""engine"", but it does not really have a focus other than to provide a broad range of low level services. Still, it might be called a ""foundational services"" engine. On the other hand, Gensim more clearly classifies as an engine; it is a high level package offering a range of high level tools for topic modeling, largely based on derivations of the vector space model of information retrieval originally developed by Gerard Salton.",2
Technology,Software engineering demographics,"Software engineers form part of the workforce around the world. As of 2016, it is estimated that there are 21 million professional software developers.

",2
Technology,Software engineering professionalism,"Software engineering professionalism is a movement to make software engineering a profession, with aspects such as degree and certification programs, professional associations, professional ethics, and government licensing.  The field is a licensed discipline in Texas in the United States (Texas Board of Professional Engineers, since 2013), Engineers Australia(Course Accreditation since 2001, not Licensing), and many provinces in Canada.

",2
Technology,Stevens Award,"The Stevens Award is a software engineering lecture award given by the Reengineering Forum, an industry association. The international Stevens Award was created to recognize outstanding contributions to the literature or practice of methods for software and systems development. The first award was given in 1995. The presentations focus on the current state of software methods and their direction for the future.This award lecture is named in memory of Wayne Stevens (1944-1993), a consultant, author, pioneer, and advocate of the practical application of software methods and tools. The Stevens Award and lecture is managed by the Reengineering Forum. The award was founded by International Workshop on Computer Aided Software Engineering (IWCASE), an international workshop association of users and developers of computer-aided software engineering (CASE) technology, which merged into The Reengineering Forum. Wayne Stevens was a charter member of the IWCASE executive board.

",2
Technology,Structural synthesis of programs,"Structural synthesis of programs (SSP) is a special form of (automatic) program synthesis that is based on propositional calculus. More precisely, it uses intuitionistic logic for describing the structure of a program in such a detail that the program can be automatically composed from pieces like subroutines or even computer commands. It is assumed that these pieces have been implemented correctly, hence no correctness verification of these pieces is needed. SSP is well suited for automatic composition of services for service-oriented architectures and for synthesis of large simulation programs.",2
Technology,System appreciation,"System appreciation is an activity often included in the maintenance phase of software engineering projects.  Key deliverables from this phase include documentation that describes what the system does in terms of its functional features, and how it achieves those features in terms of its architecture and design.  Software architecture recovery is often the first step within System appreciation.

",2
Technology,System context diagram,"A system context diagram (SCD) in engineering is a diagram that defines the boundary between the system, or part of a system, and its environment, showing the entities that interact with it. This diagram is a high level view of a system. It is similar to a block diagram.

",2
Technology,System requirements specification,"A System Requirements Specification (SyRS) (abbreviated SysRS when need to be distinct from a software requirements specification (SRS)) is a structured collection of information that embodies the requirements of a system.A business analyst (BA), sometimes titled system analyst, is responsible for analyzing the business needs of their clients and stakeholders to help identify business problems and propose solutions. Within the systems development life cycle domain, the BA typically performs a liaison function between the business side of an enterprise and the information technology department or external service providers.",2
Technology,Systems development life cycle,"In systems engineering, information systems and software engineering, the systems development life cycle (SDLC), also referred to as the application development life-cycle, is a process for planning, creating, testing, and deploying an information system. The systems development life cycle concept applies to a range of hardware and software configurations, as a system can be composed of hardware only, software only, or a combination of both. There are usually six stages in this cycle: requirement analysis, design, development and testing, implementation, documentation, and evaluation.",2
Technology,Systems modeling,"Systems modeling or system modeling is the interdisciplinary study of the use of models to conceptualize and construct systems in business and IT development.A common type of systems modeling is function modeling, with specific techniques such as the Functional Flow Block Diagram and IDEF0. These models can be extended using functional decomposition, and can be linked to requirements models for further systems partition.
Contrasting the functional modeling, another type of systems modeling is architectural modeling which uses the systems architecture to conceptually model the structure, behavior, and more views of a system.
The Business Process Modeling Notation (BPMN), a graphical representation for specifying business processes in a workflow, can also be considered to be a systems modeling language.

",2
Technology,Tertiary review,"In software engineering, a tertiary review is a systematic review of systematic reviews. It is also referred to as a tertiary study in the software engineering literature. However, Umbrella review is the term more commonly used in medicine.
Kitchenham et al.  suggest that methodologically there is no difference between a systematic review and tertiary review. However, as the software engineering community has started performing tertiary reviews new concerns unique to tertiary reviews have surfaced. These include the challenge of quality assessment of systematic reviews, search validation and the additional risk of double counting.

",2
Technology,View model,"A view model or viewpoints framework in systems engineering, software engineering, and enterprise engineering is a framework which defines a coherent set of views to be used in the construction of a system architecture, software architecture, or  enterprise architecture. A view is a representation of a whole system from the perspective of a related set of concerns.Since the early 1990s there have been a number of efforts to prescribe approaches for describing and analyzing system architectures. These recent efforts define a set of views (or viewpoints). They are sometimes referred to as architecture frameworks or enterprise architecture frameworks, but are usually called ""view models"".
Usually a view is a work product that presents specific architecture data for a given system. However, the same term is sometimes used to refer to a view definition, including the particular viewpoint and the corresponding guidance that defines each concrete view. The term view model is related to view definitions.",2
Technology,Bayesian learning mechanisms,"Bayesian learning mechanisms are probabilistic causal models used in computer science to research the fundamental underpinnings of machine learning, and in cognitive neuroscience, to model conceptual development.Bayesian learning mechanisms have also been used in economics and cognitive psychology to study social learning in theoretical models of herd behavior.",2
Technology,List of datasets for machine-learning research,"These datasets are applied for machine learning research and have been cited in peer-reviewed academic journals. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce.

",2
Technology,Outline of machine learning,"The following outline is provided as an overview of and topical guide to machine learning. Machine learning is a subfield of soft computing within computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a ""field of study that gives computers the ability to learn without being explicitly programmed"". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.",2
Technology,80 Million Tiny Images,"80 Million Tiny Images is a dataset intended for training machine learning systems. It contains 79,302,017 32×32 pixel color images, scaled down from images extracted from the World Wide Web in 2008 using automated web search queries on a set of 75,062 non-abstract nouns derived from WordNet. The words in the search terms were then used as labels for the images. The researchers used seven web search resources for this purpose: Altavista, Ask.com, Flickr, Cydral, Google, Picsearch and Webshots.The 80 Million Tiny Images dataset was retired from use by its creators in 2020, after a paper by researchers Abeba Birhane and Vinay Prabhu found that some of the labeling of several publicly available image datasets, including 80 Million Tiny Images, was causing models trained on them to exhibit racial and sexual bias. They have asked other researchers not to use it for further research and to delete their copies of the dataset.The CIFAR-10 dataset uses a subset of the images in this dataset, but with independently generated labels.",2
Technology,Ablation (artificial intelligence),"In artificial intelligence (AI), particularly machine learning (ML), ablation is the removal of a component of an AI system. An ablation study investigates the performance of an AI system by removing certain components to understand the contribution of the component to the overall system. The term is an analogy with biology (removal of components of an organism), and is particularly used in the analysis of artificial neural nets by analogy with ablative brain surgery. Other analogies include other neuroscience biological systems such as Drosophilla central nervous system and the vertebrate brain. Ablation studies require that a system exhibit graceful degradation: the system must continue to function even when certain components are missing or degraded. According to some researchers, ablation studies have been deemed a convenient technique in investigating artificial intelligence and its durability to structural damages. Ablation studies damage and/or remove certain components in a controlled setting to investigate all possible outcomes of system failure; this characterizes how each action impacts the system's overall performance and capabilities. The ablation process can be used to test systems that perform tasks such as speech recognition, visual object recognition, and robot control.",2
Technology,Action model learning,"Action model learning (sometimes abbreviated action learning) is an area of machine learning concerned with creation and modification of software agent's knowledge about effects and preconditions of the actions that can be executed within its environment. This knowledge is usually represented in logic-based action description language and used as the input for automated planners.
Learning action models is important when goals change. When an agent acted for a while, it can use its accumulated knowledge about actions in the domain to make better decisions. Thus, learning action models differs from reinforcement learning. It enables reasoning about actions instead of expensive trials in the world. Action model learning is a form of inductive reasoning, where new knowledge is generated based on agent's observations. It differs from standard supervised learning in that correct input/output pairs are never presented, nor imprecise action models explicitly corrected.
Usual motivation for action model learning is the fact that manual specification of action models for planners is often a difficult, time consuming, and error-prone task (especially in complex environments).",2
Technology,Active learning (machine learning),"Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs. In statistics literature, it is sometimes also called optimal experimental design. The information source is also called teacher or oracle.
There are situations in which unlabeled data is abundant but manual labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm is overwhelmed by uninformative examples.  Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e.g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning.

",2
Technology,Adversarial machine learning,"Adversarial machine learning is a machine learning technique that attempts to exploit models by taking advantage of obtainable model information and using it to create malicious attacks. The most common reason is to cause a malfunction in a machine learning model.
Most machine learning techniques were designed to work on specific problem sets in which the training and test data are generated from the same statistical distribution (IID). When those models are applied to the real world, adversaries may supply data that violates that statistical assumption. This data may be arranged to exploit specific vulnerabilities and compromise the results. The four most common adversarial machine learning strategies are evasion, poisoning, model stealing (extraction), and inference.",2
Technology,AIXI,"AIXI ['ai̯k͡siː] is a theoretical mathematical formalism for artificial general intelligence.
It combines Solomonoff induction with sequential decision theory.
AIXI was first proposed by Marcus Hutter in 2000 and several results regarding AIXI are proved in Hutter's 2005 book Universal Artificial Intelligence.AIXI is a reinforcement learning agent. It maximizes the expected total rewards received from the environment. Intuitively, it simultaneously considers every computable hypothesis (or environment). In each time step, it looks at every possible program and evaluates how many rewards that program generates depending on the next action taken. The promised rewards are then weighted by the subjective belief that this program constitutes the true environment. This belief is computed from the length of the program: longer programs are considered less likely, in line with Occam's razor. AIXI then selects the action that has the highest expected total reward in the weighted sum of all these programs.

",2
Technology,Algorithm selection,"In computer science, a selection algorithm is an algorithm for finding the kth smallest number in a list or array; such a number is called the kth order statistic. This includes the cases of finding the minimum, maximum, and median elements. There are O(n)-time (worst-case linear time) selection algorithms, and sublinear performance is possible for structured data; in the extreme, O(1) for an array of sorted data. Selection is a subproblem of more complex problems like the nearest neighbor and shortest path problems. Many selection algorithms are derived by generalizing a sorting algorithm, and conversely some sorting algorithms can be derived as repeated application of selection.
The simplest case of a selection algorithm is finding the minimum (or maximum) element by iterating through the list, keeping track of the running minimum – the minimum so far – (or maximum) and can be seen as related to the selection sort. Conversely, the hardest case of a selection algorithm is finding the median. In fact, a specialized median-selection algorithm can be used to build a general selection algorithm, as in median of medians. The best-known selection algorithm is Quickselect, which is related to Quicksort; like Quicksort, it has (asymptotically) optimal average performance, but poor worst-case performance, though it can be modified to give optimal worst-case performance as well.",2
Technology,Algorithmic bias,"Algorithmic bias describes systematic and repeatable errors in a computer system that create ""unfair"" outcomes, such as ""privileging"" one category over another in ways different from the intended function of the algorithm. 
Bias can emerge from many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm. For example, algorithmic bias has been observed in search engine results and social media platforms. This bias can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race, gender, sexuality, and ethnicity. The study of algorithmic bias is most concerned with algorithms that reflect ""systematic and unfair"" discrimination. This bias has only recently been addressed in legal frameworks, such as the European Union's General Data Protection Regulation (2018) and the proposed Artificial Intelligence Act (2021).
As algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise (in part due to the psychological phenomenon of automation bias), and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design.
Algorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech. It has also arisen in criminal justice, healthcare, and hiring, compounding existing racial, socioeconomic, and gender biases. The relative inability of facial recognition technology to accurately identify darker-skinned faces has been linked to multiple wrongful arrests of black men, an issue stemming from imbalanced datasets. Problems in understanding, researching, and discovering algorithmic bias persist due to the proprietary nature of algorithms, which are typically treated as trade secrets. Even when full transparency is provided, the complexity of certain algorithms poses a barrier to understanding their functioning. Furthermore, algorithms may change, or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis. In many cases, even within a single website or application, there is no single ""algorithm"" to examine, but a network of many interrelated programs and data inputs, even between users of the same service.",2
Technology,Algorithmic inference,"Algorithmic inference gathers new developments in the statistical inference methods made feasible by the powerful computing devices widely available to any data analyst. Cornerstones in this field are computational learning theory, granular computing, bioinformatics, and, long ago, structural probability (Fraser 1966).
The main focus is on the algorithms which compute statistics rooting the study of a random phenomenon, along with the amount of data they must feed on to produce reliable results. This shifts the interest of mathematicians from the study of the distribution laws to the functional properties of the statistics, and the interest of computer scientists from the algorithms for processing data to the information they process.",2
Technology,Anomaly detection,"In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behaviour. Such examples may arouse suspicions of being generated by a different mechanism, or appear inconsistent with the remainder of that set of data.Anomaly detection finds application in many domains including cyber security, medicine, machine vision, statistics, neuroscience, law enforcement and financial fraud to name only a few. Anomalies were initially searched for clear rejection or omission from the data to aid statistical analysis, for example to compute the mean or standard deviation. They were also removed to better predictions from models such as linear regression, and more recently their removal aids the performance of machine learning algorithms. However, in many applications anomalies themselves are of interest and are the observations most desirous in the entire data set, which need to be identified and separated from noise or irrelevant outliers.
Three broad categories of anomaly detection techniques exist. Supervised anomaly detection techniques require a data set that has been labeled as ""normal"" and ""abnormal"" and involves training a classifier. However, this approach is rarely used in anomaly detection due to the general unavailability of labelled data and the inherent unbalanced nature of the classes. Semi-supervised anomaly detection techniques assume that some portion of the data is labelled. This may be any combination of the normal or anomalous data, but more often than not the techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the model. Unsupervised anomaly detection techniques assume the data is unlabelled and are by far the most commonly used due to their wider and relevant application.

",2
Technology,Apprenticeship learning,"In artificial intelligence, apprenticeship learning (or learning from demonstration) is the process of learning by observing an expert.  It can be viewed as a form of supervised learning, where the training dataset consists of task executions by a demonstration teacher.

",2
Technology,Artificial intelligence in hiring,"Artificial intelligence (AI) in hiring involves the use of technology to automate aspects of the hiring process. Advances in artificial intelligence, such as the advent of machine learning and the growth of big data, enable AI to be utilized to recruit, screen, and predict the success of applicants. Proponents of artificial intelligence in hiring claim it reduces bias, assists with finding qualified candidates, and frees up human resource workers' time for other tasks, while opponents worry that AI perpetuates inequalities in the workplace and will eliminate jobs.

",2
Technology,Associative classifier,"An associative classifier (AC) is a kind of supervised learning model that uses association rules to assign a target value. The term associative classification was coined by Bing Liu et al., in which the authors defined a model made of rules ""whose right-hand side are restricted to the classification class attribute"".",2
Technology,Astrostatistics,"Astrostatistics is a discipline which spans astrophysics, statistical analysis and data mining. It is used to process the vast amount of data produced by automated scanning of the cosmos, to characterize complex datasets, and to link astronomical data to astrophysical theory.  Many branches of statistics are involved in astronomical analysis including nonparametrics, multivariate regression and multivariate classification, time series analysis, and especially Bayesian inference. The field is closely related to Astroinformatics.",2
Technology,Attention (machine learning),"In neural networks, attention is a technique that mimics cognitive attention. The effect enhances some parts of the input data while diminishing other parts — the thought being that the network should devote more focus to that small but important part of the data. Learning which part of the data is more important than others depends on the context and is trained by gradient descent.
Attention-like mechanisms were introduced in the 1990s under names like multiplicative modules, sigma pi units, and hypernetworks. Its flexibility comes from its role as ""soft weights"" that can change during runtime, in contrast to standard weights that must remain fixed at runtime.  Uses of attention include  memory in neural Turing machines, reasoning tasks in differentiable neural computers, language processing in transformers, and multi-sensory data processing (sound, images, video, and text) in perceivers.

",2
Technology,Australian Institute for Machine Learning,"The Australian Institute for Machine Learning (AIML) is an artificial intelligence and machine learning research and translation institute based at the Lot Fourteen innovation precinct in Adelaide, South Australia. 
An institute of the University of Adelaide, AIML was established in 2018 as Lot Fourteen’s first tenant in a joint initiative with the Government of South Australia to support the state’s transition to a more modern economy, attract global businesses to establish operations at Lot Fourteen, and assist the government to “improve productivity, efficiency and service delivery” for the state.With more than 160 members, AIML is Australia’s largest university-based research site dedicated to machine learning and ranks consistently among the global top sites for its computer vision research capability.

",2
Technology,Automated machine learning,"Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. AutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning. The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. Common techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.

",2
Technology,Automation in construction,"Automation in construction is the combination of methods, processes, and systems that allow for greater machine autonomy in construction activities. Construction automation may have multiple goals, including but not limited to, reducing jobsite injuries, decreasing activity completion times, and assisting with quality control and quality assurance. Some systems may be fielded as a direct response to increasing skilled labor shortages in some countries. Opponents claim that increased automation may lead to less construction jobs and that software leaves heavy equipment vulnerable to hackers.",2
Technology,Bag-of-words model,"The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision.The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.An early reference to ""bag of words"" in a linguistic context can be found in Zellig Harris's 1954 article on Distributional Structure.The Bag-of-words model is one example of a Vector space model.

",2
Technology,Ball tree,"In computer science, a ball tree, balltree or metric tree, is a space partitioning data structure for organizing points in a multi-dimensional space. The ball tree gets its name from the fact that it partitions data points into a nested set of intersecting hyperspheres known as ""balls"". The resulting data structure has characteristics that make it useful for a number of applications, most notably nearest neighbor search.

",2
Technology,Base rate,"In probability and statistics, the base rate (also known as prior probabilities), is the class probabilities unconditional on ""featural evidence"" (likelihoods). 
For example, if it were the case that 1% of the public were medical professionals, and 99% of the public were not medical professionals, then the base rate of medical professionals is simply 1%.
The normative method for integrating base rates and featural evidence is given by Bayes' rule.
In the sciences, including medicine, the base rate is critical for comparison. It may be perceived as impressive that 1,000 people recovered from their winter cold while using 'Treatment X', until the entirety of the 'Treatment X' population is evaluated to find that the base rate of success is only 1/100 (i.e. 100,000 people tried the treatment, but the other 99,000 people never recovered from their winter cold). The treatment's effectiveness is clearer when such base rate information (i.e. ""1,000 people... out of how many?"") is available. Note that controls may likewise offer further information for comparison; for example, this would be the case if the control groups, who were using no treatment at all, had their own base rate success of 5/100: this would indicate that 'Treatment X' makes things worse, despite the initial claim bringing attention to the 1,000 people.",2
Technology,Bayesian interpretation of kernel regularization,"Within bayesian statistics for machine learning, kernel methods arise from the assumption of an inner product space or similarity structure on inputs. For some such methods, such as support vector machines (SVMs), the original formulation and its regularization were not Bayesian in nature. It is helpful to understand them from a Bayesian perspective.  Because the kernels are not necessarily positive semidefinite, the underlying structure may not be inner product spaces, but instead more general reproducing kernel Hilbert spaces.  In Bayesian probability kernel methods are a key component of Gaussian processes, where the kernel function is known as the covariance function.  Kernel methods have traditionally been used in supervised learning problems where the input space is usually a space of vectors while the output space is a space of scalars. More recently these methods have been extended to problems that deal with multiple outputs such as in multi-task learning.A mathematical equivalence between the regularization and the Bayesian point of view is easily proved in cases where the reproducing kernel Hilbert space is finite-dimensional. The infinite-dimensional case raises subtle mathematical issues; we will consider here the finite-dimensional case. We start with a brief review of the main ideas underlying kernel methods for scalar learning, and briefly introduce the concepts of regularization and Gaussian processes. We then show how both points of view arrive at essentially equivalent estimators, and show the connection that ties them together.

",2
Technology,Bayesian optimization,"Bayesian optimization is a sequential design strategy for global optimization of black-box functions that does not assume any functional forms. It is usually employed to optimize expensive-to-evaluate functions.

",2
Technology,Bayesian regret,"In stochastic game theory, Bayesian regret is the expected difference (""regret"") between the utility of a Bayesian strategy and that of the optimal strategy (the one with the highest expected payoff). 
The term Bayesian refers to Thomas Bayes (1702–1761), who proved a special case of what is now called Bayes' theorem, who provided the first mathematical treatment of a non-trivial problem of statistical data analysis using what is now known as Bayesian inference.",2
Technology,Bayesian structural time series,"Bayesian structural time series (BSTS) model is a statistical technique used for feature selection, time series forecasting, nowcasting, inferring causal impact and other applications. The model is designed to work with time series data.
The model has also promising application in the field of analytical marketing. In particular, it can be used in order to assess how much different marketing campaigns have contributed to the change in web search volumes, product sales, brand popularity and other relevant indicators. Difference-in-differences models and interrupted time series designs are alternatives to this approach. ""In contrast to classical difference-in-differences schemes, state-space models make it possible to (i) infer the temporal evolution of attributable impact, (ii) incorporate empirical priors on the parameters in a fully Bayesian treatment, and (iii) flexibly accommodate multiple sources of variation, including the time-varying influence of contemporaneous covariates, i.e., synthetic controls.""",2
Technology,Bias–variance tradeoff,"In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter estimated across samples can be reduced by increasing the bias in the estimated parameters.
The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:
The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).
The variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting).The bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.",2
Technology,Binary classification,"Binary classification is the task of classifying the elements of a set into two groups (each called class) on the basis of a classification rule. Typical binary classification problems include:

Medical testing to determine if a patient has certain disease or not;
Quality control in industry, deciding whether a specification has been met;
In information retrieval, deciding whether a page should be in the result set of a search or not.Binary classification is dichotomization applied to a practical situation. In many practical binary classification problems, the two groups are not symmetric, and rather than overall accuracy, the relative proportion of different types of errors is of interest. For example, in medical testing, detecting a disease when it is not present (a false positive) is considered differently from not detecting a disease when it is present (a false negative).

",2
Technology,Bioserenity,"BioSerenity is medtech company created in 2014 that develops ambulatory medical devices to help diagnose and monitor patients with chronic diseases such as epilepsy. The medical devices are composed of medical sensors, smart clothing, a smart phone app for Patient Reported Outcome, a web platform to perform data analysis through Medical Artificial Intelligence for detection of digital biomarkers. The company initially focused on Neurology a domain in which it reported contributing to the diagnosis of 30 000 patients per year. It now also operates in Sleep Disorders and Cardiology. BioSerenity reported it provides pharmaceutical companies with solutions for companion diagnostics.",2
Technology,Bradley–Terry model,"The Bradley–Terry model is a probability model that can predict the outcome of a paired comparison. Given a pair of individuals i and j drawn from some population, it estimates the probability that the pairwise comparison i > j turns out true, as

  
    
      
        P
        (
        i
        >
        j
        )
        =
        
          
            
              p
              
                i
              
            
            
              
                p
                
                  i
                
              
              +
              
                p
                
                  j
                
              
            
          
        
      
    
    {\displaystyle P(i>j)={\frac {p_{i}}{p_{i}+p_{j}}}}
  where pi is a positive real-valued score assigned to individual i. The comparison i > j can be read as ""i is preferred to j"", ""i ranks higher than j"", or ""i beats j"", depending on the application.
For example, pi may represent the skill of a team in a sports tournament, estimated from the number of times i has won a match. 
  
    
      
        P
        (
        i
        >
        j
        )
      
    
    {\displaystyle P(i>j)}
   then represents the probability that i will win a match against j. Another example used to explain the model's purpose is that of scoring products in a certain category by quality. While it's hard for a person to draft a direct ranking of (many) brands of wine, it may be feasible to compare a sample of pairs of wines and say, for each pair, which one is better. The Bradley–Terry model can then be used to derive a full ranking.

",2
Technology,Catastrophic interference,"Catastrophic interference, also known as catastrophic forgetting, is the tendency of an artificial neural network to completely and abruptly forget previously learned information upon learning new information. Neural networks are an important part of the network approach and connectionist approach to cognitive science. With these networks, human capabilities such as memory and learning can be modeled using computer simulations. Catastrophic interference is an important issue to consider when creating connectionist models of memory. It was originally brought to the attention of the scientific community by research from McCloskey and Cohen (1989), and Ratcliff (1990). It is a radical manifestation of the 'sensitivity-stability' dilemma or the 'stability-plasticity' dilemma. Specifically, these problems refer to the challenge of making an artificial neural network that is sensitive to, but not disrupted by, new information. Lookup tables and connectionist networks lie on the opposite sides of the stability plasticity spectrum. The former remains completely stable in the presence of new information but lacks the ability to generalize, i.e. infer general principles, from new inputs. On the other hand, connectionist networks like the standard backpropagation network can generalize to unseen inputs, but they are very sensitive to new information. Backpropagation models can be considered good models of human memory insofar as they mirror the human ability to generalize but these networks often exhibit less stability than human memory. Notably, these backpropagation networks are susceptible to catastrophic interference. This is an issue when modelling human memory, because unlike these networks, humans typically do not show catastrophic forgetting.

",2
Technology,Category utility,"Category utility is a measure of ""category goodness"" defined in Gluck & Corter (1985) and Corter & Gluck (1992). It attempts to maximize both the probability that two objects in the same category have attribute values in common, and the probability that objects from different categories have different attribute values. It was intended to supersede more limited measures of category goodness such as ""cue validity"" (Reed 1972; Rosch & Mervis 1975) and ""collocation index"" (Jones 1983). It provides a normative information-theoretic measure of the predictive advantage gained by the observer who possesses knowledge of the given category structure (i.e., the class labels of instances) over the observer who does not possess knowledge of the category structure. In this sense the motivation for the category utility measure is similar to the information gain metric used in decision tree learning. In certain presentations, it is also formally equivalent to the mutual information, as discussed below. A review of category utility in its probabilistic incarnation, with applications to machine learning, is provided in Witten & Frank (2005, pp. 260–262).",2
Technology,CIML community portal,"The computational intelligence and machine learning (CIML) community portal is an international multi-university initiative.  Its primary purpose is to help facilitate a virtual scientific community infrastructure for all those involved with, or interested in, computational intelligence and machine learning.  This includes CIML research-, education, and application-oriented resources residing at the portal and others that are linked from the CIML site.",2
Technology,Cognitive robotics,"Cognitive Robotics  or Cognitive Technology is a subfield of robotics concerned with endowing a robot with intelligent behavior by providing it with a processing architecture that will allow it to learn and reason about how to behave in response to complex goals in a complex world. Cognitive robotics may be considered the engineering branch of embodied cognitive science and embodied embedded cognition. In Software Robotics, Cognitive Robotics or Cognitive Technology is a form of Hyper Automation and implies a combination of Robotic Process Automation, Artificial Intelligence, Machine Learning, Deep Learning, Optical Character Recognition, Image Processing, Software Development and System Integration.

",2
Technology,Committee machine,"A committee machine is a type of artificial neural network using a divide and conquer strategy in which the responses of multiple neural networks (experts) are combined into a single response.  The combined response of the committee machine is supposed to be superior to those of its constituent experts.  Compare with ensembles of classifiers.

",2
Technology,Computational learning theory,"In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms.",2
Technology,Concept class,"In computational learning theory in mathematics, a concept over a domain X is a total Boolean function over X. A concept class is a class of concepts. Concept classes are a subject of computational learning theory.
Concept class terminology frequently appears in model theory associated with probably approximately correct (PAC) learning. In this setting, if one takes a set Y as a set of (classifier output) labels, and X is a set of examples, the map 
  
    
      
        c
        :
        X
        →
        Y
      
    
    {\displaystyle c:X\to Y}
  , i.e. from examples to classifier labels (where 
  
    
      
        Y
        =
        {
        0
        ,
        1
        }
      
    
    {\displaystyle Y=\{0,1\}}
   and where c is a subset of X), c is then said to be a concept. A concept class 
  
    
      
        C
      
    
    {\displaystyle C}
   is then a collection of such concepts.
Given a class of concepts C, a subclass D is reachable if there exists a sample s such that D contains exactly those concepts in C that are extensions to s. Not every subclass is reachable.",2
Technology,Concept drift,"In predictive analytics and machine learning, concept drift means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.
The term concept refers to the quantity to be predicted. More generally, it can also refer to other phenomena of interest besides the target concept, such as an input, but, in the context of concept drift, the term commonly refers to the target variable.",2
Technology,Conditional random field,"Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering ""neighbouring"" samples, a CRF can take context into account. To do so, the predictions are modelled as a graphical model, which represents the presence of dependencies between the predictions. What kind of graph is used depends on the application. For example, in natural language processing, ""linear chain"" CRFs are popular, for which each prediction is dependent only on its immediate neighbours. In image processing, the graph typically connects locations to nearby and/or similar locations to enforce that they receive similar predictions.
Other examples where CRFs are used are: labeling or parsing of sequential data for natural language processing or biological sequences, POS tagging, shallow parsing, named entity recognition, gene finding, peptide critical functional region finding, and object recognition and image segmentation in computer vision.",2
Technology,Connectionist temporal classification,"Connectionist temporal classification (CTC) is a type of neural network output and associated scoring function, for training recurrent neural networks (RNNs) such as LSTM networks to tackle sequence problems where the timing is variable. It can be used for tasks like on-line handwriting recognition or recognizing phones in speech audio. CTC refers to the outputs and scoring, and is independent of the underlying neural network structure. It was introduced in 2006.The input is a sequence of observations, and the outputs are a sequence of labels, which can include blank outputs. The difficulty of training comes from there being many more observations than there are labels. For example in speech audio there can be multiple time slices which correspond to a single phoneme. Since we don't know the alignment of the observed sequence with the target labels we predict a probability distribution at each time step. A CTC network has a continuous output (e.g. softmax), which is fitted through training to model the probability of a label. CTC does not attempt to learn boundaries and timings: Label sequences are considered equivalent if they differ only in alignment, ignoring blanks. Equivalent label sequences can occur in many ways – which makes scoring a non-trivial task, but there is an efficient forward–backward algorithm for that.
CTC scores can then be used with the back-propagation algorithm to update the neural network weights.
Alternative approaches to a CTC-fitted neural network include a hidden Markov model (HMM).

",2
Technology,Constrained conditional model,"A constrained conditional model (CCM) is a machine learning and inference framework that augments the learning of conditional (probabilistic or discriminative) models with declarative constraints. The constraint can be used as a way to incorporate expressive prior knowledge into the model and bias the assignments made by the learned model to satisfy these constraints. The framework can be used to support decisions in an expressive output space while maintaining modularity and tractability of training and inference.
Models of this kind have recently attracted much attention within the natural language processing (NLP) community.
Formulating problems as constrained optimization problems over the output of learned models has several advantages. It allows one to focus on the modeling of problems by providing the opportunity to incorporate domain-specific knowledge as global constraints using a first order language. Using this declarative framework frees the developer from low level feature engineering while capturing the problem's domain-specific properties and guarantying exact inference. From a machine learning perspective it allows decoupling the stage of model generation (learning) from that of the constrained inference stage, thus helping to simplify the learning stage while improving the quality of the solutions. For example, in the case of generating compressed sentences, rather than simply relying on a language model to retain the most commonly used n-grams in the sentence, constraints can be used to ensure that if a modifier is kept in the compressed sentence, its subject will also be kept.

",2
Technology,Convolutional neural network,"In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of artificial neural network (ANN), most commonly applied to analyze visual imagery. CNNs are also known as Shift Invariant or Space Invariant Artificial Neural Networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are only equivariant, as opposed to invariant, to translation. They have applications in image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing, brain–computer interfaces, and financial time series.CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The ""full connectivity"" of these networks make them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble patterns of increasing complexity using smaller and simpler patterns embossed in their filters. Therefore, on a scale of connectivity and complexity, CNNs are on the lower extreme.
Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.
CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage.",2
Technology,Count sketch,"Count sketch  is a type of dimensionality reduction that is particularly efficient in statistics, machine learning and algorithms.
It was invented by 
Moses Charikar, Kevin Chen and Martin Farach-Colton in an effort to speed up the AMS Sketch by Alon, Matias and Szegedy for approximating the frequency moments of streams.The sketch is nearly identical to the Feature hashing algorithm by John Moody, but differs in its use of hash functions with low dependence, which makes it more practical.
In order to still have a high probability of success, the median trick is used to aggregate multiple count sketches, rather than the mean.
These properties allow use for explicit kernel methods, bilinear pooling in neural networks and is a cornerstone in many numerical linear algebra algorithms.",2
Technology,Coupled pattern learner,"Coupled Pattern Learner (CPL) is a machine learning algorithm which couples the semi-supervised learning of categories and relations to forestall the problem of semantic drift associated with boot-strap learning methods.

",2
Technology,Cross-entropy method,"The cross-entropy (CE) method is a Monte Carlo method for importance sampling and optimization. It is applicable to both combinatorial and continuous problems, with either a static or noisy objective.
The method approximates the optimal importance sampling estimator by repeating two phases:
Draw a sample from a probability distribution.
Minimize the cross-entropy between this distribution and a target distribution to produce a better sample in the next iteration.Reuven Rubinstein developed the method in the context of rare event simulation, where tiny probabilities must be estimated, for example in network reliability analysis, queueing models, or performance analysis of telecommunication systems. The method has also been applied to the traveling salesman, quadratic assignment, DNA sequence alignment, max-cut and buffer allocation problems.

",2
Technology,Cross-validation (statistics),"Cross-validation, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.
Cross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.  In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set). The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).
One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.
In summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance.",2
Technology,Curse of dimensionality,"The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience.  The expression was coined by Richard E. Bellman when considering problems in dynamic programming.Dimensionally cursed phenomena occur in domains such as numerical analysis, sampling, combinatorics, machine learning, data mining and databases. The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. In order to obtain a reliable result, the amount of data needed often grows exponentially with the dimensionality. Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient.",2
Technology,Data augmentation,"Data augmentation in data analysis are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. It acts as a regularizer and helps reduce overfitting when training a machine learning model. It is closely related to oversampling in data analysis.

",2
Technology,Data exploration,"Data exploration is an approach similar to initial data analysis, whereby a data analyst uses visual exploration to understand what is in a dataset and the characteristics of the data, rather than through traditional data management systems. These characteristics can include size or amount of data, completeness of the data, correctness of the data, possible relationships amongst data elements or files/tables in the data.
Data exploration is typically conducted using a combination of automated and manual activities. Automated activities can include data profiling or data visualization or tabular reports to give the analyst an initial view into the data and an understanding of key characteristics.This is often followed by manual drill-down or filtering of the data to identify anomalies or patterns identified through the automated actions.  Data exploration can also require manual scripting and queries into the data (e.g. using languages such as SQL or R) or using spreadsheets or similar tools to view the raw data.All of these activities are aimed at creating a mental model and understanding of the data in the mind of the analyst, and defining basic metadata (statistics, structure, relationships) for the data set that can be used in further analysis.Once this initial understanding of the data is had, the data can be pruned or refined by removing unusable parts of the data (data cleansing), correcting poorly formatted elements and defining relevant relationships across datasets. This process is also known as determining data quality.Data exploration can also refer to the ad hoc querying or visualization of data to identify potential relationships or insights that may be hidden in the data and does not require to formulate assumptions beforehand.Traditionally, this had been a key area of focus for statisticians, with John Tukey being a key evangelist in the field. Today, data exploration is more widespread and is the focus of data analysts and data scientists; the latter being a relatively new role within enterprises and larger organizations.

",2
Technology,Data pre-processing,"Data preprocessing can refer to manipulation or dropping of data before it is used in order to ensure or enhance performance, and is an important step in the data mining process. The phrase ""garbage in, garbage out"" is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in out-of-range values (e.g., Income: −100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), and missing values, etc. 
Analyzing data that has not been carefully screened for such problems can produce misleading results. Thus, the representation and quality of data is first and foremost before running any analysis. 
Often, data preprocessing is the most important phase of a machine learning project, especially in computational biology. If there is much irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase is more difficult. Data preparation and filtering steps can take considerable amount of processing time. Examples of data preprocessing include cleaning, instance selection, normalization, one hot encoding, transformation, feature extraction and selection, etc. The product of data preprocessing is the final training set.
Data preprocessing may affect the way in which outcomes of the final data processing can be interpreted. This aspect should be carefully considered when interpretation of the results is a key point, such in the multivariate processing of chemical data (chemometrics).",2
Technology,Decision list,"Decision lists are a representation for Boolean functions which can be easily learnable from examples.  Single term decision lists are more expressive than disjunctions and conjunctions; however, 1-term decision lists are less expressive than the general disjunctive normal form and the conjunctive normal form.
The language specified by a k-length decision list includes as a subset the language specified by a k-depth decision tree.
Learning decision lists can be used for attribute efficient learning.",2
Technology,Decision tree pruning,"Pruning is a data compression technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that are non-critical and redundant to classify instances. Pruning reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting.
One of the questions that arises in a decision tree algorithm is the optimal size of the final tree.  A tree that is too large risks overfitting the training data and poorly generalizing to new samples.  A small tree might not capture important structural information about the sample space.  However, it is hard to tell when a tree algorithm should stop because it is impossible to tell if the addition of a single extra node will dramatically decrease error.  This problem is known as the horizon effect.  A common strategy is to grow the tree until each node contains a small number of instances then use pruning to remove nodes that do not provide additional information.Pruning should reduce the size of a learning tree without reducing predictive accuracy as measured by a cross-validation set.  There are many techniques for tree pruning that differ in the measurement that is used to optimize performance.

",2
Technology,Deeplearning4j,"Eclipse Deeplearning4j is a programming library written in Java for the Java virtual machine (JVM). It is a framework with wide support for deep learning algorithms. Deeplearning4j includes implementations of the restricted Boltzmann machine, deep belief net, deep autoencoder, stacked denoising autoencoder and recursive neural tensor network, word2vec, doc2vec, and GloVe. These algorithms all include distributed parallel versions that integrate with Apache Hadoop and Spark.Deeplearning4j is open-source software released under Apache License 2.0, developed mainly by a machine learning group headquartered in San Francisco. It is supported commercially by the startup Skymind, which bundles DL4J, TensorFlow, Keras and other deep learning libraries in an enterprise distribution called the Skymind Intelligence Layer. Deeplearning4j was contributed to the Eclipse Foundation in October 2017.

",2
Technology,Developmental robotics,"Developmental robotics (DevRob), sometimes called epigenetic robotics, is a scientific field which aims at studying the developmental mechanisms, architectures and constraints that allow lifelong and open-ended learning of new skills and new knowledge in embodied machines. As in human children, learning is expected to be cumulative and of progressively increasing complexity, and to result from self-exploration of the world in combination with social interaction. The typical methodological approach consists in starting from theories of human and animal development elaborated in fields such as developmental psychology, neuroscience, developmental and evolutionary biology, and linguistics, then to formalize and implement them in robots, sometimes exploring extensions or variants of them. The experimentation of those models in robots allows researchers to confront them with reality, and as a consequence, developmental robotics also provides feedback and novel hypotheses on theories of human and animal development.
Developmental robotics is related to but differs from evolutionary robotics (ER).  ER uses populations of robots that evolve over time, whereas DevRob is interested in how the organization of a single robot's control system develops through experience, over time.
DevRob is also related to work done in the domains of robotics and artificial life.

",2
Technology,Dimensionality reduction,"Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with). Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics.Methods are commonly divided into linear and nonlinear approaches. Approaches can also be divided into feature selection and feature extraction. Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses.",2
Technology,Discovery system (AI research),"A discovery system is an artificial intelligence system that attempts to discover new scientific concepts or laws.
Notable discovery systems have included:

Autoclass
Automated Mathematician
DALTON
Eurisko
Glauber
Machine for Questions and Answers
Stahl

",2
Technology,Document classification,"Document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done ""manually"" (or ""intellectually"") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.
The documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied.
Documents may be classified according to their subjects or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach.",2
Technology,Drift (data science),"In data science and related fields, drift is an evolution of data that invalidates the data model. Common areas where identification of data drift is important are machine learning and data mining, as well as maintenance of large software systems.  Drift detection and drift adaptation are of paramount importance in the fields that involve dynamically changing data and data models.",2
Technology,Eager learning,"In artificial intelligence, eager learning is a learning method in which the system tries to construct a general, input-independent target function during training of the system, as opposed to lazy learning, where generalization beyond the training data is delayed until a query is made to the system. 
The main advantage gained in employing an eager learning method, such as an artificial neural network, is that the target function will be approximated globally during training, thus requiring much less space than using a lazy learning system. Eager learning systems also deal much better with noise in the training data. Eager learning is an example of offline learning, in which post-training queries to the system have no effect on the system itself, and thus the same query to the system will always produce the same result.
The main disadvantage with eager learning is that it is generally unable to provide good local approximations in the target function.",2
Technology,Early stopping,"In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner's performance on data outside of the training set. Past that point, however, improving the learner's fit to the training data comes at the expense of increased generalization error. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit. Early stopping rules have been employed in many different machine learning methods, with varying amounts of theoretical foundation.",2
Technology,Elastic matching,"Elastic matching is one of the pattern recognition techniques in computer science. Elastic matching (EM) is also known as deformable template, flexible matching, or nonlinear template matching.Elastic matching can be defined as an optimization problem of two-dimensional warping specifying corresponding pixels between subjected images.",2
Technology,ELMo,"Emo  is a rock music genre characterized by an emphasis on emotional expression, sometimes through confessional lyrics. It emerged as a style of post-hardcore from the mid-1980s hardcore punk movement in Washington, D.C., where it was known as emotional hardcore or emocore and pioneered by bands such as Rites of Spring and Embrace. In the early–mid 1990s, emo was adopted and reinvented by alternative rock, indie rock and/or punk rock bands such as Sunny Day Real Estate, Jawbreaker, Weezer, Cap'n Jazz, and Jimmy Eat World, with Weezer breaking into the mainstream during this time. By the mid-1990s, bands such as Braid, the Promise Ring, and the Get Up Kids emerged from the burgeoning Midwest emo scene, and several independent record labels began to specialize in the genre. Meanwhile, screamo, a more aggressive style of emo using screamed vocals, also emerged, pioneered by the San Diego bands Heroin and Antioch Arrow. Screamo achieved mainstream success in the 2000s with bands like Hawthorne Heights, Silverstein, Story of the Year, Thursday, the Used, and Underoath.
Often seen as a subculture, emo also signifies a specific relationship between fans and artists and certain aspects of fashion, culture and behavior. Emo fashion has been associated with skinny jeans, black eyeliner, tight t-shirts with band names, studded belts, and flat, straight, jet-black hair with long bangs. Since the early to mid 2000s, fans of emo music who dress like this are referred to as ""emo kids"" or ""emos"" and known for listening to bands like My Chemical Romance, Fall Out Boy, Hawthorne Heights, The Used, and AFI. The emo subculture was stereotypically associated with social alienation, sensitivity, misanthropy, introversion and angst. Purported links to depression, self-harm and suicide, combined with its rise in popularity in the early 2000s, inspired a backlash against emo, with bands such as My Chemical Romance and Panic! at the Disco rejecting the emo label because of the social stigma and controversy surrounding it.
Emo and its subgenre emo pop entered mainstream culture in the early 2000s with the success of Jimmy Eat World and Dashboard Confessional and many artists signed to major record labels. Bands such as My Chemical Romance, AFI, Fall Out Boy and the Red Jumpsuit Apparatus continued the genre's popularity during the rest of the decade. By the early 2010s, emo's popularity had declined, with some groups changing their sound and others disbanding. Meanwhile, however, a mainly underground emo revival emerged, with bands such as The World Is a Beautiful Place & I Am No Longer Afraid to Die and Modern Baseball, some drawing on the sound and aesthetic of 1990s emo. During the late 2010s, a fusion genre called emo rap was mainstream, with some of emo rap's most famous artists including Lil Peep, XXXTentacion and Juice Wrld.",2
Technology,EM algorithm and GMM model,"In statistics, EM (expectation maximization) algorithm handles latent variables, while GMM is the Gaussian mixture model.

",2
Technology,Empirical dynamic modeling,"Empirical dynamic modeling (EDM) is a framework for analysis and prediction of nonlinear dynamical systems. Applications include population dynamics, ecosystem service, medicine, neuroscience, dynamical systems and geophysics. EDM was originally developed by Robert May and George Sugihara.  It can be considered a methodology for data modeling, predictive analytics, dynamical system analysis, machine learning and time series analysis.

",2
Technology,Empirical risk minimization,"Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an algorithm will work in practice (the true ""risk"") because we don't know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the ""empirical"" risk).",2
Technology,Equalized odds,"Equalized odds, also referred to as conditional procedure accuracy equality and disparate mistreatment, is a measure of fairness in machine learning. A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal true positive rate and equal false positive rate, satisfying the formula:

For example, 
  
    
      
        A
      
    
    {\displaystyle A}
   could be gender, race, and other characteristics that we want to be free of bias, while 
  
    
      
        Y
      
    
    {\displaystyle Y}
   would be whether the person is qualified for the degree, and the output 
  
    
      
        R
      
    
    {\displaystyle R}
   would be the school's decision whether to offer the person to study for the degree. In this context, higher university enrollment rates of African Americans compared to whites with similar test scores may also fulfill the condition of Equalized odds.
Originally, the concept is defined for binary class. In 2017, Blake Woodworth generalized the concept further for multiple classes.",2
Technology,Evaluation of binary classifiers,"The evaluation of binary classifiers compares two methods of assigning a binary attribute, one of which is usually a standard method and the other is being investigated. There are many metrics that can be used to measure the performance of a classifier or predictor; different fields have different preferences for specific metrics due to different goals. For example, in medicine sensitivity and specificity are often used, while in computer science precision and recall are preferred. An important distinction is between metrics that are independent on the prevalence (how often each category occurs in the population), and metrics that depend on the prevalence – both types are useful, but they have very different properties.",2
Technology,Evolutionary programming,"Evolutionary programming is one of the four major evolutionary algorithm paradigms.  It is similar to genetic programming, but the structure of the program to be optimized is fixed, while its numerical parameters are allowed to evolve.
It was first used by Lawrence J. Fogel in the US in 1960 in order to use simulated evolution as a learning process aiming to generate artificial intelligence. Fogel used finite-state machines as predictors and evolved them.
Currently evolutionary programming is a wide evolutionary computing dialect with no fixed structure or (representation), in contrast with some of the other dialects. It has become harder to distinguish from evolutionary strategies.
Its main variation operator is mutation; members of the population are viewed as part of a specific species rather than members of the same species therefore each parent generates an offspring, using a (μ + μ) survivor selection.",2
Technology,Evolvability (computer science),The term evolvability is used for a recent framework of computational learning introduced by Leslie Valiant in his paper of the same name and described below. The aim of this theory is to model biological evolution and categorize which types of mechanisms are evolvable. Evolution is an extension of PAC learning and learning from statistical queries.,2
Technology,Expectation propagation,"Expectation propagation (EP) is a technique in Bayesian machine learning.EP finds approximations to a probability distribution. It uses an iterative approach that uses the factorization structure of the target distribution.  It differs from other Bayesian approximation approaches such as variational Bayesian methods.More specifically, suppose we wish to approximate an intractable probability distribution 
  
    
      
        p
        (
        
          x
        
        )
      
    
    {\displaystyle p(\mathbf {x} )}
   with a tractable distribution 
  
    
      
        q
        (
        
          x
        
        )
      
    
    {\displaystyle q(\mathbf {x} )}
  . Expectation propagation achieves this approximation by minimizing the Kullback-Leibler divergence 
  
    
      
        
          K
          L
        
        (
        p
        
          |
        
        
          |
        
        q
        )
      
    
    {\displaystyle \mathrm {KL} (p||q)}
  . Variational Bayesian methods minimize 
  
    
      
        
          K
          L
        
        (
        q
        
          |
        
        
          |
        
        p
        )
      
    
    {\displaystyle \mathrm {KL} (q||p)}
   instead.If 
  
    
      
        q
        (
        
          x
        
        )
      
    
    {\displaystyle q(\mathbf {x} )}
   is a Gaussian 
  
    
      
        
          
            N
          
        
        (
        
          x
        
        
          |
        
        μ
        ,
        Σ
        )
      
    
    {\displaystyle {\mathcal {N}}(\mathbf {x} |\mu ,\Sigma )}
  , then 
  
    
      
        
          K
          L
        
        (
        p
        
          |
        
        
          |
        
        q
        )
      
    
    {\displaystyle \mathrm {KL} (p||q)}
   is minimized with 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   and 
  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
   being equal to the mean of 
  
    
      
        p
        (
        
          x
        
        )
      
    
    {\displaystyle p(\mathbf {x} )}
   and the covariance of 
  
    
      
        p
        (
        
          x
        
        )
      
    
    {\displaystyle p(\mathbf {x} )}
  , respectively; this is called moment matching.",2
Technology,Explanation-based learning,"Explanation-based learning (EBL) is a form of machine learning that exploits a very strong, or even perfect, domain theory (i.e. a formal theory of an application domain akin to a domain model in ontology engineering, not to be confused with Scott's domain theory) in order to make generalizations or form concepts from training examples. It is also linked with Encoding (memory) to help with Learning.",2
Technology,Fairness (machine learning),"Fairness in Machine Learning refers to the various attempts at correcting algorithmic bias in automated decision processes based on machine learning models. As it is the case with many ethical concepts, definitions of fairness and bias are always controversial. In general, fairness and bias are considered relevant when the decision process impacts people's lives, in particular with respect to a set of variables considered sensitive, such as gender, ethnicity, sexual orientation, disability, etc. In machine learning, the problem of algorithmic bias is well known and well studied. Outcomes may be skewed by a range of factors and thus might be considered unfair with respect to certain groups or individuals. An example would be the way social media sites deliver personalized news to consumers.",2
Technology,Feature (machine learning),"In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of ""feature"" is related to that of explanatory variable used in statistical techniques such as linear regression.",2
Technology,Feature engineering,"Feature engineering or feature extraction  or feature discovery is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data.  The motivation is to use these extra features to improve the quality of results from a machine learning process, compared with supplying only the raw data to the machine learning process. 

",2
Technology,Feature hashing,"In machine learning, feature hashing, also known as the hashing trick (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array. This trick is often attributed to Weinberger et al., but there exists a much earlier description of this method published by John Moody in 1989.eel666_h___ygy__9ydjw",2
Technology,Feature learning,"In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features  and use them to perform  a specific task.
Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.
Feature learning can be either supervised or unsupervised.

In supervised feature learning, features are learned using labeled input data. Examples include supervised neural networks, multilayer perceptron and (supervised) dictionary learning.
In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization and various forms of clustering.

",2
Technology,Feature scaling,"Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.",2
Technology,Federated learning,"Federated learning (also known as collaborative learning) is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging them. This approach stands in contrast to traditional centralized machine learning techniques where all the local datasets are uploaded to one server, as well as to more classical decentralized approaches which often assume that local data samples are identically distributed.
Federated learning enables multiple actors to build a common, robust machine learning model without sharing data, thus allowing to address critical issues such as data privacy, data security, data access rights and access to heterogeneous data. Its applications are spread over a number of industries including defense, telecommunications, IoT, and pharmaceutics.",2
Technology,Flow-based generative model,"A flow-based generative model is a generative model used in machine learning that explicitly models a probability distribution by leveraging normalizing flow, which is a statistical method using the change-of-variable law of probabilities to transform a simple distribution into a complex one.
The direct modeling of likelihood provides many advantages. For example, the negative log-likelihood can be directly computed and minimized as the loss function. Additionally, novel samples can be generated by sampling from the initial distribution, and applying the flow transformation.
In contrast, many alternative generative modeling methods such as variational autoencoder (VAE) and generative adversarial network do not explicitly represent the likelihood function.",2
Technology,Flux (machine-learning framework),"Flux is an open-source machine-learning software library and ecosystem written in Julia. Its current stable release is v0.12.8. It has a layer-stacking-based interface for simpler models, and has a strong support on interoperability with other Julia packages instead of a monolithic design. For example, GPU support is implemented transparently by CuArrays.jl This is in contrast to some other machine learning frameworks which are implemented in other languages with Julia bindings, such as TensorFlow.jl, and thus are more limited by the functionality present in the underlying implementation, which is often in C or C++. Flux joined NumFOCUS as an affiliated project in December of 2021.Flux's focus on interoperability has enabled, for example, support for Neural Differential Equations, by fusing Flux.jl and DifferentialEquations.jl into DiffEqFlux.jl.Flux supports recurrent and convolutional networks. It is also capable of differentiable programming through its source-to-source automatic differentiation package, Zygote.jl.Julia is a popular language in machine-learning and Flux.jl is its most highly regarded machine-learning repository. A demonstration compiling Julia code to run in Google's tensor processing unit (TPU) received praise from Google Brain AI lead Jeff Dean.Flux has been used as a framework to build neural networks that work with homomorphic encrypted data without ever decrypting it. This kind of application is envisioned to be central for privacy to future API using machine-learning models.Flux.jl is an intermediate representation for running high level programs on CUDA hardware. It was the predecessor to CUDAnative.jl which is also a GPU programming language.

",2
Technology,Formal concept analysis,"In information science, formal concept analysis (FCA) is a principled way of deriving a concept hierarchy or formal ontology from a collection of objects and their properties. Each concept in the hierarchy represents the objects sharing some set of properties; and each sub-concept in the hierarchy represents a subset of the objects (as well as a superset of the properties) in the concepts above it. The term was introduced by Rudolf Wille in 1981, and builds on the mathematical theory of lattices and ordered sets that was developed by Garrett Birkhoff and others in the 1930s.
Formal concept analysis finds practical application in fields including data mining, text mining, machine learning, knowledge management, semantic web, software development, chemistry and biology.",2
Technology,Generative model,"In statistical classification, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent, but three major types can be distinguished, following Jebara (2004):

A generative model is a statistical model of the joint probability distribution 
  
    
      
        P
        (
        X
        ,
        Y
        )
      
    
    {\displaystyle P(X,Y)}
   on given observable variable X and target variable Y;
A discriminative model is a model of the conditional probability 
  
    
      
        P
        (
        Y
        ∣
        X
        =
        x
        )
      
    
    {\displaystyle P(Y\mid X=x)}
   of the target Y, given an observation x; and
Classifiers computed without using a probability model are also referred to loosely as ""discriminative"".The distinction between these last two classes is not consistently made; Jebara (2004) refers to these three classes as generative learning, conditional learning, and discriminative learning, but Ng & Jordan (2002) only distinguish two classes, calling them generative classifiers (joint distribution) and discriminative classifiers (conditional distribution or no distribution), not distinguishing between the latter two classes. Analogously, a classifier based on a generative model is a generative classifier, while a classifier based on a discriminative model is a discriminative classifier, though this term also refers to classifiers that are not based on a model.
Standard examples of each, all of which are linear classifiers, are:

generative classifiers:
naive Bayes classifier and
linear discriminant analysis
discriminative model:
logistic regressionIn application to classification, one wishes to go from an observation x to a label y (or probability distribution on labels). One can compute this directly, without using a probability distribution (distribution-free classifier); one can estimate the probability of a label given an observation, 
  
    
      
        P
        (
        Y
        
          |
        
        X
        =
        x
        )
      
    
    {\displaystyle P(Y|X=x)}
   (discriminative model), and base classification on that; or one can estimate the joint distribution 
  
    
      
        P
        (
        X
        ,
        Y
        )
      
    
    {\displaystyle P(X,Y)}
   (generative model), from that compute the conditional probability 
  
    
      
        P
        (
        Y
        
          |
        
        X
        =
        x
        )
      
    
    {\displaystyle P(Y|X=x)}
  , and then base classification on that. These are increasingly indirect, but increasingly probabilistic, allowing more domain knowledge and probability theory to be applied. In practice different approaches are used, depending on the particular problem, and hybrids can combine strengths of multiple approaches.

",2
Technology,Genetic algorithm,"In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection. Some examples of GA applications include optimizing decision trees for better performance, solving sudoku puzzles, hyperparameter optimization, etc.",2
Technology,Glossary of artificial intelligence,"This glossary of artificial intelligence is a list of definitions of terms and concepts relevant to the study of artificial intelligence, its sub-disciplines, and related fields. Related glossaries include Glossary of computer science, Glossary of robotics, and Glossary of machine vision.",2
Technology,Grammar induction,"Grammar induction (or grammatical inference) is the process in machine learning of learning a formal grammar (usually as a collection of re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.",2
Technology,Granular computing,"Granular computing (GrC) is an emerging computing paradigm of information processing that concerns the processing of complex information entities called ""information granules"", which arise in the process of data abstraction and derivation of knowledge from information or data.  Generally speaking, information granules are collections of entities that usually originate at the numeric level and are arranged together due to their similarity, functional or physical adjacency, indistinguishability, coherency, or the like.
At present, granular computing is more a theoretical perspective than a coherent set of methods or principles.  As a theoretical perspective, it encourages an approach to data that recognizes and exploits the knowledge present in data at various levels of resolution or scales.  In this sense, it encompasses all methods which provide flexibility and adaptability in the resolution at which knowledge or information is extracted and represented.",2
Technology,Graph neural network,"A graph neural network (GNN) is a class of neural network for processing data best represented by graph data structures. They were popularized by their use in supervised learning on properties of various molecules.Since their inception, variants of the message passing neural network (MPNN) framework have been proposed. These models optimize GNNs for use on larger graphs and apply them to domains such as social networks, citation networks, and online communities. GNNs have also been relatively successful in various NP-hard combinatorial problems, automated planning and path-planning areas due to the inherent graph structure of data.   GNNs are a weak form of the Weisfeiler–Lehman graph isomorphism test, so a GNN model can be at least as powerful as this test if it meets certain conditions. Researchers are attempting to unite GNNs with other ""geometric deep learning models"" to better understand how and why these models work.
In the case of the absence of a known graph structure for example a k-nearest neighbor graph can be heuristically induced.
GNNs can be understood as a generalization of convolutional neural networks (which are used on 2-dimensional black and white image data and 3-dimensional color image data) to graph-structured data.

",2
Technology,Highway network,"In machine learning, the Highway Network was the first working very deep feedforward neural network with hundreds of layers, much deeper than previous artificial neural networks.
It uses skip connections modulated by learned gating mechanisms to regulate information flow, inspired by Long Short-Term Memory (LSTM) recurrent neural networks.
The advantage of a Highway Network over the common deep neural networks is that it solves or partially prevents the vanishing gradient problem, thus leading to easier to optimize neural networks.
The gating mechanisms facilitate information flow across many layers (""information highways"").Highway Networks have been used as part of text sequence labeling and speech recognition tasks.
An open-gated or gateless Highway Network variant called Residual neural network was used to win the ImageNet 2015 competition. This has become the most cited neural network of the 21st century.",2
Technology,Hyperparameter (machine learning),"In machine learning, a hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are derived via training.
Hyperparameters can be classified as model hyperparameters, that cannot be inferred while fitting the machine to the training set because they refer to the model selection task, or algorithm hyperparameters, that in principle have no influence on the performance of the model but affect the speed and quality of the learning process. An example of a model hyperparameter is the topology and size of a neural network. Examples of algorithm hyperparameters are learning rate and batch size as well as mini-batch size. Batch size can refer to the full data sample where mini-batch size would be a smaller sample set.
Different model training algorithms require different hyperparameters, some simple algorithms (such as ordinary least squares regression) require none. Given these hyperparameters, the training algorithm learns the parameters from the data. For instance, LASSO is an algorithm that adds a regularization hyperparameter to ordinary least squares regression, which has to be set before estimating the parameters through the training algorithm.",2
Technology,Hyperparameter optimization,"In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.
The same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data.  The objective function takes a tuple of hyperparameters and returns the associated loss. Cross-validation is often used to estimate this generalization performance.",2
Technology,Inauthentic text,"An inauthentic text is a computer-generated expository document meant to appear as genuine, but which is actually meaningless.  Frequently they are created in order to be intermixed with genuine documents and thus manipulate the results of search engines, as with Spam blogs.  They are also carried along in email in order to fool spam filters by giving the spam the superficial characteristics of legitimate text.
Sometimes nonsensical documents are created with computer assistance for humorous effect, as with Dissociated press or Flarf poetry.  They have also been used to challenge the veracity of a publication—MIT students submitted papers generated by a computer program called SCIgen to a conference, where they were initially accepted.  This led the students to claim that the bar for submissions was too low.
With the amount of computer generated text outpacing the ability of people to humans to curate it, there needs some means of distinguishing between the two.  Yet automated approaches to determining absolutely whether a text is authentic or not face intrinsic challenges of semantics.  Noam Chomsky coined the phrase ""Colorless green ideas sleep furiously"" giving an example of grammatically-correct, but semantically incoherent sentence; some will point out that in certain contexts one could give this sentence (or any phrase) meaning.
The first group to use the expression in this regard can be found below from Indiana University.  Their work explains in detail an attempt to detect inauthentic texts and identify pernicious problems of inauthentic texts in cyberspace.  The site has a means of submitting text that assesses, based on supervised learning, whether a corpus is inauthentic or not.  Many users have submitted incorrect types of data and have correspondingly commented on the scores. This application is meant for a specific kind of data; therefore, submitting, say, an email, will not return a meaningful score.",2
Technology,Inductive bias,"The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered.In machine learning, one aims to construct algorithms that are able to learn to predict a certain target output. To achieve this, the learning algorithm is presented some training examples that demonstrate the intended relation of input and output values. Then the learner is supposed to approximate the correct output, even for examples that have not been shown during training.  Without any additional assumptions, this problem cannot be solved since unseen situations might have an arbitrary output value. The kind of necessary assumptions about the nature of the target function are subsumed in the phrase inductive bias.A classical example of an inductive bias is Occam's razor, assuming that the simplest consistent hypothesis about the target function is actually the best. Here consistent means that the hypothesis of the learner yields correct outputs for all of the examples that have been given to the algorithm.
Approaches to a more formal definition of inductive bias are based on mathematical logic. Here, the inductive bias is a logical formula that, together with the training data, logically entails the hypothesis generated by the learner. However, this strict formalism fails in many practical cases, where the inductive bias can only be given as a rough description (e.g. in the case of artificial neural networks), or not at all.

",2
Technology,Inductive probability,"Inductive probability attempts to give the probability of future events based on past events. It is the basis for inductive reasoning, and gives the mathematical basis for learning and the perception of patterns. It is a source of knowledge about the world.
There are three sources of knowledge: inference, communication, and deduction. Communication relays information found using other methods.  Deduction establishes new facts based on existing facts.  Inference establishes new facts from data. Its basis is Bayes' theorem.
Information describing the world is written in a language. For example, a simple mathematical language of propositions may be chosen. Sentences may be written down in this language as strings of characters.  But in the computer it is possible to encode these sentences as strings of bits (1s and 0s). Then the language may be encoded so that the most commonly used sentences are the shortest. This internal language implicitly represents probabilities of statements.
Occam's razor says the ""simplest theory, consistent with the data is most likely to be  correct"". The ""simplest theory"" is interpreted as the representation of the theory written in this internal language. The theory with the shortest encoding in this internal language is most likely to be correct.",2
Technology,Inductive programming,"Inductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints.
Depending on the programming language used, there are several kinds of inductive programming. Inductive functional programming, which uses functional programming languages such as Lisp or Haskell, and most especially inductive logic programming, which uses logic programming languages such as Prolog and other logical representations  such as description logics, have been more prominent, but other (programming) language paradigms have also been used, such as constraint programming or probabilistic programming.

",2
Technology,Inferential theory of learning,"Inferential Theory of Learning (ITL) is an area of machine learning which describes inferential processes performed by learning agents. ITL has been continuously developed by Ryszard S. Michalski, starting in the 1980s. The first known publication of ITL was in 1983. In ITL learning process is viewed as a search (inference) through hypotheses space guided by a specific goal. Results of learning need to be stored. Stored information will later be used by the learner for future inferences. Inferences are split into multiple categories including conclusive, deduction, and induction. In order for an inference to be considered complete it was required that all categories must be taken into account. This is how the ITL varies from other machine learning theories like Computational Learning Theory and Statistical Learning Theory; which both use singular forms of inference.",2
Technology,Instance selection,"Instance selection (or dataset reduction, or dataset condensation) is an important data pre-processing step that can be applied in many machine learning (or data mining) tasks. Approaches for instance selection can be applied for reducing the original dataset to a manageable volume, leading to a reduction of the computational resources that are necessary for performing the learning process. Algorithms of instance selection can also be applied for removing noisy instances, before applying learning algorithms. This step can improve the accuracy in classification problems.
Algorithm for instance selection should identify a subset of the total available data to achieve the original purpose of the data mining (or machine learning) application as if the whole data had been used. Considering this, the optimal outcome of IS would be the minimum data subset that can accomplish the same task with no performance loss, in comparison with the performance achieved when the task is performed using the whole available data. Therefore, every instance selection strategy should deal with a trade-off between the reduction rate of the dataset and the classification quality.",2
Technology,Instance-based learning,"In machine learning, instance-based learning (sometimes called memory-based learning) is a family of learning algorithms that, instead of performing explicit generalization, compare new problem instances with instances seen in training, which have been stored in memory. Because computation is postponed until a new instance is observed, these algorithms are sometimes referred to as ""lazy.""It is called instance-based because it constructs hypotheses directly from the training instances themselves.
This means that the hypothesis complexity can grow with the data: in the worst case, a hypothesis is a list of n training items and the computational complexity of classifying a single new instance is O(n). One advantage that instance-based learning has over other methods of machine learning is its ability to adapt its model to previously unseen data. Instance-based learners may simply store a new instance or throw an old instance away.
Examples of instance-based learning algorithms are the k-nearest neighbors algorithm, kernel machines and RBF networks.: ch. 8  These store (a subset of) their training set; when predicting a value/class for a new instance, they compute distances or similarities between this instance and the training instances to make a decision.
To battle the memory complexity of storing all training instances, as well as the risk of overfitting to noise in the training set, instance reduction algorithms have been proposed.",2
Technology,Instantaneously trained neural networks,"Instantaneously trained neural networks are feedforward artificial neural networks that create a new hidden neuron node for each novel training sample. The weights to this hidden neuron separate out not only this training sample but others that are near it, thus providing generalization.  This separation is done using the nearest hyperplane that can be written down instantaneously. In the two most important implementations the neighborhood of generalization either varies with the training sample (CC1 network) or remains constant (CC4 network). These networks use unary coding for an effective representation of the data sets.This type of network was first proposed in a 1993 paper of Subhash Kak. Since then, instantaneously trained neural networks have been proposed as models of short term learning and used in web search, and financial time series prediction applications. They have also been used in instant classification of documents and for deep learning and data mining.As in other neural networks, their normal use is as software, but they have also been implemented in hardware using FPGAs and by optical implementation.",2
Technology,Intelligent automation,"Intelligent automation, or alternately intelligent process automation, is a software term that refers to a combination of artificial intelligence (AI) and robotic process automation (RPA). Companies use intelligent automation to cut costs by using artificial-intelligence-powered robotic software to replace workers who handle repetitive tasks. The term is similar to hyperautomation, a concept identified by research group Gartner as being one of the top technology trends of 2020.",2
Technology,Isotropic position,"In the fields of machine learning, the theory of computation, and random matrix theory, a probability distribution over vectors is said to be in isotropic position if its covariance matrix is equal to the identity matrix. 

",2
Technology,Journal of Machine Learning Research,"The Journal of Machine Learning Research is a peer-reviewed open access scientific journal covering machine learning. It was established in 2000 and the first editor-in-chief was Leslie Kaelbling. The current editors-in-chief are Francis Bach (Inria), David Blei (Columbia University) and Bernhard Schölkopf (Max Planck Institute for Intelligent Systems).",2
Technology,Kernel density estimation,"In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable.  Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzen–Rosenblatt window method,  after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form. One of the famous applications of kernel density estimation is in estimating the class-conditional marginal densities of data when using a naive Bayes classifier, which can improve its prediction accuracy.",2
Technology,Kernel embedding of distributions,"In machine learning, the kernel embedding of distributions (also called the kernel mean or mean map) comprises a class of nonparametric methods in which a probability distribution is represented as an element of a reproducing kernel Hilbert space  (RKHS).   A generalization of the individual data-point feature mapping done in classical kernel methods, the embedding of distributions into infinite-dimensional feature spaces can preserve all of the statistical features of arbitrary distributions, while allowing one to compare and manipulate distributions using Hilbert space operations such as inner products, distances, projections, linear transformations, and spectral analysis.    This learning framework is very general and can be applied to distributions over any space 
  
    
      
        Ω
      
    
    {\displaystyle \Omega }
   on which a sensible kernel function (measuring similarity between elements of 
  
    
      
        Ω
      
    
    {\displaystyle \Omega }
  ) may be defined.  For example, various kernels have been proposed for learning from data which are: vectors in 
  
    
      
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle \mathbb {R} ^{d}}
  , discrete classes/categories, strings, graphs/networks, images, time series, manifolds, dynamical systems, and other structured objects.  The theory behind kernel embeddings of distributions has been primarily developed by  Alex Smola, Le Song , Arthur Gretton, and Bernhard Schölkopf. A review of recent works on kernel embedding of distributions can be found in.The analysis of distributions is fundamental in machine learning and statistics,  and many algorithms in these fields rely on information theoretic approaches such as entropy, mutual information, or Kullback–Leibler divergence.  However, to estimate these quantities, one must first either perform density estimation, or employ sophisticated space-partitioning/bias-correction strategies which are typically infeasible for high-dimensional data.  Commonly, methods for modeling complex distributions rely on parametric assumptions that may be unfounded or computationally challenging (e.g. Gaussian mixture models), while nonparametric methods like kernel density estimation (Note: the smoothing kernels in this context have a different interpretation than the kernels discussed here) or characteristic function representation (via the Fourier transform of the distribution) break down in high-dimensional settings.Methods based on the kernel embedding of distributions sidestep these problems and also possess the following advantages:
Data may be modeled without restrictive assumptions about the form of the distributions and relationships between variables
Intermediate density estimation is not needed
Practitioners may specify the properties of a distribution most relevant for their problem (incorporating prior knowledge via choice of the kernel)
If a characteristic kernel is used, then the embedding can uniquely preserve all information about a distribution, while thanks to the kernel trick, computations on the potentially infinite-dimensional RKHS can be implemented in practice as simple Gram matrix operations
Dimensionality-independent rates of convergence for the empirical kernel mean (estimated using samples from the distribution)  to the kernel embedding of the true underlying distribution can be proven.
Learning algorithms based on this framework exhibit good generalization ability and finite sample convergence, while often being simpler and more effective than information theoretic methodsThus, learning via the kernel embedding of distributions offers a principled drop-in replacement for information theoretic approaches and is a framework which not only subsumes many popular methods in machine learning and statistics as special cases, but also can lead to entirely new learning algorithms.",2
Technology,Knowledge distillation,"In machine learning, knowledge distillation is the process of transferring knowledge from a large model to a smaller one. While large models (such as very deep neural networks or ensembles of many models) have higher knowledge capacity than small models, this capacity might not be fully utilized.  It can be computationally just as expensive to evaluate a model even if it utilizes little of its knowledge capacity.  Knowledge distillation transfers knowledge from a large model to a smaller model without loss of validity.  As smaller models are less expensive to evaluate, they can be deployed on less powerful hardware (such as a mobile device).Knowledge distillation has been successfully used in several applications of machine learning such as object detection, acoustic models, and natural language processing.
Recently, it has also been introduced to graph neural networks applicable to non-grid data.",2
Technology,Knowledge graph embedding,"In representation learning, knowledge graph embedding (KGE), also referred to as knowledge representation learning (KRL), or multi-relation learning, is a machine learning task of learning a low-dimensional representation of a knowledge graph's entities and relations while preserving their semantic meaning.  Leveraging their embedded representation, knowledge graphs (KGs) can be used for various applications such as link prediction, triple classification, entity recognition, clustering, and relation extraction.",2
Technology,Knowledge integration,"Knowledge integration is the process of synthesizing multiple knowledge models (or representations) into a common model (representation).
Compared to information integration, which involves merging information having different schemas and representation models, knowledge integration focuses more on synthesizing the understanding of a given subject from different perspectives.
For example, multiple interpretations are possible of a set of student grades, typically each from a certain perspective. An overall, integrated view and understanding of this information can be achieved if these interpretations can be put under a common model, say, a student performance index.
The Web-based Inquiry Science Environment (WISE), from the University of California at Berkeley has been developed along the lines of knowledge integration theory.
Knowledge integration has also been studied as the process of incorporating new information into a body of existing knowledge with an interdisciplinary approach.  This process involves determining how the new information and the existing knowledge interact, how existing knowledge should be modified to accommodate the new information, and how the new information should be modified in light of the existing knowledge.
A learning agent that actively investigates the consequences of new information can detect and exploit a variety of learning opportunities; e.g., to resolve knowledge conflicts and to fill knowledge gaps.  By exploiting these learning opportunities the learning agent is able to learn beyond the explicit content of the new information.
The machine learning program KI, developed by Murray and Porter at the University of Texas at Austin, was created to study the use of automated and semi-automated knowledge integration to assist knowledge engineers constructing a large knowledge base.
A possible technique which can be used is semantic matching. More recently, a technique useful to minimize the effort in mapping validation and visualization has been presented which is based on Minimal Mappings. Minimal mappings are high quality mappings such that i) all the other mappings can be computed from them in time linear in the size of the input graphs, and ii) none of them can be dropped without losing property i).
The University of Waterloo operates a Bachelor of Knowledge Integration undergraduate degree program as an academic major or minor. The program started in 2008.

",2
Technology,Labeled data,"Labeled data is a group of samples that have been tagged with one or more labels. Labeling typically takes a set of unlabeled data and augments each piece of it with informative tags. For example, a data label might indicate whether a photo contains a horse or a cow, which words were uttered in an audio recording, what type of action is being performed in a video, what the topic of a news article is, what the overall sentiment of a tweet is, or whether a dot in an X-ray is a tumor.
Labels can be obtained by asking humans to make judgments about a given piece of unlabeled data. Labeled data is significantly more expensive to obtain than the raw unlabeled data.",2
Technology,Large margin nearest neighbor,"Large margin nearest neighbor (LMNN) classification is a statistical machine learning algorithm for metric learning. It learns a pseudometric designed for k-nearest neighbor classification. The algorithm is based on semidefinite programming, a sub-class of convex optimization.
The goal of supervised learning (more specifically classification) is to learn a decision rule that can categorize data instances into pre-defined classes. The  k-nearest neighbor rule assumes a training data set of labeled instances (i.e. the classes are known). It classifies a new data instance with the class obtained from the majority vote of the k closest (labeled) training instances. Closeness is measured with a pre-defined metric. Large margin nearest neighbors is an algorithm that learns this global (pseudo-)metric in a supervised fashion to improve the classification accuracy of the k-nearest neighbor rule.",2
Technology,Large width limits of neural networks,"Artificial neural networks are a class of models used in machine learning, and inspired by biological neural networks. They are the core component of modern deep learning algorithms. Computation in artificial neural networks is usually organized into sequential layers of artificial neurons. The number of neurons in a layer is called the layer width. Theoretical analysis of artificial neural networks sometimes considers the limiting case that layer width becomes large or infinite. This limit enables simple analytic statements to be made about neural network predictions, training dynamics, generalization, and loss surfaces. This wide layer limit is also of practical interest, since finite width neural networks often perform strictly better as layer width is increased.

",2
Technology,Latent space,"A latent space, also known as a latent feature space or embedding space, is an embedding of a set of items within a manifold in which items which resemble each other more closely are positioned closer to one another in the latent space. Position within the latent space can be viewed as being defined by a set of latent variables that emerge from the resemblances from the objects.
In most cases, the dimensionality of the latent space is chosen to be lower than the dimensionality of the feature space from which the data points are drawn, making the construction of a latent space an example of dimensionality reduction, which can also be viewed as a form of data compression or machine learning.
A number of algorithms exist to create latent space embeddings given a set of data items and a similarity function.

",2
Technology,Lazy learning,"In machine learning, lazy learning is a learning method in which generalization of the training data is, in theory, delayed until a query is made to the system, as opposed to eager learning, where the system tries to generalize the training data before receiving queries.The primary motivation for employing lazy learning, as in the K-nearest neighbors algorithm, used by online recommendation systems (""people who viewed/purchased/listened to this movie/item/tune also ..."") is that the data set is continuously updated with new entries (e.g., new items for sale at Amazon, new movies to view at Netflix, new clips at YouTube, new music at Spotify or Pandora). Because of the continuous update, the ""training data"" would be rendered obsolete in a relatively short time especially in areas like books and movies, where new best-sellers or hit movies/music are published/released continuously. Therefore, one cannot really talk of a ""training phase"".
Lazy classifiers are most useful for large, continuously changing datasets with few attributes that are commonly queried. Specifically, even if a large set of attributes exist - for example, books have a year of publication, author/s, publisher, title, edition, ISBN, selling price, etc. - recommendation queries rely on far fewer attributes - e.g., purchase or viewing co-occurrence data, and user ratings of items purchased/viewed.",2
Technology,Leakage (machine learning),"In statistics and machine learning, leakage (also known as data leakage or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time, causing the predictive scores (metrics) to overestimate the model's utility when run in a production environment.Leakage is often subtle and indirect, making it hard to detect and eliminate. Leakage can cause a statistician or modeler to select a suboptimal model, which could be outperformed by a leakage-free model.",2
Technology,Learnable function class,"In statistical learning theory, a learnable function class is a set of functions for which an algorithm can be devised to asymptotically minimize the expected risk, uniformly over all probability distributions. The concept of learnable classes are closely related to regularization in machine learning, and provides large sample justifications for certain learning algorithms.

",2
Technology,Learning automaton,"A learning automaton is one type of machine learning algorithm studied since 1970s. Learning automata select their current action based on past experiences from the environment. It will fall into the range of reinforcement learning if the environment is stochastic and a Markov decision process (MDP) is used.

",2
Technology,Learning curve (machine learning),"Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', that is, methods that leverage data to improve performance on some set of tasks. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers; but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning. Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain.  In its application across business problems, machine learning is also referred to as predictive analytics.",2
Technology,Learning rate,"In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. Since it influences to what extent newly acquired information overrides old information, it metaphorically represents the speed at which a machine learning model ""learns"". In the adaptive control literature, the learning rate is commonly referred to as gain.In setting a learning rate, there is a trade-off between the rate of convergence and overshooting. While the descent direction is usually determined from the gradient of the loss function, the learning rate determines how big a step is taken in that direction. A too high learning rate will make the learning jump over minima but a too low learning rate will either take too long to converge or get stuck in an undesirable local minimum.In order to achieve faster convergence, prevent oscillations and getting stuck in undesirable local minima the learning rate is often varied during training either in accordance to a learning rate schedule or by using an adaptive learning rate. The learning rate and its adjustments may also differ per parameter, in which case it is a diagonal matrix that can be interpreted as an approximation to the inverse of the Hessian matrix in Newton's method. The learning rate is related to the step length determined by inexact line search in quasi-Newton methods and related optimization algorithms.When conducting line searches, mini-batch sub-sampling (MBSS) affect the characteristics of the loss function along which the learning rate needs to be resolved. Static MBSS keeps the mini-batch fixed along a search direction, resulting in a smooth loss function along the search direction. Dynamic MBSS updates the mini-batch at every function evaluation, resulting in a point-wise discontinuous loss function along the search direction. Line searches that adaptively resolve learning rates for static MBSS loss functions include the parabolic approximation line (PAL) search. Line searches that adaptively resolve learning rates for dynamic MBSS loss functions include probabilistic line searches, gradient-only line searches (GOLS) and quadratic approximations.",2
Technology,Learning to rank,"Learning to rank or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems. Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. ""relevant"" or ""not relevant"") for each item. The goal of constructing the ranking model is to rank new, unseen lists in a similar way to rankings in the training data.

",2
Technology,Learning with errors,"Learning with errors (LWE) is the computational problem of inferring a linear 
  
    
      
        n
      
    
    {\displaystyle n}
  -ary function 
  
    
      
        f
      
    
    {\displaystyle f}
   over a finite ring from given samples 
  
    
      
        
          y
          
            i
          
        
        =
        f
        (
        
          
            x
          
          
            i
          
        
        )
      
    
    {\displaystyle y_{i}=f(\mathbf {x} _{i})}
   some of which may be erroneous.
The LWE problem is conjectured to be hard to solve, and thus to be useful in cryptography.
More precisely, the LWE problem is defined as follows. Let 
  
    
      
        
          
            Z
          
          
            q
          
        
      
    
    {\displaystyle \mathbb {Z} _{q}}
   denote the ring of integers modulo 
  
    
      
        q
      
    
    {\displaystyle q}
   and let

  
    
      
        
          
            Z
          
          
            q
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {Z} _{q}^{n}}
   denote the set of 
  
    
      
        n
      
    
    {\displaystyle n}
  -vectors over 
  
    
      
        
          
            Z
          
          
            q
          
        
      
    
    {\displaystyle \mathbb {Z} _{q}}
  . There exists a certain unknown linear function 
  
    
      
        f
        :
        
          
            Z
          
          
            q
          
          
            n
          
        
        →
        
          
            Z
          
          
            q
          
        
      
    
    {\displaystyle f:\mathbb {Z} _{q}^{n}\rightarrow \mathbb {Z} _{q}}
  , and the input to the LWE problem is a sample of pairs 
  
    
      
        (
        
          x
        
        ,
        y
        )
      
    
    {\displaystyle (\mathbf {x} ,y)}
  , where 
  
    
      
        
          x
        
        ∈
        
          
            Z
          
          
            q
          
          
            n
          
        
      
    
    {\displaystyle \mathbf {x} \in \mathbb {Z} _{q}^{n}}
   and 
  
    
      
        y
        ∈
        
          
            Z
          
          
            q
          
        
      
    
    {\displaystyle y\in \mathbb {Z} _{q}}
  , so that with high probability 
  
    
      
        y
        =
        f
        (
        
          x
        
        )
      
    
    {\displaystyle y=f(\mathbf {x} )}
  . Furthermore, the deviation from the equality is according to some known noise model. The problem calls for finding the function 
  
    
      
        f
      
    
    {\displaystyle f}
  , or some close approximation thereof, with high probability.
The LWE problem was introduced by Oded Regev in 2005 (who won the 2018 Gödel Prize for this work), it is a generalization of the parity learning problem. Regev showed that the LWE problem is as hard to solve as several worst-case lattice problems. Subsequently, the LWE problem has been used as a hardness assumption to create public-key cryptosystems, such as the ring learning with errors key exchange by Peikert.",2
Technology,Leave-one-out error,"For mathematical analysis and statistics, Leave-one-out error can refer to the following:

Leave-one-out cross-validation Stability (CVloo, for stability of Cross Validation with leave one out): An algorithm f has CVloo stability β with respect to the loss function V if the following holds:
  
    
      
        ∀
        i
        ∈
        {
        1
        ,
        .
        .
        .
        ,
        m
        }
        ,
        
          
            P
          
          
            S
          
        
        {
        
          sup
          
            z
            ∈
            Z
          
        
        
          |
        
        V
        (
        
          f
          
            S
          
        
        ,
        
          z
          
            i
          
        
        )
        −
        V
        (
        
          f
          
            
              S
              
                
                  |
                
                i
              
            
          
        
        ,
        
          z
          
            i
          
        
        )
        
          |
        
        ≤
        
          β
          
            C
            V
          
        
        }
        ≥
        1
        −
        
          δ
          
            C
            V
          
        
      
    
    {\displaystyle \forall i\in \{1,...,m\},\mathbb {P} _{S}\{\sup _{z\in Z}|V(f_{S},z_{i})-V(f_{S^{|i}},z_{i})|\leq \beta _{CV}\}\geq 1-\delta _{CV}}
  

Expected-to-leave-one-out error Stability (
  
    
      
        E
        l
        o
        
          o
          
            e
            r
            r
          
        
      
    
    {\displaystyle Eloo_{err}}
  , for Expected error from leaving one out): An algorithm f has 
  
    
      
        E
        l
        o
        
          o
          
            e
            r
            r
          
        
      
    
    {\displaystyle Eloo_{err}}
   stability if for each n there exists a
  
    
      
        
          β
          
            E
            L
          
          
            m
          
        
      
    
    {\displaystyle \beta _{EL}^{m}}
   and a 
  
    
      
        
          δ
          
            E
            L
          
          
            m
          
        
      
    
    {\displaystyle \delta _{EL}^{m}}
   such that:
  
    
      
        ∀
        i
        ∈
        {
        1
        ,
        .
        .
        .
        ,
        m
        }
        ,
        
          
            P
          
          
            S
          
        
        {
        
          |
        
        I
        [
        
          f
          
            S
          
        
        ]
        −
        
          
            1
            m
          
        
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        V
        (
        
          f
          
            
              S
              
                
                  |
                
                i
              
            
          
        
        ,
        
          z
          
            i
          
        
        )
        
          |
        
        ≤
        
          β
          
            E
            L
          
          
            m
          
        
        }
        ≥
        1
        −
        
          δ
          
            E
            L
          
          
            m
          
        
      
    
    {\displaystyle \forall i\in \{1,...,m\},\mathbb {P} _{S}\{|I[f_{S}]-{\frac {1}{m}}\sum _{i=1}^{m}V(f_{S^{|i}},z_{i})|\leq \beta _{EL}^{m}\}\geq 1-\delta _{EL}^{m}}
  , with 
  
    
      
        
          β
          
            E
            L
          
          
            m
          
        
      
    
    {\displaystyle \beta _{EL}^{m}}
  and 
  
    
      
        
          δ
          
            E
            L
          
          
            m
          
        
      
    
    {\displaystyle \delta _{EL}^{m}}
   going to zero for 
  
    
      
        n
        →
        inf
      
    
    {\displaystyle n\rightarrow \inf }",2
Technology,Life-time of correlation,"In probability theory and related fields, the life-time of correlation measures the timespan over which there is appreciable autocorrelation or cross-correlation in stochastic processes.",2
Technology,Linear predictor function,"In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable.  This sort of function usually comes in linear regression, where the coefficients are called regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis.  In many of these models, the coefficients are referred to as ""weights"".

",2
Technology,Linear separability,"In Euclidean geometry, linear separability is a property of two sets of points. This is most easily visualized in two dimensions (the Euclidean plane) by thinking of one set of points as being colored blue and the other set of points as being colored red. These two sets are linearly separable if there exists at least one line in the plane with all of the blue points on one side of the line and all the red points on the other side. This idea immediately generalizes to higher-dimensional Euclidean spaces if the line is replaced by a hyperplane.
The problem of determining if a pair of sets is linearly separable and finding a separating hyperplane if they are, arises in several areas.  In statistics and machine learning, classifying certain types of data is a problem for which good algorithms exist that are based on this concept.

",2
Technology,Local case-control sampling,"In machine learning, local case-control sampling  is an algorithm used to reduce the complexity of training a logistic regression classifier. The algorithm reduces the training complexity by selecting a small subsample of the original dataset for training. It assumes the availability of a (unreliable) pilot estimation of the parameters. It then performs a single pass over the entire dataset using the pilot estimation to identify the most ""surprising"" samples. In practice, the pilot may come from prior knowledge or training using a subsample of the dataset. The algorithm is most effective when the underlying dataset is imbalanced. It exploits the structures of conditional imbalanced datasets more efficiently than alternative methods, such as case control sampling and weighted case control sampling.

",2
Technology,M-Theory (learning framework),"In Machine Learning and Computer Vision, M-Theory is a learning framework inspired by feed-forward processing in the ventral stream of visual cortex and originally developed for recognition and classification of objects in visual scenes. M-Theory was later applied to other areas, such as speech recognition. On certain image recognition tasks, algorithms based on a specific instantiation of M-Theory, HMAX, achieved human-level performance.The core principle of M-Theory is extracting representations invariant to various transformations of images (translation, scale, 2D and 3D rotation and others). In contrast with other approaches using invariant representations, in M-Theory they are not hardcoded into the algorithms, but learned. M-Theory also shares some principles with Compressed Sensing. The theory proposes multilayered hierarchical learning architecture, similar to that of visual cortex.",2
Technology,Logic learning machine,"Logic learning machine (LLM) is a machine learning method based on the generation of intelligible rules. LLM is an efficient implementation of the Switching Neural Network (SNN) paradigm, developed by Marco Muselli, Senior Researcher at the Italian National Research Council CNR-IEIIT in Genoa.
LLM has been employed in many different sectors, including the field of medicine (orthopedic patient classification, DNA micro-array analysis  and Clinical Decision Support Systems ), financial services and supply chain management.",2
Technology,Machine Learning (journal),"Machine Learning  is a peer-reviewed scientific journal, published since 1986.
In 2001, forty editors and members of the editorial board of Machine Learning resigned in order to support the Journal of Machine Learning Research (JMLR), saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet.Following the mass resignation, Kluwer changed their publishing policy to allow authors to self-archive their papers online after peer-review.",2
Technology,Machine learning control,"Machine learning control (MLC) is a subfield of machine learning, intelligent control and control theory
which solves optimal control problems with methods of machine learning.
Key applications are complex nonlinear systems
for which linear control theory methods are not applicable.

",2
Technology,Machine learning in bioinformatics,"Machine learning in bioinformatics is the application of machine learning algorithms to bioinformatics, including genomics, proteomics, microarrays, systems biology, evolution, and text mining.Prior to the emergence of machine learning, bioinformatics algorithms had to be programmed by hand; for problems such as protein structure prediction, this proved difficult. Machine learning techniques, such as deep learning can learn features of data sets, instead of requiring the programmer to define them individually. The algorithm can further learn how to combine low-level features into more abstract features, and so on. This multi-layered approach allows such systems to make sophisticated predictions when appropriately trained. These methods contrast with other computational biology approaches which, while exploiting existing datasets, do not allow the data to be interpreted and analyzed in unanticipated ways. In recent years, the size and number of available biological datasets have skyrocketed.",2
Technology,Machine learning in earth sciences,"Applications of machine learning in earth sciences include geological mapping, gas leakage detection and geological features identification. Machine learning (ML) is a type of artificial intelligence (AI) that enables computer systems to classify, cluster, identify and analyze vast and complex sets of data while eliminating the need for explicit instructions and programming. Earth science is the study of the origin, evolution, and future of the planet Earth. The Earth system can be subdivided into four major components including the solid earth, atmosphere, hydrosphere and biosphere.A variety of algorithms may be applied depending on the nature of the earth science exploration. Some algorithms may perform significantly better than others for particular objectives. For example, convolutional neural networks (CNN) are good at interpreting images, artificial neural networks (ANN) perform well in soil classification but more computationally expensive to train than support-vector machine (SVM) learning. The application of machine learning has been popular in recent decades, as the development of other technologies such as unmanned aerial vehicles (UAVs), ultra-high resolution remote sensing technology and high-performance computing units lead to the availability of large high-quality datasets and more advanced algorithms.",2
Technology,Machine learning in physics,"Applying classical methods of machine learning to the study of quantum systems is the focus of an emergent area of physics research. A basic example of this is quantum state tomography, where a quantum state is learned from measurement. Other examples include learning Hamiltonians, learning quantum phase transitions, and automatically generating new quantum experiments. Classical machine learning is effective at processing large amounts of experimental or calculated data in order to characterize an unknown quantum system, making its application useful in contexts including quantum information theory, quantum technologies development, and computational materials design. In this context, it can be used for example as a tool to interpolate pre-calculated interatomic potentials or directly solving the Schrödinger equation with a variational method.

",2
Technology,Machine learning in video games,"In video games, various artificial intelligence techniques have been used in a variety of ways, ranging from non-player character (NPC) control to procedural content generation (PCG). Machine learning is a subset of artificial intelligence that focuses on using algorithms and statistical models to make machines act without specific programming. This is in sharp contrast to traditional methods of artificial intelligence such as search trees and expert systems.
Information on machine learning techniques in the field of games is mostly known to public through research projects as most gaming companies choose not to publish specific information about their intellectual property. The most publicly known application of machine learning in games is likely the use of deep learning agents that compete with professional human players in complex strategy games. There has been a significant application of machine learning on games such as Atari/ALE, Doom, Minecraft, StarCraft, and car racing. Other games that did not originally exists as video games, such as chess and Go have also been affected by the machine learning.",2
Technology,Manifold hypothesis,"In theoretical computer science and the study of machine learning, the manifold hypothesis is the hypothesis that many high-dimensional data sets that occur in the real world actually lie along low-dimensional manifolds inside that high-dimensional space. As a consequence of the manifold hypothesis, many data sets that appear to initially require many variables to describe, can actually be described by a comparatively small number of variables, likened to the local coordinate system of the underlying manifold. It is suggested that this principle underpins the effectiveness of machine learning algorithms in describing high-dimensional data sets by considering a few common features.
The manifold hypothesis is related to the effectiveness of nonlinear dimensionality reduction techniques in machine learning. Many techniques of dimensional reduction make the assumption that data lies along a low-dimensional submanifold, such as manifold sculpting, manifold alignment, and manifold regularization.

",2
Technology,Manifold regularization,"In machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathematical structure with useful properties. The technique also assumes that the function to be learned is smooth: data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of Tikhonov regularization. Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition.",2
Technology,The Master Algorithm,"The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field.

",2
Technology,Matchbox Educable Noughts and Crosses Engine,"The Matchbox Educable Noughts and Crosses Engine (sometimes called the Machine Educable Noughts and Crosses Engine or MENACE) was a mechanical computer made from 304 matchboxes designed and built by artificial intelligence researcher Donald Michie in 1961. It was designed to play human opponents in games of noughts and crosses (tic-tac-toe) by returning a move for any given state of play and to refine its strategy through reinforcement learning.
Michie did not have a computer readily available, so he worked around this restriction by building it out of matchboxes. The matchboxes used by Michie each represented a single possible layout of a noughts and crosses grid. When the computer first played, it would randomly choose moves based on the current layout. As it played more games, through a reinforcement loop, it disqualified strategies that led to losing games, and supplemented strategies that led to winning games. Michie held a tournament against MENACE in 1961, wherein he experimented with different openings.
Following MENACE's maiden tournament against Michie, it demonstrated successful artificial intelligence in its strategy. Michie's essays on MENACE's weight initialisation and the BOXES algorithm used by MENACE became popular in the field of computer science research. Michie was honoured for his contribution to machine learning research, and was twice commissioned to program a MENACE simulation on an actual computer.",2
Technology,Matrix regularization,"In the field of statistical learning theory, matrix regularization generalizes notions of vector regularization to cases where the object to be learned is a matrix. The purpose of regularization is to enforce conditions, for example sparsity or smoothness, that can produce stable predictive functions. For example, in the more common vector framework, Tikhonov regularization optimizes over

  
    
      
        
          min
          
            x
          
        
        ‖
        A
        x
        −
        y
        
          ‖
          
            2
          
        
        +
        λ
        ‖
        x
        
          ‖
          
            2
          
        
      
    
    {\displaystyle \min _{x}\|Ax-y\|^{2}+\lambda \|x\|^{2}}
  to find a vector 
  
    
      
        x
      
    
    {\displaystyle x}
   that is a stable solution to the regression problem. When the system is described by a matrix rather than a vector, this problem  can be written as

  
    
      
        
          min
          
            X
          
        
        ‖
        A
        X
        −
        Y
        
          ‖
          
            2
          
        
        +
        λ
        ‖
        X
        
          ‖
          
            2
          
        
        ,
      
    
    {\displaystyle \min _{X}\|AX-Y\|^{2}+\lambda \|X\|^{2},}
  where the vector norm enforcing a regularization penalty on 
  
    
      
        x
      
    
    {\displaystyle x}
   has been extended to a matrix norm on 
  
    
      
        X
      
    
    {\displaystyle X}
  .
Matrix regularization has applications in matrix completion, multivariate regression, and multi-task learning. Ideas of feature and group selection can also be extended to matrices, and these can be generalized to the nonparametric case of multiple kernel learning.

",2
Technology,Meta learning (computer science),"Meta learning
is a subfield of machine learning where automatic learning algorithms are applied to metadata about machine learning experiments. As of 2017 the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing learning algorithms or to learn (induce) the learning algorithm itself, hence the alternative term learning to learn.Flexibility is important because each learning algorithm is based on a set of assumptions about the data, its inductive bias. This means that it will only learn well if the bias matches the learning problem. A learning algorithm may perform very well in one domain, but not on the next. This poses strong restrictions on the use of machine learning or data mining techniques, since the relationship between the learning problem (often some kind of database) and the effectiveness of different learning algorithms is not yet understood.
By using different kinds of metadata, like properties of the learning problem, algorithm properties (like performance measures), or patterns previously derived from the data, it is possible to learn, select, alter or combine different learning algorithms to effectively solve a given learning problem. Critiques of meta learning approaches bear a strong resemblance to the critique of metaheuristic, a possibly related problem. A good analogy to meta-learning, and the inspiration for Jürgen Schmidhuber's early work (1987) and Yoshua Bengio et al.'s work (1991), considers that genetic evolution learns the learning procedure encoded in genes and executed in each individual's brain. In an open-ended hierarchical meta learning system using genetic programming, better evolutionary methods can be learned by meta evolution, which itself can be improved by meta meta evolution, etc.

",2
Technology,Mixture model,"In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. However, while problems associated with ""mixture distributions"" relate to deriving the properties of the overall population from those of the sub-populations, ""mixture models"" are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information.
Mixture models should not be confused with models for compositional data, i.e., data whose components are constrained to sum to a constant value (1, 100%, etc.). However, compositional models can be thought of as mixture models, where members of the population are sampled at random. Conversely, mixture models can be thought of as compositional models, where the total size reading population has been normalized to 1.",2
Technology,MLOps,"In ancient Roman religion, Ops or  Opis (Latin: ""Plenty"") was a fertility deity and earth goddess of Sabine origin. Her equivalent in Greek mythology was Rhea.

",2
Technology,Mountain car problem,"Mountain Car, a standard testing domain in Reinforcement learning, is a problem in which an under-powered car must drive up a steep hill. Since gravity is stronger than the car's engine, even at full throttle, the car cannot simply accelerate up the steep slope. The car is situated in a valley and must learn to leverage potential energy by driving up the opposite hill before the car is able to make it to the goal at the top of the rightmost hill. The domain has been used as a test bed in various Reinforcement Learning papers.

",2
Technology,Multi-agent reinforcement learning,"Multi-agent reinforcement learning (MARL) is a sub-field of reinforcement learning. It focuses on studying the behavior of multiple agents that coexist in a shared environment. Each agent is motivated by its own rewards, and does actions to advance its own interests; these interests may be opposed to the interests of other agents, resulting in complex group dynamics.
Multi-agent reinforcement learning is closely related to game theory and especially repeated games. Its study combines the pursuit of finding ideal algorithms that maximize rewards with a more sociological set of concepts. While research in single-agent reinforcement learning is concerned with finding the algorithm that gets the biggest number of points for one agent, research in multi-agent reinforcement learning evaluates and quantifies social metrics, such as cooperation, reciprocity, equity, social influence, language and discrimination.

",2
Technology,Multi-armed bandit,"In probability theory and machine learning, the multi-armed bandit problem (sometimes called the K- or N-armed bandit problem) is a problem in which a fixed limited set of resources must be allocated between competing (alternative) choices in a way that maximizes their expected gain, when each choice's properties are only partially known at the time of allocation, and may become better understood as time passes or by allocating resources to the choice. This is a classic reinforcement learning problem that exemplifies the exploration–exploitation tradeoff dilemma. The name comes from imagining a gambler at a row of slot machines (sometimes known as ""one-armed bandits""), who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine. The multi-armed bandit problem also falls into the broad category of stochastic scheduling.
In the problem, each machine provides a random reward from a probability distribution specific to that machine, that is not known a-priori. The objective of the gambler is to maximize the sum of rewards earned through a sequence of lever pulls. The crucial tradeoff the gambler faces at each trial is between ""exploitation"" of the machine that has the highest expected payoff and ""exploration"" to get more information about the expected payoffs of the other machines. The trade-off between exploration and exploitation is also faced in machine learning. In practice, multi-armed bandits have been used to model problems such as managing research projects in a large organization, like a science foundation or a pharmaceutical company. In early versions of the problem, the gambler begins with no initial knowledge about the machines.
Herbert Robbins in 1952, realizing the importance of the problem, constructed convergent population selection strategies in ""some aspects of the sequential design of experiments"". A theorem, the Gittins index, first published by John C. Gittins, gives an optimal policy for maximizing the expected discounted reward.",2
Technology,Multi-task learning,"Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately. Early versions of MTL were called ""hints"".
In a widely cited 1997 paper, Rich Caruana gave the following characterization:Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.
In the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly. One example is a spam-filter, which can be treated as distinct but related classification tasks across different users. To make this more concrete, consider that different people have different distributions of features which distinguish spam emails from legitimate ones, for example an English speaker may find that all emails in Russian are spam, not so for Russian speakers. Yet there is a definite commonality in this classification task across users, for example one common feature might be text related to money transfer. Solving each user's spam classification problem jointly via MTL can let the solutions inform each other and improve performance. Further examples of settings for MTL include multiclass classification and multi-label classification.Multi-task learning works because regularization induced by requiring an algorithm to perform well on a related task can be superior to regularization that prevents overfitting by penalizing all complexity uniformly. One situation where MTL may be particularly helpful is if the tasks share significant commonalities and are generally slightly under sampled. However, as discussed below, MTL has also been shown to be beneficial for learning unrelated tasks.",2
Technology,Multilinear principal component analysis,"Within statistics, Multilinear principal component analysis (MPCA)  is a multilinear extension of principal component analysis (PCA). MPCA is employed in the analysis of n-way arrays, i.e. a cube or hyper-cube of numbers, also informally referred to as a ""data tensor"".  N-way arrays may be decomposed, analyzed, or modeled by 

linear tensor models such as CANDECOMP/Parafac, or
multilinear tensor models, such as multilinear principal component analysis (MPCA), or multilinear independent component analysis (MICA), etc.The origin of MPCA can be traced back to the Tucker decomposition and Peter Kroonenberg's ""M-mode PCA/3-mode PCA"" work. In 2000, De Lathauwer et al. restated Tucker and Kroonenberg's work in clear and concise numerical computational terms in their SIAM paper entitled ""Multilinear Singular Value Decomposition"", (HOSVD) and in their paper ""On the Best Rank-1 and Rank-(R1, R2, ..., RN ) Approximation of Higher-order Tensors"".Circa 2001, Vasilescu reframed the data analysis, recognition and synthesis problems as multilinear tensor problems based on the insight that most observed data are the compositional consequence of several causal factors of data formation, and are well suited for multi-modal data tensor analysis.  The power of the tensor framework was showcased by analyzing human motion joint angles, facial images or textures in terms of their causal factors of data formation in the following works: Human Motion Signatures
(CVPR 2001, ICPR 2002), face recognition – TensorFaces,
(ECCV 2002, CVPR 2003, etc.) and computer graphics – TensorTextures (Siggraph 2004).
Historically, MPCA has been referred to as ""M-mode PCA"", a terminology which was coined by Peter Kroonenberg in 1980. In 2005, Vasilescu and Terzopoulos introduced the Multilinear PCA terminology as a way to better differentiate between linear and multilinear tensor decomposition, as well as, to better differentiate between the work that computed 2nd order statistics associated with each data tensor mode(axis), and subsequent work on Multilinear Independent Component Analysis that computed higher order statistics associated with each tensor mode/axis.
Multilinear PCA may be applied to compute the causal factors of data formation, or as signal processing tool on data tensors whose individual observation have either been vectorized, or whose observations are treated as matrix and concatenated into a data tensor.
MPCA computes a set of orthonormal matrices associated with each mode of the data tensor which are analogous to the orthonormal row and column space of a matrix computed by the matrix SVD.  This transformation aims to capture as high a variance as possible, accounting for as much of the variability in the data associated with each data tensor mode(axis).

",2
Technology,Multilinear subspace learning,"Multilinear subspace learning is an approach to dimensionality reduction.Dimensionality reduction can be performed on a data tensor whose observations have been vectorized and organized into a data tensor, or whose observations are matrices that are concatenated into a data tensor.  Here are some examples of data tensors whose observations are vectorized  or whose observations are matrices concatenated into data tensor images (2D/3D), video sequences (3D/4D), and hyperspectral cubes (3D/4D).
The mapping from a high-dimensional vector space to a set of lower dimensional vector spaces is a multilinear projection. When observations are retained in the same organizational structure as the sensor provides them;  as matrices or higher order tensors, their representations are computed by performing N multiple linear projections.Multilinear subspace learning algorithms are higher-order generalizations of linear subspace learning methods such as principal component analysis (PCA), independent component analysis (ICA), linear discriminant analysis (LDA) and canonical correlation analysis (CCA).

",2
Technology,Multimodal sentiment analysis,"Multimodal sentiment analysis is a new dimension of the traditional text-based sentiment analysis, which goes beyond the analysis of texts, and includes other modalities such as audio and visual data. It can be bimodal, which includes different combinations of two modalities, or trimodal, which incorporates three modalities. With the extensive amount of social media data available online in different forms such as videos and images, the conventional text-based sentiment analysis has evolved into more complex models of multimodal sentiment analysis, which can be applied in the development of  virtual assistants, analysis of YouTube movie reviews, analysis of news videos, and emotion recognition (sometimes known as emotion detection) such as depression monitoring, among others.
Similar to the traditional sentiment analysis, one of the most basic task in multimodal sentiment analysis is sentiment classification, which classifies different sentiments into categories such as positive, negative, or neutral. The complexity of analyzing text, audio, and visual features to perform such a task requires the application of different fusion techniques, such as feature-level, decision-level, and hybrid fusion. The performance of these fusion techniques and the classification algorithms applied, are influenced by the type of textual, audio, and visual features employed in the analysis.

",2
Technology,Multiple instance learning,"In machine learning, multiple-instance learning (MIL) is a type of supervised learning.  Instead of receiving a set of instances which are individually labeled, the learner receives a set of labeled bags, each containing many instances. In the simple case of multiple-instance binary classification, a bag may be labeled negative if all the instances in it are negative.  On the other hand, a bag is labeled positive if there is at least one instance in it which is positive.  From a collection of labeled bags, the learner tries to either (i) induce a concept that will label individual instances correctly or (ii) learn how to label bags without inducing the concept.
Babenko (2008) gives a simple example for MIL. Imagine several people, and each of them has a key chain that contains few keys. Some of these people are able to enter a certain room, and some aren’t. The task is then to predict whether a certain key or a certain key chain can get you into that room. To solve this problem we need to find the exact key that is common for all the “positive” key chains. If we can correctly identify this key, we can also correctly classify an entire key chain - positive if it contains the required key, or negative if it doesn't.",2
Technology,Multiple-instance learning,"In machine learning, multiple-instance learning (MIL) is a type of supervised learning.  Instead of receiving a set of instances which are individually labeled, the learner receives a set of labeled bags, each containing many instances. In the simple case of multiple-instance binary classification, a bag may be labeled negative if all the instances in it are negative.  On the other hand, a bag is labeled positive if there is at least one instance in it which is positive.  From a collection of labeled bags, the learner tries to either (i) induce a concept that will label individual instances correctly or (ii) learn how to label bags without inducing the concept.
Babenko (2008) gives a simple example for MIL. Imagine several people, and each of them has a key chain that contains few keys. Some of these people are able to enter a certain room, and some aren’t. The task is then to predict whether a certain key or a certain key chain can get you into that room. To solve this problem we need to find the exact key that is common for all the “positive” key chains. If we can correctly identify this key, we can also correctly classify an entire key chain - positive if it contains the required key, or negative if it doesn't.",2
Technology,Multiplicative weight update method,"The multiplicative weights update method is an algorithmic technique most commonly used for decision making and prediction, and also widely deployed in game theory and algorithm design. The simplest use case is the problem of prediction from expert advice, in which a decision maker needs to iteratively decide on an expert whose advice to follow. The method assigns initial weights to the experts (usually identical initial weights), and updates these weights multiplicatively and iteratively according to the feedback of how well an expert performed: reducing it in case of poor performance, and increasing it otherwise. It was discovered repeatedly in very diverse fields such as machine learning (AdaBoost, Winnow, Hedge), optimization (solving linear programs), theoretical computer science (devising fast algorithm for LPs and SDPs), and game theory.",2
Technology,Multitask optimization,"Multi-task optimization is a paradigm in the optimization literature that focuses on solving multiple  self-contained tasks simultaneously.  The paradigm has been inspired by the well-established concepts of transfer learning and multi-task learning in predictive analytics. 
The key motivation behind multi-task optimization is that if optimization tasks are related to each other in terms of their optimal solutions or the general characteristics of their function landscapes, the search progress can be transferred to substantially accelerate the search on the other. 
The success of the paradigm is not necessarily limited to one-way knowledge transfers from simpler to more complex tasks. In practice an attempt is to intentionally solve a more difficult task that may unintentionally solve several smaller problems.",2
Technology,Multivariate adaptive regression spline,"In statistics, multivariate adaptive regression splines (MARS) is a form of regression analysis introduced by Jerome H. Friedman in 1991. It is a non-parametric regression technique and can be seen as an extension of linear models that automatically models nonlinearities and interactions between variables.
The term ""MARS"" is trademarked and licensed to Salford Systems. In order to avoid trademark infringements, many open-source implementations of MARS are called ""Earth"".",2
Technology,Natarajan dimension,"In the theory of Probably Approximately Correct Machine Learning, the Natarajan dimension characterizes the complexity of learning a set of functions, generalizing from the Vapnik-Chervonenkis dimension for boolean functions to multi-class functions. Originally introduced as the Generalized Dimension by Natarajan, it was subsequently renamed the Natarajan Dimension by Haussler and Long.",2
Technology,Native-language identification,"Native-language identification (NLI) is the task of determining an author's native language (L1) based only on their writings in a second language (L2). NLI works through identifying language-usage patterns that are common to specific L1 groups and then applying this knowledge to predict the native language of previously unseen texts. This is motivated in part by applications in second-language acquisition, language teaching and forensic linguistics, amongst others.",2
Technology,Nature Machine Intelligence,"Nature Machine Intelligence is a transformative (offering optional open access) scientific journal dedicated to covering machine learning and artificial intelligence. It was created by Nature Research in response to the machine learning explosion of the 2010s. It launched in January 2019, and its opening was met with controversy and boycotts within the machine learning research community due to opposition to Nature publishing the journal as closed access. To address this issue, now Nature Machine Intelligence gives authors an option to publish open access papers for an additional fees, and ""authors remain owners of the research reported, and the code and data supporting the main findings of an article should be openly available. Moreover, preprints are allowed, in fact encouraged, and a link to the preprint can be added below the abstract, visible to all readers.""According to the Journal Citation Reports, Nature Machine Intelligence has a 2020 impact factor of 15.508.",2
Technology,Nearest neighbor search,"Nearest neighbor search (NNS), as a form of proximity search,  is the optimization problem of finding the point in a given set that is closest (or most similar) to a given point. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. 
Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set S of points in a space M and a query point q ∈ M, find the closest point in S to q. Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a k-NN search, where we need to find the k closest points.
Most commonly M is a  metric space and dissimilarity is expressed as a distance metric, which is symmetric and satisfies the triangle inequality. Even more common, M is taken to be the d-dimensional vector space where dissimilarity is measured using the Euclidean distance, Manhattan distance or other distance metric. However, the dissimilarity function can be arbitrary. One example is asymmetric Bregman divergence, for which the triangle inequality does not hold.",2
Technology,Neural modeling fields,"Neural modeling field (NMF) is a mathematical framework for machine learning which combines ideas from neural networks, fuzzy logic, and model based recognition. It has also been referred to as modeling fields,  modeling fields theory (MFT),  Maximum likelihood artificial neural networks (MLANS).
This framework has been developed by Leonid Perlovsky at the AFRL.  NMF is interpreted as a mathematical description of mind's mechanisms, including concepts, emotions, instincts, imagination, thinking, and understanding.  NMF is a multi-level, hetero-hierarchical system. At each level in NMF there are concept-models encapsulating the knowledge; they generate so-called top-down signals, interacting with input, bottom-up signals. These interactions are governed by dynamic equations, which drive concept-model learning, adaptation, and formation of new concept-models for better correspondence to the input, bottom-up signals.",2
Technology,Neural network Gaussian process,"Bayesian networks are a modeling tool for assigning probabilities to events, and thereby characterizing the uncertainty in a model's predictions. Deep learning and artificial neural networks are approaches used in machine learning to build computational models which learn from training examples. Bayesian neural networks merge these fields. They are a type of artificial neural network whose parameters and predictions are both probabilistic. While standard artificial neural networks often assign high confidence even to incorrect predictions, Bayesian neural networks can more accurately evaluate how likely their predictions are to be correct.
Neural Network Gaussian Processes (NNGPs) are equivalent to Bayesian neural networks in a particular limit, and provide a closed form way to evaluate Bayesian neural networks. They are a Gaussian process probability distribution which describes the distribution over predictions made by the corresponding Bayesian neural network. Computation in artificial neural networks is usually organized into sequential layers of artificial neurons. The number of neurons in a layer is called the layer width. The equivalence between NNGPs and Bayesian neural networks occurs when the layers in a Bayesian neural network become infinitely wide (see figure). This 
large width limit is of practical interest, since finite width neural networks typically perform strictly better as layer width is increased.The NNGP also appears in several other contexts: it describes the distribution over predictions made by wide non-Bayesian artificial neural networks after random initialization of their parameters, but before training; it appears as a term in neural tangent kernel prediction equations; it is used in deep information propagation to characterize whether hyperparameters and architectures will be trainable. 
It is related to other large width limits of neural networks.

",2
Technology,Neural network quantum states,"Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics. The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley, engaging with the theory of quantum mind, which posits that quantum effects play a role in cognitive function. However, typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. One important motivation for these investigations is the difficulty to train classical neural networks, especially in big data applications. The hope is that features of quantum computing such as quantum parallelism or the effects of interference and entanglement can be used as resources. Since the technological implementation of a quantum computer is still in a premature stage, such quantum neural network models are mostly theoretical proposals that await their full implementation in physical experiments.
Most Quantum neural networks are developed as feed-forward networks. Similar to their classical counterparts, this structure intakes input from one layer of qubits, and passes that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits. The layers do not have to be of the same width, meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data, classical computer with quantum data, and quantum computer with quantum data.",2
Technology,Node2vec,"node2vec is an algorithm to generate vector representations of nodes on a graph. The node2vec framework learns low-dimensional representations for nodes in a graph through the use of random walks through a graph starting at a target node. It is useful for a variety of machine learning applications. Besides reducing the engineering effort, representations learned by the algorithm lead to greater predictive power. node2vec follows the intuition that random walks through a graph can be treated like sentences in a corpus. Each node in a graph is treated like an individual word, and a random walk is treated as a sentence. By feeding these ""sentences"" into a skip-gram, or by using the continuous bag of words model paths found by random walks can be treated as sentences, and traditional data-mining techniques for documents can be used. The algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and argues that the added flexibility in exploring neighborhoods is the key to learning richer representations of nodes in graphs.
The algorithm is considered one of the best graph classifiers.

",2
Technology,Novelty detection,"Novelty detection is the mechanism by which an intelligent organism is able to identify an incoming sensory pattern as being hitherto unknown. If the pattern is sufficiently salient or associated with a high positive or strong negative utility, it will be given computational resources for effective future processing.
The principle is long known in neurophysiology, with roots in the orienting response research by E. N. Sokolov in the 1950s. The reverse phenomenon is habituation, i.e., the phenomenon that known patterns yield a less marked response. Early neural modeling attempts were by Yehuda Salu. An increasing body of knowledge has been collected concerning the corresponding mechanisms in the brain. In technology, the principle became important for radar detection methods during the Cold War, where unusual aircraft-reflection patterns could indicate an attack by a new type of aircraft. Today, the phenomenon plays an important role in machine learning and data science, where the corresponding methods are known as anomaly detection or outlier detection. An extensive methodological overview is given by Markou and Singh.",2
Technology,Occam learning,"In computational learning theory, Occam learning is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to probably approximately correct (PAC) learning, where the learner is evaluated on its predictive power of a test set.
Occam learnability implies PAC learning, and for a wide variety of concept classes, the converse is also true: PAC learnability implies Occam learnability.

",2
Technology,OpenNN,"OpenNN (Open Neural Networks Library) is a software library written in the C++ programming language which implements neural networks, a main area of deep learning research. The library is open-source, licensed under the GNU Lesser General Public License.

",2
Technology,Overfitting,"In mathematical modeling, overfitting is ""the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably"". An overfitted model is a mathematical model that contains more parameters than can be justified by the data. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.: 45 Underfitting occurs when a mathematical model cannot adequately capture the underlying structure of the data. An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing. Under-fitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.
The possibility of over-fitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data; then over-fitting occurs when a model begins to ""memorize"" training data rather than ""learning"" to generalize from a trend. 
As an extreme example, if the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. (For an illustration, see Figure 2.) Such a model, though, will typically fail severely when making predictions. 
The potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data. Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting (a phenomenon sometimes known as shrinkage). In particular, the value of the coefficient of determination will shrink relative to the original data.
To lessen the chance or amount of overfitting, several techniques are available (e.g., model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout). The basis of some techniques is either (1) to explicitly penalize overly complex models or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter.

",2
Technology,Paraphrasing (computational linguistics),"Paraphrase or paraphrasing in computational linguistics is the natural language processing task of detecting and generating paraphrases. Applications of paraphrasing are varied including information retrieval, question answering, text summarization, and plagiarism detection. Paraphrasing is also useful in the evaluation of machine translation, as well as semantic parsing and generation of new samples to expand existing corpora.

",2
Technology,Parity learning,"Parity learning is a problem in machine learning. An algorithm that solves this problem must find a function ƒ, given some samples (x, ƒ(x)) and the assurance that ƒ computes the parity of bits at some fixed locations. The samples are generated using some distribution over the input. The problem is easy to solve using Gaussian elimination provided that a sufficient number of samples (from a distribution which is not too skewed) are provided to the algorithm.",2
Technology,Pattern language (formal languages),"In logic, mathematics, computer science, and linguistics, a formal language consists of words whose letters are taken from an alphabet and are well-formed according to a specific set of rules.
The alphabet of a formal language consists of symbols, letters, or tokens that concatenate into strings of the language. Each string concatenated from symbols of this alphabet is called a word, and the words that belong to a particular formal language are sometimes called well-formed words or well-formed formulas. A formal language is often defined by means of a formal grammar such as a regular grammar or context-free grammar, which consists of its formation rules.
The field of formal language theory studies primarily the purely syntactical aspects of such languages—that is, their internal structural patterns. Formal language theory sprang out of linguistics, as a way of understanding the syntactic regularities of natural languages.
In computer science, formal languages are used among others as the basis for defining the grammar of programming languages and formalized versions of subsets of natural languages in which the words of the language represent concepts that are associated with particular meanings or semantics. In computational complexity theory, decision problems are typically defined as formal languages, and complexity classes are defined as the sets of the formal languages that can be parsed by machines with limited computational power. In logic and the foundations of mathematics, formal languages are used to represent the syntax of axiomatic systems, and mathematical formalism is the philosophy that all of mathematics can be reduced to the syntactic manipulation of formal languages in this way.",2
Technology,Pattern recognition,"Pattern recognition is the automated recognition of patterns and regularities in data. It has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power. These activities can be viewed as two facets of the same field of application, and they have undergone substantial development over the past few decades.
Pattern recognition systems are commonly trained from labeled ""training"" data. When no labeled data are available, other algorithms can be used to discover previously unknown patterns. KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition focuses more on the signal and also takes acquisition and Signal Processing into consideration. It originated in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition.
In machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is ""spam"" or ""non-spam""). Pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence.Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform ""most likely"" matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors.",2
Technology,Perceiver,"Perceiver is a transformer adapted to be able to process non-textual data, such as images, sounds and video, and spatial data. Transformers underlie other notable systems such as BERT and GPT-3, which preceded Perceiver. It adopts an asymmetric attention mechanism to distill inputs into a latent bottleneck, allowing it to learn from large amounts of heterogeneous data. Perceiver matches or outperforms specialized models on classification tasks.",2
Technology,Phi coefficient,"In statistics, the phi coefficient (or mean square contingency coefficient and denoted by φ or rφ) is a measure of association for two binary variables. In machine learning, it is known as the Matthews correlation coefficient (MCC) and used as a measure of the quality of binary (two-class) classifications, introduced by biochemist Brian W. Matthews in 1975. Introduced by Karl Pearson, and also known as the Yule phi coefficient from its introduction by Udny Yule in 1912 this measure is similar to the Pearson correlation coefficient in its interpretation. In fact, a Pearson correlation coefficient estimated for two binary variables will return the phi coefficient. Two binary variables are considered positively associated if most of the data falls along the diagonal cells. In contrast, two binary variables are considered negatively associated if most of the data falls off the diagonal. If we have a 2×2 table for two random variables x and y

where n11,  n10, n01, n00, are non-negative counts of numbers of observations that sum to n, the total number of observations.  The phi coefficient that describes the association of x and y is

  
    
      
        ϕ
        =
        
          
            
              
                n
                
                  11
                
              
              
                n
                
                  00
                
              
              −
              
                n
                
                  10
                
              
              
                n
                
                  01
                
              
            
            
              
                n
                
                  1
                  ∙
                
              
              
                n
                
                  0
                  ∙
                
              
              
                n
                
                  ∙
                  0
                
              
              
                n
                
                  ∙
                  1
                
              
            
          
        
        .
      
    
    {\displaystyle \phi ={\frac {n_{11}n_{00}-n_{10}n_{01}}{\sqrt {n_{1\bullet }n_{0\bullet }n_{\bullet 0}n_{\bullet 1}}}}.}
  Phi is related to the point-biserial correlation coefficient and Cohen's d and estimates the extent of the relationship between two variables (2×2).The phi coefficient can also be expressed using only 
  
    
      
        n
      
    
    {\displaystyle n}
  , 
  
    
      
        
          n
          
            11
          
        
      
    
    {\displaystyle n_{11}}
  , 
  
    
      
        
          n
          
            1
            ∙
          
        
      
    
    {\displaystyle n_{1\bullet }}
  , and 
  
    
      
        
          n
          
            ∙
            1
          
        
      
    
    {\displaystyle n_{\bullet 1}}
  , as

  
    
      
        ϕ
        =
        
          
            
              n
              
                n
                
                  11
                
              
              −
              
                n
                
                  1
                  ∙
                
              
              
                n
                
                  ∙
                  1
                
              
            
            
              
                n
                
                  1
                  ∙
                
              
              
                n
                
                  ∙
                  1
                
              
              (
              n
              −
              
                n
                
                  1
                  ∙
                
              
              )
              (
              n
              −
              
                n
                
                  ∙
                  1
                
              
              )
            
          
        
        .
      
    
    {\displaystyle \phi ={\frac {nn_{11}-n_{1\bullet }n_{\bullet 1}}{\sqrt {n_{1\bullet }n_{\bullet 1}(n-n_{1\bullet })(n-n_{\bullet 1})}}}.}",2
Technology,Physics-informed neural networks,"Physics-informed neural networks (PINNs) are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process, and can be described by partial differential equations (PDEs). They overcome the low data availability of some biological and engineering systems that makes most state-of-the-art machine learning techniques lack robustness, rendering them ineffective in these scenarios. The prior knowledge of general physical laws acts in the training of neural networks (NNs) as a regularization agent that limits the space of admissible solutions, increasing the correctness of the function approximation. This way, embedding this prior information into a neural network results in enhancing the information content of the available data, facilitating the learning algorithm to capture the right solution and to generalize well even with a low amount of training examples.

",2
Technology,Predictive learning,"Predictive learning is a technique of machine learning in which an agent tries to build a model of its environment by trying out different actions in various circumstances. It uses knowledge of the effects its actions appear to have, turning them into planning operators. These allow the agent to act purposefully in its world. Predictive learning is one attempt to learn with a minimum of pre-existing mental structure. It may have been inspired by Piaget's account of how children construct knowledge of the world by interacting with it. Gary Drescher's book 'Made-up Minds' was seminal for the area. 
The idea that predictions and Unconscious inference are used by the brain to construct a model of the world, in which it can identify causes of percepts, is however even older and goes at least back to Hermann von Helmholtz. Those ideas were later picked up in the field of Predictive coding.
Another related predictive learning theory is Jeff Hawkins' memory-prediction framework, which is laid out in his On Intelligence.",2
Technology,Predictive state representation,"In computer science, a predictive state representation (PSR) is a way to model a state of controlled dynamical system from a history of actions taken and resulting observations. PSR captures the state of a system as a vector of predictions for future tests (experiments) that can be done on the system. A test is a sequence of action-observation pairs and its prediction is the probability of the test's observation-sequence happening if the test's action-sequence were to be executed on the system. One of the advantage of using PSR is that the predictions are directly related to observable quantities.  This is in contrast to other models of dynamical systems, such as partially observable Markov decision processes (POMDPs) where the state of the system is represented as a probability distribution over unobserved nominal states.",2
Technology,Preference learning,"Preference learning is a subfield in machine learning, which is a classification method based on observed preference information. In the view of supervised learning, preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items.
While the concept of preference learning has been emerged for some time in many fields such as economics, it's a relatively new topic in Artificial Intelligence research. Several workshops have been discussing preference learning and related topics in the past decade.

",2
Technology,Prior knowledge for pattern recognition,"Pattern recognition is a very active field of research intimately bound to machine learning. Also known as classification or statistical classification, pattern recognition aims at building a classifier that can determine the class of an input pattern. This procedure, known as training, corresponds to learning an unknown decision function based only on a set of input-output pairs 
  
    
      
        (
        
          
            x
          
          
            i
          
        
        ,
        
          y
          
            i
          
        
        )
      
    
    {\displaystyle ({\boldsymbol {x}}_{i},y_{i})}
   that form the training data (or training set). Nonetheless, in real world applications such as character recognition, a certain amount of information on the problem is usually known beforehand. The incorporation of this prior knowledge into the training is the key element that will allow an increase of performance in many applications.

",2
Technology,Proactive learning,"Proactive learning is a generalization of active learning designed to relax unrealistic assumptions and thereby reach practical applications.
""Active learning seeks to select the most informative unlabeled instances and ask an omniscient oracle for their labels, so as to retrain a learning algorithm maximizing accuracy. However, the oracle is assumed to be infallible (never wrong), indefatigable (always answers), individual (only one oracle), and insensitive to costs (always free or always charges the same).""""In real life, it is possible and more general to have multiple sources of information with differing reliabilities or areas of expertise. Active learning also assumes that the single oracle is perfect, always providing a correct answer when requested. In reality, though, an ""oracle"" (if we generalize the term to mean any source of expert information) may be incorrect (fallible) 
with a probability that should be a function of the difficulty of the question. Moreover, an oracle may be reluctant – it may refuse to answer if it is too uncertain or too busy. Finally, active learning presumes the oracle is either free or charges uniform cost in label elicitation.
Such an assumption is naive since cost is likely to be regulated by difficulty (amount of work required to formulate an answer) or other factors.""Proactive learning relaxes all four of these assumptions, relying on a decision-theoretic approach to jointly select the optimal oracle and instance, by casting the problem as a utility optimization problem subject to a budget constraint.

",2
Technology,Proaftn,"Proaftn is a fuzzy classification method that belongs to the class of supervised learning algorithms. The acronym Proaftn stands for: (PROcédure d'Affectation Floue pour la problématique du Tri Nominal), which means in English: Fuzzy Assignment Procedure for Nominal Sorting.
The method enables to determine the fuzzy indifference relations by generalizing the indices (concordance and discordance) used in the ELECTRE III method. To determine the fuzzy indifference relations, PROAFTN uses the general scheme of the discretization technique described in, that establishes a set of pre-classified cases called a training set.
To resolve the classification problems, Proaftn proceeds by the following stages:Stage 1. Modeling of classes: In this stage, the prototypes of the classes are conceived using the two following steps:

Step 1. Structuring: The prototypes and their parameters (thresholds, weights, etc.) are established using the available knowledge given by the expert.
Step 2. Validation: We use one of the two following techniques in order to validate or adjust the parameters obtained in the first step through the assignment  examples known as a training set.Direct technique: It consists in adjusting the parameters through the training set and with the expert intervention.
Indirect technique: It consists in fitting the parameters without the expert intervention as used in machine learning approaches.In multicriteria classification problem, the indirect technique is known as preference disaggregation analysis. This technique requires less cognitive effort than the former technique; it uses an automatic method to determine the optimal parameters, which minimize the classification errors.
Furthermore, several heuristics and metaheuristics were used to learn the multicriteria classification method Proaftn.Stage 2. Assignment: After conceiving the prototypes, Proaftn proceeds to assign the new objects to specific classes.",2
Technology,Probabilistic numerics,"Probabilistic numerics is a scientific field at the intersection of statistics, machine learning and applied mathematics, where tasks in numerical analysis including finding numerical solutions for integration, linear algebra, optimisation and differential equations are seen as problems of statistical, probabilistic, or Bayesian inference.",2
Technology,Probability matching,"Probability matching is a decision strategy in which predictions of class membership are proportional to the class base rates.  Thus, if in the training set positive examples are observed 60% of the time, and negative examples are observed 40% of the time, then the observer using a probability-matching strategy will predict (for unlabeled examples) a class label of ""positive"" on 60% of instances, and a class label of ""negative"" on 40% of instances.  
The optimal Bayesian decision strategy (to maximize the number of correct predictions, see Duda, Hart & Stork (2001)) in such a case is to always predict ""positive"" (i.e., predict the majority category in the absence of other information), which has 60% chance of winning rather than matching which has 52% of winning  (where p is the probability of positive realization, the result of matching would be 
  
    
      
        
          p
          
            2
          
        
        +
        (
        1
        −
        p
        
          )
          
            2
          
        
      
    
    {\displaystyle p^{2}+(1-p)^{2}}
  , here 
  
    
      
        .6
        ×
        .6
        +
        .4
        ×
        .4
      
    
    {\displaystyle .6\times .6+.4\times .4}
  ).  The probability-matching strategy is of psychological interest because it is frequently employed by human subjects in decision and classification studies (where it may be related to Thompson sampling).
The only case when probability matching will yield same results as Bayesian decision strategy mentioned above is when all class base rates are the same. So, if in the training set positive examples are observed 50% of the time, then the Bayesian strategy would yield 50% accuracy (1 × .5), just as probability matching (.5 ×.5 + .5 × .5).",2
Technology,Product of experts,"Product of experts (PoE) is a machine learning technique. It models a probability distribution by combining the output from several simpler distributions.
It was proposed by Geoffrey Hinton, along with an algorithm for training the parameters of such a system.
The core idea is to combine several probability distributions (""experts"") by multiplying their density functions—making the PoE classification similar to an ""and"" operation. This allows each expert to make decisions on the basis of a few dimensions without having to cover the full dimensionality of a problem.
This is related to (but quite different from) a mixture model, where several probability distributions are combined via an ""or"" operation, which is a weighted sum of their density functions.",2
Technology,Programming by example,"In computer science, programming by example (PbE), also termed programming by demonstration or more generally as demonstrational programming, is an end-user development technique for teaching a computer new behavior by demonstrating actions on concrete examples. The system records user actions and infers a generalized program that can be used on new examples.
PbE is intended to be easier to do than traditional computer programming, which generally requires learning and using a programming language.  Many PbE systems have been developed as research prototypes, but few have found widespread real-world application.  More recently, PbE has proved to be a useful paradigm for creating scientific work-flows. PbE is used in two independent clients for the BioMOBY protocol: Seahawk and Gbrowse moby.
Also the programming by demonstration (PbD) term has been mostly adopted by robotics researchers for teaching new behaviors to the robot through a physical demonstration of the task. The usual distinction in literature between these terms is that in PbE the user gives a prototypical product of the computer execution, such as a row in the desired results of a query; while in PbD the user performs a sequence of actions that the computer must repeat, generalizing it to be used in different data sets. For final users, to automate a workflow in a complex tool (e.g. Photoshop), the most simple case of PbD is the  macro recorder.",2
Technology,Proximal gradient methods for learning,"Proximal gradient (forward backward splitting) methods for learning is an area of research in optimization and statistical learning theory which studies algorithms for a general class of convex regularization problems where the regularization penalty may not be differentiable. One such example is 
  
    
      
        
          ℓ
          
            1
          
        
      
    
    {\displaystyle \ell _{1}}
   regularization (also known as Lasso) of the form

  
    
      
        
          min
          
            w
            ∈
            
              
                R
              
              
                d
              
            
          
        
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        (
        
          y
          
            i
          
        
        −
        ⟨
        w
        ,
        
          x
          
            i
          
        
        ⟩
        
          )
          
            2
          
        
        +
        λ
        ‖
        w
        
          ‖
          
            1
          
        
        ,
        
        
           where 
        
        
          x
          
            i
          
        
        ∈
        
          
            R
          
          
            d
          
        
        
           and 
        
        
          y
          
            i
          
        
        ∈
        
          R
        
        .
      
    
    {\displaystyle \min _{w\in \mathbb {R} ^{d}}{\frac {1}{n}}\sum _{i=1}^{n}(y_{i}-\langle w,x_{i}\rangle )^{2}+\lambda \|w\|_{1},\quad {\text{ where }}x_{i}\in \mathbb {R} ^{d}{\text{ and }}y_{i}\in \mathbb {R} .}
  Proximal gradient methods offer a general framework for solving regularization problems from statistical learning theory with penalties that are tailored to a specific problem application. Such customized penalties can help to induce certain structure in problem solutions, such as sparsity (in the case of lasso) or group structure (in the case of  group lasso).

",2
Technology,Pythia (machine learning),"Pythia is an ancient text restoration model that recovers missing characters from a damaged text input using deep neural networks. It was created by Yannis Assael, Thea Sommerschield, and Jonathan Prag, researchers from Google DeepMind and the University of Oxford.To study the society and the history of ancient civilisations, ancient history relies on disciplines such as Epigraphy, the study of ancient inscribed texts. Hundreds of thousands of these texts, known as inscriptions, have survived to our day, but are often damaged over the centuries. Illegible parts of the text must then be restored by specialists, called epigraphists, in order to extract meaningful information from the text and use it to expand our knowledge of the context in which the text was written. Pythia takes as input the damaged text, and is trained to return hypothesised restorations of ancient Greek inscriptions, working as an assistive aid for ancient historians. Its neural network architecture works at both the character- and word-level, thereby effectively handling long-term context information, and dealing efficiently with incomplete word representations. Pythia is applicable to any discipline dealing with ancient texts (philology, papyrology, codicology) and can work in any language (ancient or modern).",2
Technology,Quantification (machine learning),"In machine learning and data mining, quantification (variously called learning to quantify, or supervised prevalence estimation, or class prior estimation) is the task of using supervised learning in order to train models (quantifiers) that estimate the relative frequencies (also known as prevalence values) of the classes of interest in a sample of unlabelled data items. 
For instance, in a sample of 100,000 unlabelled tweets known to express opinions about a certain political candidate, a quantifier may be used to estimate the percentage of these 100,000 tweets which belong to class `Positive' (i.e., which manifest a positive stance towards this candidate), and to do the same for classes `Neutral' and `Negative'.
Quantification may also be viewed as the task of training predictors that estimate a (discrete) probability distribution, i.e., that generate a predicted distribution that approximates the unknown true distribution of the items across the classes of interest. Quantification is different from classification, since the goal of classification is to predict the class labels of individual data items, while the goal of quantification it to predict the class prevalence values of sets of data items. Quantification is also different from regression, since in regression the training data items have real-valued labels, while in quantification the training data items have class labels.
It has been shown in multiple research works
that performing quantification by classifying all unlabelled instances and then counting the instances that have been attributed to each class (the 'classify and count' method) usually leads to suboptimal quantification accuracy. This suboptimality may be seen as a direct consequence of 'Vapnik's principle', which states:

If you possess a restricted amount of information for solving some problem, try to solve the problem directly and never solve a more general problem as an intermediate step. It is possible that the available information is sufficient for a direct solution but is insufficient for solving a more general intermediate problem.
In our case, the problem to be solved directly is quantification, while the more general intermediate problem is classification. As a result of the suboptimality of the 'classify and count' method, quantification has evolved as a task in its own right, different (in goals, methods, techniques, and evaluation measures) from classification.",2
Technology,Quantum machine learning,"Quantum machine learning is the integration of quantum algorithms within machine learning programs. The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data. Beyond quantum computing, the term ""quantum machine learning"" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments. Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa. Furthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as ""quantum learning theory"".",2
Technology,Query-level feature,"A query-level feature or QLF is a ranking feature utilized in a machine-learned ranking algorithm.
Example QLFs:

How many times has this query been run in the last month?
How many words are in the query?
What is the sum/average/min/max/median of the BM25F values for the query?",2
Technology,Rademacher complexity,"In computational learning theory (machine learning and theory of computation), Rademacher complexity, named after Hans Rademacher, measures richness of a class of real-valued functions with respect to a probability distribution.

",2
Technology,RAMnets,"RAMnets is one of the oldest practical neurally inspired classification algorithms. The RAMnets  is also known as a type of ""n-tuple recognition method"" or ""weightless neural network"".

",2
Technology,Random forest,"Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned. Random decision forests correct for decision trees' habit of overfitting to their training set.: 587–588  Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. However, data characteristics can affect their performance.The first algorithm for random decision forests was created in 1995 by Tin Kam Ho using the random subspace method, which, in Ho's formulation, is a way to implement the ""stochastic discrimination"" approach to classification proposed by Eugene Kleinberg.An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered ""Random Forests"" as a trademark in 2006 (as of 2019, owned by Minitab, Inc.). The extension combines Breiman's ""bagging"" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.
Random forests are frequently used as ""blackbox"" models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration.",2
Technology,Random indexing,"Random indexing is a dimensionality reduction method and computational framework for distributional semantics, based on the insight that very-high-dimensional vector space model implementations are impractical, that models need not grow in dimensionality when new items (e.g. new terminology) are encountered, and that a high-dimensional model can be projected into a space of lower dimensionality without compromising L2 distance metrics if the resulting dimensions are chosen appropriately.
This is the original point of the random projection approach to dimension reduction first formulated as the Johnson–Lindenstrauss lemma, and locality-sensitive hashing has some of the same starting points. Random indexing, as used in representation of language, originates from the work of Pentti Kanerva on sparse distributed memory, and can be described as an incremental formulation of a random projection.It can be also verified that random indexing is a random projection technique for the construction of Euclidean spaces—i.e. L2 normed vector spaces. In Euclidean spaces, random projections are elucidated using the Johnson–Lindenstrauss lemma.The TopSig technique extends the random indexing model to produce bit vectors for comparison with the Hamming distance similarity function. It is used for improving the performance of information retrieval and document clustering. In a similar line of research, Random Manhattan Integer Indexing (RMII) is proposed for improving the performance of the methods that employ the Manhattan distance between text units. Many random indexing methods primarily generate similarity from co-occurrence of items in a corpus. Reflexive Random Indexing (RRI) generates similarity from co-occurrence and from shared occurrence with other items.

",2
Technology,Random projection,"In mathematics and statistics, random projection is a technique used to reduce the dimensionality of a set of points which lie in Euclidean space. Random projection methods are known for their power, simplicity, and low error rates when compared to other methods. According to experimental results, random projection preserves distances well, but empirical results are sparse. They have been applied to many natural language tasks under the name random indexing.

",2
Technology,Relational data mining,"Relational data mining is the data mining technique for relational
databases. Unlike traditional data mining algorithms, which look for
patterns in a single table (propositional patterns), 
relational data mining algorithms look for patterns among multiple tables
(relational patterns). For most types of propositional
patterns, there are corresponding relational patterns. For example,
there are relational classification rules (relational classification), relational regression tree, and relational association rules.
There are several approaches to relational data mining:

Inductive Logic Programming (ILP)
Statistical Relational Learning (SRL)
Graph Mining
Propositionalization
Multi-view learning",2
Technology,Representer theorem,"For computer science, in statistical learning theory, a representer theorem is any of several related results stating that a minimizer 
  
    
      
        
          f
          
            ∗
          
        
      
    
    {\displaystyle f^{*}}
   of a regularized empirical risk functional defined over a reproducing kernel Hilbert space can be represented as a finite linear combination of kernel products evaluated on the input points in the training set data.",2
Technology,Right to explanation,"In the regulation of algorithms, particularly artificial intelligence and its subfield of machine learning, a right to explanation (or right to an explanation) is a right to be given an explanation for an output of the algorithm. Such rights primarily refer to individual rights to be given an explanation for decisions that significantly affect an individual, particularly legally or financially. For example, a person who applies for a loan and is denied may ask for an explanation, which could be ""Credit bureau X reports that you declared bankruptcy last year; this is the main factor in considering you too likely to default, and thus we will not give you the loan you applied for.""
Some such legal rights already exist, while the scope of a general ""right to explanation"" is a matter of ongoing debate. There have been arguments made that a ""social right to explanation"" is a crucial foundation for an information society, particularly as the institutions of that society will need to use digital technologies, artificial intelligence, machine learning. In other words, that the related automated decision making systems that use explainability would be more trustworthy and transparent. Without this right, which could be constituted both legally and through professional standards, the public will be left without much recourse to challenge the decisions of automated systems. One of the emerging problems is how to communicate an explanation to a user, should it be through text, a high-level visual diagram, video or some other medium, and how can an explainable system scope the explanation in a reasonable way? 

",2
Technology,Robot learning,"Robot learning is a research field at the intersection of machine learning and robotics. It studies techniques allowing a robot to acquire novel skills or adapt to its environment through learning algorithms. The embodiment of the robot, situated in a physical embedding, provides at the same time specific difficulties (e.g. high-dimensionality, real time constraints for collecting data and learning) and opportunities for guiding the learning process (e.g. sensorimotor synergies, motor primitives).
Example of skills that are targeted by learning algorithms include sensorimotor skills such as locomotion, grasping, active object categorization,  as well as interactive skills such as joint manipulation of an object with a human peer, and linguistic skills such as the grounded and situated meaning of human language. Learning can happen either through autonomous self-exploration or through guidance from a human teacher, like for example in robot learning by imitation.
Robot learning can be closely related to adaptive control, reinforcement learning as well as developmental robotics which considers the problem of autonomous lifelong acquisition of repertoires of skills.
While machine learning is frequently used by computer vision algorithms employed in the context of robotics, these applications are usually not referred to as ""robot learning"".

",2
Technology,Robotic process automation,"Robotic process automation (RPA) is a form of business process automation technology based on metaphorical software robots (bots) or on artificial intelligence (AI)/digital workers.  It is sometimes referred to as software robotics (not to be confused with robot software).
In traditional workflow automation tools, a software developer produces a list of actions to automate a task and interface to the back end system using internal application programming interfaces (APIs) or dedicated scripting language. In contrast, RPA systems develop the action list by watching the user perform that task in the application's graphical user interface (GUI), and then perform the automation by repeating those tasks directly in the GUI. This can lower the barrier to the use of automation in products that might not otherwise feature APIs for this purpose.
RPA tools have strong technical similarities to graphical user interface testing tools. These tools also automate interactions with the GUI, and often do so by repeating a set of demonstration actions performed by a user. RPA tools differ from such systems in that they allow data to be handled in and between multiple applications, for instance, receiving email containing an invoice, extracting the data, and then typing that into a bookkeeping system.

",2
Technology,ROCm,"ROCm is an Advanced Micro Devices (AMD) software stack for graphics processing unit (GPU) programming. ROCm spans several domains: general-purpose computing on graphics processing units (GPGPU), high performance computing (HPC), heterogeneous computing. It offers several programming models: HIP (GPU-kernel-based programming), OpenMP/Message Passing Interface (MPI) (directive-based programming), OpenCL.
ROCm is free, libre and open-source software (except the GPU firmware blobs), it is distributed under various licenses.",2
Technology,Rule induction,"Rule induction is an area of machine learning in which formal rules are extracted from a set of observations.  The rules extracted may represent a full scientific model of the data, or merely represent local patterns in the data.
Data mining in general and rule induction in detail are trying to create algorithms without human programming but with analyzing existing data structures.: 415-  In the easiest case, a rule is expressed with “if-then statements” and was created with the ID3 algorithm for decision tree learning.: 7 : 348  Rule learning algorithm are taking training data as input and creating rules by partitioning the table with cluster analysis.: 7  A possible alternative over the ID3 algorithm is genetic programming which evolves a program until it fits to the data.: 2 Creating different algorithm and testing them with input data can be realized in the WEKA software.: 125  Additional tools are machine learning libraries for Python like scikit-learn.

",2
Technology,Sample complexity,"The sample complexity of a machine learning algorithm represents the number of training-samples that it needs in order to successfully learn a target function.
More precisely, the sample complexity is the number of training-samples that we need to supply to the algorithm, so that the function returned by the algorithm is within an arbitrarily small error of the best possible function, with probability arbitrarily close to 1.
There are two variants of sample complexity:

The weak variant fixes a particular input-output distribution;
The strong variant takes the worst-case sample complexity over all input-output distributions.The No free lunch theorem, discussed below, proves that, in general, the strong sample complexity is infinite, i.e. that there is no algorithm that can learn the globally-optimal target function using a finite number of training samples.
However, if we are only interested in a particular class of target functions (e.g, only linear functions) then the sample complexity is finite, and it depends linearly on the VC dimension on the class of target functions.

",2
Technology,Scikit-multiflow,scikit-mutliflow (also known as skmultiflow) is a free and open source software machine learning library for multi-output/multi-label and stream data written in Python.,2
Technology,Self-supervised learning,"Self-supervised learning (SSL) is a method of machine learning. It learns from unlabeled sample data. It can be regarded as an intermediate form between supervised and unsupervised learning. It is based on an artificial neural network. The neural network learns in two steps. First, the task is solved based on pseudo-labels which help to initialize the network weights. Second, the actual task is performed with supervised or unsupervised learning. Self-supervised learning has produced promising results in recent years and has found practical application in audio processing and is being used by Facebook and others for speech recognition. The primary appeal of SSL is that training can occur with data of lower quality, rather than improving ultimate outcomes. Self-supervised learning more closely imitates the way humans learn to classify objects.",2
Technology,Semantic analysis (machine learning),"In machine learning, semantic analysis of a corpus is the task of building structures that approximate concepts from a large set of documents. It generally does not involve prior semantic understanding of the documents. A metalanguage based on predicate logic can analyze the speech of humans.: 93-  Another strategy to understand the semantics of a text is symbol grounding. If language is grounded, it is equal to recognizing a machine readable meaning. For the restricted domain of spatial analysis, a computer based language understanding system was demonstrated.: 123 Latent semantic analysis (sometimes latent semantic indexing), is a class of techniques where documents are represented as vectors in term space. A prominent example is PLSI.
Latent Dirichlet allocation involves attributing document terms to topics.
n-grams and hidden Markov models work by representing the term stream as a Markov chain where each term is derived from the few terms before it.",2
Technology,Semantic folding,"Semantic folding theory describes a procedure for encoding the semantics of natural language text in a semantically grounded binary representation. This approach provides a framework for modelling how language data is processed by the neocortex.

",2
Technology,Semi-supervised learning,"Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data). It is a special instance of weak supervision.
Unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render large, fully labeled training sets infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.
A set of 
  
    
      
        l
      
    
    {\displaystyle l}
   independently identically distributed examples 
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            l
          
        
        ∈
        X
      
    
    {\displaystyle x_{1},\dots ,x_{l}\in X}
   with corresponding labels 
  
    
      
        
          y
          
            1
          
        
        ,
        …
        ,
        
          y
          
            l
          
        
        ∈
        Y
      
    
    {\displaystyle y_{1},\dots ,y_{l}\in Y}
   and 
  
    
      
        u
      
    
    {\displaystyle u}
   unlabeled examples 
  
    
      
        
          x
          
            l
            +
            1
          
        
        ,
        …
        ,
        
          x
          
            l
            +
            u
          
        
        ∈
        X
      
    
    {\displaystyle x_{l+1},\dots ,x_{l+u}\in X}
   are processed. Semi-supervised learning combines this information to surpass the classification performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning.
Semi-supervised learning may refer to either transductive learning or inductive learning. The goal of transductive learning is to infer the correct labels for the given unlabeled data 
  
    
      
        
          x
          
            l
            +
            1
          
        
        ,
        …
        ,
        
          x
          
            l
            +
            u
          
        
      
    
    {\displaystyle x_{l+1},\dots ,x_{l+u}}
   only. The goal of inductive learning is to infer the correct mapping from 
  
    
      
        X
      
    
    {\displaystyle X}
   to 
  
    
      
        Y
      
    
    {\displaystyle Y}
  .
Intuitively, the learning problem can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam.
It is unnecessary (and, according to Vapnik's principle, imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.",2
Technology,Sequence labeling,"In machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values.  A common example of a sequence labeling task is part of speech tagging, which seeks to assign a part of speech to each word in an input sentence or document.  Sequence labeling can be treated as a set of independent classification tasks, one per member of the sequence.  However, accuracy is generally improved by making the optimal label for a given element dependent on the choices of nearby elements, using special algorithms to choose the globally best set of labels for the entire sequence at once.
As an example of why finding the globally best label sequence might produce better results than labeling one item at a time, consider the part-of-speech tagging task just described.  Frequently, many words are members of multiple parts of speech, and the correct label of such a word can often be deduced from the correct label of the word to the immediate left or right.  For example, the word ""sets"" can be either a noun or verb.  In a phrase like ""he sets the books down"", the word ""he"" is unambiguously a pronoun, and ""the"" unambiguously a determiner, and using either of these labels, ""sets"" can be deduced to be a verb, since nouns very rarely follow pronouns and are less likely to precede determiners than verbs are.  But in other cases, only one of the adjacent words is similarly helpful.  In ""he sets and then knocks over the table"", only the word ""he"" to the left is helpful (cf. ""...picks up the sets and then knocks over..."").  Conversely, in ""... and also sets the table"" only the word ""the"" to the right is helpful (cf. ""... and also sets of books were ..."").  An algorithm that proceeds from left to right, labeling one word at a time, can only use the tags of left-adjacent words and might fail in the second example above; vice versa for an algorithm that proceeds from right to left.
Most sequence labeling algorithms are probabilistic in nature, relying on statistical inference to find the best sequence.  The most common statistical models in use for sequence labeling make a Markov assumption, i.e. that the choice of label for a particular word is directly dependent only on the immediately adjacent labels; hence the set of labels forms a Markov chain.  This leads naturally to the hidden Markov model (HMM), one of the most common statistical models used for sequence labeling.  Other common models in use are the maximum entropy Markov model and conditional random field.

",2
Technology,Similarity learning,"Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.

",2
Technology,Solomonoff's theory of inductive inference,"Solomonoff's theory of inductive inference is a mathematical proof that if a universe is generated by an algorithm, then observations of that universe, encoded as a dataset, are best predicted by the smallest executable archive of that dataset.  This formalization of Occam's razor  for induction was introduced by Ray Solomonoff, based on probability theory and theoretical computer science. In essence, Solomonoff's induction derives the posterior probability of any computable theory, given a sequence of observed data. This posterior probability is derived from Bayes rule and some universal prior, that is, a prior that assigns a positive probability to any computable theory.

",2
Technology,Sparse dictionary learning,"Sparse coding is a representation learning method which aims at finding a sparse representation of the input data (also known as sparse coding) in the form of a linear combination of basic elements as well as those basic elements themselves. These elements are called atoms and they compose a dictionary. Atoms in the dictionary are not required to be orthogonal, and they may be an over-complete spanning set. This problem setup also allows the dimensionality of the signals being represented to be higher than the one of the signals being observed. The above two properties lead to having seemingly redundant atoms that allow multiple representations of the same signal but also provide an improvement in sparsity and flexibility of the representation.
One of the most important applications of sparse dictionary learning is in the field of compressed sensing or signal recovery. In compressed sensing, a high-dimensional signal can be recovered with only a few linear measurements provided that the signal is sparse or nearly sparse. Since not all signals satisfy this sparsity condition, it is of great importance to find a sparse representation of that signal such as the wavelet transform or the directional gradient of a rasterized matrix. Once a matrix or a high dimensional vector is transferred to a sparse space, different recovery algorithms like basis pursuit, CoSaMP or fast non-iterative algorithms can be used to recover the signal.
One of the key principles of dictionary learning is that the dictionary has to be inferred from the input data. The emergence of sparse dictionary learning methods was stimulated by the fact that in signal processing one typically wants to represent the input data using as few components as possible. Before this approach the general practice was to use predefined dictionaries (such as Fourier or wavelet transforms). However, in certain cases a dictionary that is trained to fit the input data can significantly improve the sparsity, which has applications in data decomposition, compression and analysis and has been used in the fields of image denoising and classification, video and audio processing. Sparsity and overcomplete dictionaries have immense applications in image compression, image fusion and inpainting.",2
Technology,Spatial embedding,"Spatial embedding is one of feature learning techniques used in spatial analysis where points, lines, polygons or other spatial data types. representing geographic locations are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per geographic object to a continuous vector space with a much lower dimension.
Such embedding methods allow complex spatial data to be used in neural networks and have been shown to improve performance in spatial analysis tasks

",2
Technology,Spike-and-slab regression,"In statistics, spike-and-slab regression is a Bayesian variable selection technique that is particularly useful when the number of possible predictors is larger than the number of observations.Initially, the idea of the spike-and-slab model was proposed by Mitchell & Beauchamp (1988). The approach was further significantly developed by Madigan & Raftery (1994) and George & McCulloch (1997). The final adjustments to the model were done by Ishwaran & Rao (2005).",2
Technology,Stability (learning theory),"Stability, also known as algorithmic stability, is a notion in computational learning theory of how a  machine learning algorithm is perturbed by small changes to its inputs. A stable learning algorithm is one for which the prediction does not change much when the training data is modified slightly. For instance, consider a machine learning algorithm that is being trained to recognize handwritten letters of the alphabet, using 1000 examples of handwritten letters and their labels (""A"" to ""Z"") as a training set. One way to modify this training set is to leave out an example, so that only 999 examples of handwritten letters and their labels are available. A stable learning algorithm would produce a similar classifier with both the 1000-element and 999-element training sets.
Stability can be studied for many types of learning problems, from language learning to inverse problems in physics and engineering, as it is a property of the learning process rather than the type of information being learned. The study of stability gained importance in computational learning theory in the 2000s when it was shown to have a connection with generalization. It was shown that for large classes of learning algorithms, notably empirical risk minimization algorithms, certain types of stability ensure good generalization.

",2
Technology,Statistical classification,"In statistics, classification is the problem of identifying which of a set of categories (sub-populations) an observation (or observations) belongs to.  Examples are assigning a given email to the ""spam"" or ""non-spam"" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).
Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features.  These properties may variously be categorical (e.g. ""A"", ""B"", ""AB"" or ""O"", for blood type), ordinal (e.g. ""large"", ""medium"" or ""small""), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.
An algorithm that implements classification, especially in a concrete implementation, is known as a classifier.  The term ""classifier"" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.
Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable.  In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes.  Other fields may use different terminology: e.g. in community ecology, the term ""classification"" normally refers to cluster analysis.",2
Technology,Statistical learning theory,"Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. Statistical learning theory deals with the statistical inference problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics.",2
Technology,Statistical relational learning,"Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure.
Note that SRL is sometimes called Relational Machine Learning (RML) in the literature. Typically, the knowledge representation formalisms developed in SRL use (a subset of) first-order logic to describe relational properties of a domain in a general manner (universal quantification) and draw upon probabilistic graphical models (such as Bayesian networks or Markov networks) to model the uncertainty; some also build upon the methods of inductive logic programming. Significant contributions to the field have been made since the late 1990s.As is evident from the characterization above, the field is not strictly limited to learning aspects; it is equally concerned with reasoning (specifically probabilistic inference) and knowledge representation. Therefore, alternative terms that reflect the main foci of the field include statistical relational learning and reasoning (emphasizing the importance of reasoning) and first-order probabilistic languages (emphasizing the key properties of the languages with which models are represented).",2
Technology,Stochastic block model,"The stochastic block model is a generative model for random graphs. This model tends to produce graphs containing communities, subsets of nodes characterized by being connected with one another with particular edge densities. For example, edges may be more common within communities than between communities. Its mathematical formulation has been firstly introduced in 1983 in the field of social network by Holland et al. The stochastic block model is important in statistics, machine learning, and network science, where it serves as a useful benchmark for the task of recovering community structure in graph data.

",2
Technology,Struc2vec,"struc2vec is a framework to generate node vector representations on a graph that preserve the structural identity. In contrast to node2vec, that optimizes node embeddings so that nearby nodes in the graph have similar embedding, struc2vec captures the roles of nodes in a graph, even if structurally similar nodes are far apart in the graph. It learns low-dimensional representations for nodes in a graph, generating random walks through a constructed multi-layer graph starting at each graph node. It is useful for machine learning applications where the downstream application is more related with the structural equivalence of the nodes (e.g., it can be used to detect nodes in networks with similar functions, such as interns in the social network of a corporation). struc2vec identifies nodes that play a similar role based solely on the structure of the graph, for example computing the structural identity of individuals in social networks. In particular, struc2vec employs a degree-based method to measure the pairwise structural role similarity, which is then adopted to build the multi-layer graph. Moreover, the distance between the latent representation of nodes is strongly correlated to their structural similarity. The framework contains three optimizations: reducing the length of degree sequences considered, reducing the number of pairwise similarity calculations, and reducing the number of layers in the generated graph.
struc2vec follows the intuition that random walks through a graph can be treated as sentences in a corpus. Each node in a graph is treated as an individual word, and short random walk is treated as a sentence. In its final phase, the algorithm employs Gensim's word2vec algorithm to learn embeddings based on biased random walks. Sequences of nodes are fed into a skip-gram or continuous bag of words model and traditional machine-learning techniques for classification can be used. It is considered a useful framework to learn node embeddings based on structural equivalence.",2
Technology,Structural risk minimization,"Structural risk minimization (SRM) is an inductive principle of use in machine learning. Commonly in machine learning, a generalized model must be selected from a finite data set, with the consequent problem of overfitting – the model becoming too strongly tailored to the particularities of the training set and generalizing poorly to new data. The SRM principle addresses this problem by balancing the model's complexity against its success at fitting the training data. This principle was first set out in a 1974 paper by Vladimir Vapnik and Alexey Chervonenkis and uses the VC dimension.
In practical terms, Structural Risk Minimization is implemented by minimizing 
  
    
      
        
          E
          
            t
            r
            a
            i
            n
          
        
        +
        β
        H
        (
        W
        )
      
    
    {\displaystyle E_{train}+\beta H(W)}
  , where 
  
    
      
        
          E
          
            t
            r
            a
            i
            n
          
        
      
    
    {\displaystyle E_{train}}
   is the train error, the function 
  
    
      
        H
        (
        W
        )
      
    
    {\displaystyle H(W)}
   is called a regularization function, and 
  
    
      
        β
      
    
    {\displaystyle \beta }
   is a constant.  
  
    
      
        H
        (
        W
        )
      
    
    {\displaystyle H(W)}
   is chosen such that it takes large values on parameters 
  
    
      
        W
      
    
    {\displaystyle W}
   that belong to high-capacity subsets of the parameter space. Minimizing 
  
    
      
        H
        (
        W
        )
      
    
    {\displaystyle H(W)}
   in effect limits the capacity of the accessible subsets of the parameter space, thereby controlling the trade-off between minimizing the training error and minimizing the expected gap between the training error and test error.The SRM problem can be formulated in terms of data. Given n data points consisting of data x and labels y, the objective 
  
    
      
        J
        (
        θ
        )
      
    
    {\displaystyle J(\theta )}
   is often expressed in the following manner:

  
    
      
        J
        (
        θ
        )
        =
        
          
            1
            
              2
              n
            
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        (
        
          h
          
            θ
          
        
        (
        
          x
          
            i
          
        
        )
        −
        
          y
          
            i
          
        
        
          )
          
            2
          
        
        +
        
          
            λ
            2
          
        
        
          ∑
          
            j
            =
            1
          
          
            d
          
        
        
          θ
          
            j
          
          
            2
          
        
      
    
    {\displaystyle J(\theta )={\frac {1}{2n}}\sum _{i=1}^{n}(h_{\theta }(x^{i})-y^{i})^{2}+{\frac {\lambda }{2}}\sum _{j=1}^{d}\theta _{j}^{2}}
  
The first term is the mean squared error (MSE) term between the value of the learned model, 
  
    
      
        
          h
          
            θ
          
        
      
    
    {\displaystyle h_{\theta }}
  , and the given labels 
  
    
      
        y
      
    
    {\displaystyle y}
  . This term is the training error, 
  
    
      
        
          E
          
            t
            r
            a
            i
            n
          
        
      
    
    {\displaystyle E_{train}}
  , that was discussed earlier. The second term, places a prior over the weights, to favor sparsity and penalize larger weights. The trade-off coefficient, 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
  , is a hyperparameter that places more or less importance on the regularization term. Larger 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   encourages sparser weights at the expense of a more optimal MSE, and smaller 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   relaxes regularization allowing the model to fit to data. Note that as 
  
    
      
        λ
        →
        ∞
      
    
    {\displaystyle \lambda \to \infty }
   the weights become zero, and as 
  
    
      
        λ
        →
        0
      
    
    {\displaystyle \lambda \to 0}
  , the model typically suffers from overfitting.",2
Technology,Structured sparsity regularization,"Structured sparsity regularization is a class of methods, and an area of research in statistical learning theory, that extend and generalize sparsity regularization learning methods. Both sparsity and structured sparsity regularization methods seek to exploit the assumption that the output variable 
  
    
      
        Y
      
    
    {\displaystyle Y}
   (i.e., response, or dependent variable) to be learned can be described by a reduced number of variables in the input space 
  
    
      
        X
      
    
    {\displaystyle X}
   (i.e., the domain, space of features or explanatory variables). Sparsity regularization methods focus on selecting the input variables that best describe the output. Structured sparsity regularization methods generalize and extend sparsity regularization methods, by allowing for optimal selection over structures like groups or networks of input variables in 
  
    
      
        X
      
    
    {\displaystyle X}
  .Common motivation for the use of structured sparsity methods are model interpretability, high-dimensional learning (where dimensionality of 
  
    
      
        X
      
    
    {\displaystyle X}
   may be higher than the number of observations 
  
    
      
        n
      
    
    {\displaystyle n}
  ), and reduction of computational complexity. Moreover, structured sparsity methods allow to incorporate prior assumptions on the structure of the input variables, such as overlapping groups, non-overlapping groups, and acyclic graphs. Examples of uses of structured sparsity methods include face recognition, magnetic resonance image (MRI) processing, socio-linguistic analysis in natural language processing, and analysis of genetic expression in breast cancer.",2
Technology,Supervised learning,"Supervised learning (SL) is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.
The parallel task in human and animal psychology is often referred to as concept learning.

",2
Technology,Tensor sketch,"In statistics, machine learning and algorithms, a tensor sketch is a type of dimensionality reduction that is particularly efficient when applied to vectors that have tensor structure. Such a sketch can be used to speed up explicit kernel methods, bilinear pooling in neural networks and is a cornerstone in many numerical linear algebra algorithms.",2
Technology,Time series,"In mathematics, a time series is a series of data points indexed (or listed or graphed) in time order.  Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.
A time series is very frequently plotted via a run chart (which is a temporal line chart). Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.
Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values. While regression analysis is often employed in such a way as to test relationships between one or more different time series, this type of analysis is not usually called ""time series analysis"", which refers in particular to relationships between different points in time within a single series. Interrupted time series analysis is used to detect changes in the evolution of a time series from before to after some intervention which may affect the underlying variable.
Time series data have a natural temporal ordering.  This makes time series analysis distinct from cross-sectional studies, in which there is no natural ordering of the observations (e.g. explaining people's wages by reference to their respective education levels, where the individuals' data could be entered in any order).  Time series analysis is also distinct from spatial data analysis where the observations typically relate to geographical locations (e.g. accounting for house prices by the location as well as the intrinsic characteristics of the houses). A stochastic model for a time series will generally reflect the fact that observations close together in time will be more closely related than observations further apart. In addition, time series models will often make use of the natural one-way ordering of time so that values for a given period will be expressed as deriving in some way from past values, rather than from future values (see time reversibility).
Time series analysis can be applied to real-valued, continuous data, discrete numeric data, or discrete symbolic data (i.e. sequences of characters, such as letters and words in the English language).

",2
Technology,Timeline of machine learning,"This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events are included.

",2
Technology,Toronto Declaration,"The Toronto Declaration: Protecting the Rights to Equality and Non-Discrimination in Machine Learning Systems is a declaration that advocates responsible practices for machine learning practitioners and governing bodies. It is a joint statement issued by groups including Amnesty International and Access Now, with other notable signatories including Human Rights Watch and The Wikimedia Foundation. It was published at RightsCon on May 16, 2018.The Declaration focuses on concerns of algorithmic bias and the potential for discrimination that arises from the use of machine learning and artificial intelligence in applications that may affect people's lives, ""from policing, to welfare systems, to healthcare provision, to platforms for online discourse."" A secondary concern of the document is the potential for violations of information privacy.
The goal of the Declaration is to outline ""tangible and actionable standards for states and the private sector."" The Declaration calls for tangible solutions, such as reparations for the victims of algorithmic discrimination.",2
Technology,"Training, validation, and test data sets","In machine learning, a common task is the study and construction of algorithms that can learn from and make predictions on data. Such algorithms function by making data-driven predictions or decisions, through building a mathematical model from input data. These input data used to build the model are usually divided in multiple data sets. In particular, three data sets are commonly used in different stages of the creation of the model: training, validation and test sets.
The model is initially fit on a training data set, which is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model. The model (e.g. a naive Bayes classifier) is trained on the training data set using a supervised learning method, for example using optimization methods such as gradient descent or stochastic gradient descent. In practice, the training data set often consists of pairs of an input vector (or scalar) and the corresponding output vector (or scalar), where the answer key is commonly denoted as the target (or label). The current model is run with the training data set and produces a result, which is then compared with the target, for each input vector in the training data set. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both variable selection and parameter estimation.
Successively, the fitted model is used to predict the responses for the observations in a second data set called the validation data set. The validation data set provides an unbiased evaluation of a model fit on the training data set while tuning the model's hyperparameters (e.g. the number of hidden units—layers and layer widths—in a neural network). Validation datasets can be used for regularization by early stopping (stopping training when the error on the validation data set increases, as this is a sign of over-fitting to the training data set).
This simple procedure is complicated in practice by the fact that the validation dataset's error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when over-fitting has truly begun.Finally, the test data set is a data set used to provide an unbiased evaluation of a final model fit on the training data set. If the data in the test data set has never been used in training (for example in cross-validation), the test data set is also called a holdout data set. The term ""validation set"" is sometimes used instead of ""test set"" in some literature (e.g., if the original data set was partitioned into only two subsets, the test set might be referred to as the validation set).Deciding the sizes and strategies for data set division in training, test and validation sets is very dependent on the problem and data available.

",2
Technology,Transduction (machine learning),"In logic, statistical inference, and supervised learning,
transduction or transductive inference is reasoning from
observed, specific (training) cases to specific (test) cases. In contrast,
induction is reasoning from observed training cases
to general rules, which are then applied to the test cases. The distinction is
most interesting in cases where the predictions of the transductive model are
not achievable by any inductive model. Note that this is caused by transductive
inference on different test sets producing mutually inconsistent predictions.
Transduction was introduced by Vladimir Vapnik in the 1990s, motivated by
his view that transduction is preferable to induction since, according to him, induction requires
solving a more general problem (inferring a function) before solving a more
specific problem (computing outputs for new cases): ""When solving a problem of
interest, do not solve a more general problem as an intermediate step. Try to
get the answer that you really need but not a more general one."" A similar
observation had been made earlier by Bertrand Russell:
""we shall reach the conclusion that Socrates is mortal with a greater approach to 
certainty if we make our argument purely inductive than if we go by way of 'all men are mortal' and then use 
deduction"" (Russell 1912, chap VII).
An example of learning which is not inductive would be in the case of binary
classification, where the inputs tend to cluster in two groups. A large set of
test inputs may help in finding the clusters, thus providing useful information
about the classification labels. The same predictions would not be obtainable
from a model which induces a function based only on the training cases.  Some
people may call this an example of the closely related semi-supervised learning, since Vapnik's motivation is quite different. An example of an algorithm in this category is the Transductive Support Vector Machine (TSVM).
A third possible motivation which leads to transduction arises through the need
to approximate. If exact inference is computationally prohibitive, one may at
least try to make sure that the approximations are good at the test inputs. In
this case, the test inputs could come from an arbitrary distribution (not
necessarily related to the distribution of the training inputs), which wouldn't
be allowed in semi-supervised learning. An example of an algorithm falling in
this category is the Bayesian Committee Machine (BCM).",2
Technology,Transfer learning,"Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.  This area of research bears some relation to the long history of psychological literature on transfer of learning, although practical ties between the two fields are limited. From the practical standpoint, reusing or transferring information from previously learned tasks for the learning of new tasks has the potential to significantly improve the sample efficiency of a reinforcement learning agent.

",2
Technology,Tsetlin machine,A Tsetlin Machine is an Artificial Intelligence algorithm based on propositional logic.,2
Technology,Ugly duckling theorem,"The ugly duckling theorem is an argument showing that classification is not really possible without some sort of bias. More particularly, it assumes finitely many properties combinable by logical connectives, and finitely many objects; it asserts that any two different objects share the same number of (extensional) properties. The theorem is named after Hans Christian Andersen's 1843 story ""The Ugly Duckling"", because it shows that a duckling is just as similar to a swan as two swans are to each other. It was derived by Satosi Watanabe in 1969.: 376–377 ",2
Technology,Uncertain data,"In computer science, uncertain data is data that contains noise that makes it deviate from the correct, intended or original values. In the age of big data, uncertainty or data veracity is one of the defining characteristics of data. Data is constantly growing in volume, variety, velocity and uncertainty (1/veracity). Uncertain data is found in abundance today on the web, in sensor networks, within enterprises both in their structured and unstructured sources. For example, there may be uncertainty regarding the address of a customer in an enterprise dataset, or the temperature readings captured by a sensor due to aging of the sensor. In 2012 IBM called out managing uncertain data at scale in its global technology outlook report that presents a comprehensive analysis looking three to ten years into the future seeking to identify significant, disruptive technologies that will change the world. In order to make confident business decisions based on real-world data, analyses must necessarily account for many different kinds of uncertainty present in very large amounts of data. Analyses based on uncertain data will have an effect on the quality of subsequent decisions, so the degree and types of inaccuracies in this uncertain data cannot be ignored.
Uncertain data is found in the area of sensor networks; text where noisy text is found in abundance on social media, web and within enterprises where the structured and unstructured data may be old, outdated, or plain incorrect; in modeling where the mathematical model may only be an approximation of the actual process. When representing such data in a database, some indication of the probability of the correctness of the various values also needs to be estimated.
There are three main models of uncertain data in databases. In attribute uncertainty, each uncertain attribute in a tuple is subject to its own independent probability distribution. For example, if readings are taken of temperature and wind speed, each would be described by its own probability distribution, as knowing the reading for one measurement would not provide any information about the other.
In correlated uncertainty, multiple attributes may be described by a joint probability distribution. For example, if readings are taken of the position of an object, and the x- and y-coordinates stored, the probability of different values may depend on the distance from the recorded coordinates. As distance depends on both coordinates, it may be appropriate to use a joint distribution for these coordinates, as they are not independent.
In tuple uncertainty, all the attributes of a tuple are subject to a joint probability distribution. This covers the case of correlated uncertainty, but also includes the case where there is a probability of a tuple not belonging in the relevant relation, which is indicated by all the probabilities not summing to one. For example, assume we have the following tuple from a probabilistic database:

Then, the tuple has 10% chance of not existing in the database.",2
Technology,Under-fitting,"In mathematical modeling, overfitting is ""the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably"". An overfitted model is a mathematical model that contains more parameters than can be justified by the data. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.: 45 Underfitting occurs when a mathematical model cannot adequately capture the underlying structure of the data. An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing. Under-fitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.
The possibility of over-fitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data; then over-fitting occurs when a model begins to ""memorize"" training data rather than ""learning"" to generalize from a trend. 
As an extreme example, if the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. (For an illustration, see Figure 2.) Such a model, though, will typically fail severely when making predictions. 
The potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data. Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting (a phenomenon sometimes known as shrinkage). In particular, the value of the coefficient of determination will shrink relative to the original data.
To lessen the chance or amount of overfitting, several techniques are available (e.g., model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout). The basis of some techniques is either (1) to explicitly penalize overly complex models or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter.

",2
Technology,Underfitting,"In mathematical modeling, overfitting is ""the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably"". An overfitted model is a mathematical model that contains more parameters than can be justified by the data. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.: 45 Underfitting occurs when a mathematical model cannot adequately capture the underlying structure of the data. An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing. Under-fitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.
The possibility of over-fitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data; then over-fitting occurs when a model begins to ""memorize"" training data rather than ""learning"" to generalize from a trend. 
As an extreme example, if the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. (For an illustration, see Figure 2.) Such a model, though, will typically fail severely when making predictions. 
The potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data. Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting (a phenomenon sometimes known as shrinkage). In particular, the value of the coefficient of determination will shrink relative to the original data.
To lessen the chance or amount of overfitting, several techniques are available (e.g., model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout). The basis of some techniques is either (1) to explicitly penalize overly complex models or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter.

",2
Technology,Uniform convergence in probability,"Uniform convergence in probability is a form of convergence in probability in statistical asymptotic theory and probability theory. It means that, under certain conditions, the empirical frequencies of all events in a certain event-family converge to their theoretical probabilities.  Uniform convergence in probability has applications to statistics as well as machine learning as part of statistical learning theory.
The law of large numbers says that, for each single event 
  
    
      
        A
      
    
    {\displaystyle A}
  , its empirical frequency in a sequence of independent trials converges (with high probability) to its theoretical probability. In many application however, the need arises to judge simultaneously the probabilities of events of an entire class 
  
    
      
        S
      
    
    {\displaystyle S}
   from one and the same sample. Moreover it, is required that the relative frequency of the events converge to the probability uniformly over the entire class of events 
  
    
      
        S
      
    
    {\displaystyle S}
    The Uniform Convergence Theorem gives a sufficient condition for this convergence to hold. Roughly, if the event-family is sufficiently simple (its VC dimension is sufficiently small) then uniform convergence holds.",2
Technology,Universal portfolio algorithm,The universal portfolio algorithm is a portfolio selection algorithm from the field of machine learning and information theory. The algorithm learns adaptively from historical data and maximizes the log-optimal growth rate in the long run. It was introduced by the late Stanford University information theorist Thomas M. Cover.The algorithm rebalances the portfolio at the beginning of each trading period. At the beginning of the first trading period it starts with a naive diversification. In the following trading periods the portfolio composition depends on the historical total return of all possible constant-rebalanced portfolios.,2
Technology,Unsupervised learning,"Supervised learning (SL) is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.
The parallel task in human and animal psychology is often referred to as concept learning.

",2
Technology,VACUUM,"A vacuum is a space devoid of matter. The word is derived from the Latin adjective vacuus for ""vacant"" or ""void"".  An approximation to such vacuum is a region with a gaseous pressure much less than atmospheric pressure.  Physicists often discuss ideal test results that would occur in a perfect vacuum, which they sometimes simply call ""vacuum"" or free space, and use the term partial vacuum to refer to an actual imperfect vacuum as one might have in a laboratory or in space. In engineering and applied physics on the other hand, vacuum refers to any space in which the pressure is considerably lower than atmospheric pressure. The Latin term in vacuo is used to describe an object that is surrounded by a vacuum.
The quality of a partial vacuum refers to how closely it approaches a perfect vacuum. Other things equal, lower gas pressure means higher-quality vacuum. For example, a typical vacuum cleaner produces enough suction to reduce air pressure by around 20%. But higher-quality vacuums are possible. Ultra-high vacuum chambers, common in chemistry, physics, and engineering, operate below one trillionth (10−12) of atmospheric pressure (100 nPa), and can reach around 100 particles/cm3. Outer space is an even higher-quality vacuum, with the equivalent of just a few hydrogen atoms per cubic meter on average in intergalactic space.Vacuum has been a frequent topic of philosophical debate since ancient Greek times, but was not studied empirically until the 17th century. Evangelista Torricelli produced the first laboratory vacuum in 1643, and other experimental techniques were developed as a result of his theories of atmospheric pressure. A Torricellian vacuum is created by filling a tall glass container closed at one end with mercury, and then inverting it in a bowl to contain the mercury (see below).Vacuum became a valuable industrial tool in the 20th century with the introduction of incandescent light bulbs and vacuum tubes, and a wide array of vacuum technologies has since become available. The development of human spaceflight has raised interest in the impact of vacuum on human health, and on life forms in general.",2
Technology,Validation set,"In machine learning, a common task is the study and construction of algorithms that can learn from and make predictions on data. Such algorithms function by making data-driven predictions or decisions, through building a mathematical model from input data. These input data used to build the model are usually divided in multiple data sets. In particular, three data sets are commonly used in different stages of the creation of the model: training, validation and test sets.
The model is initially fit on a training data set, which is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model. The model (e.g. a naive Bayes classifier) is trained on the training data set using a supervised learning method, for example using optimization methods such as gradient descent or stochastic gradient descent. In practice, the training data set often consists of pairs of an input vector (or scalar) and the corresponding output vector (or scalar), where the answer key is commonly denoted as the target (or label). The current model is run with the training data set and produces a result, which is then compared with the target, for each input vector in the training data set. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both variable selection and parameter estimation.
Successively, the fitted model is used to predict the responses for the observations in a second data set called the validation data set. The validation data set provides an unbiased evaluation of a model fit on the training data set while tuning the model's hyperparameters (e.g. the number of hidden units—layers and layer widths—in a neural network). Validation datasets can be used for regularization by early stopping (stopping training when the error on the validation data set increases, as this is a sign of over-fitting to the training data set).
This simple procedure is complicated in practice by the fact that the validation dataset's error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when over-fitting has truly begun.Finally, the test data set is a data set used to provide an unbiased evaluation of a final model fit on the training data set. If the data in the test data set has never been used in training (for example in cross-validation), the test data set is also called a holdout data set. The term ""validation set"" is sometimes used instead of ""test set"" in some literature (e.g., if the original data set was partitioned into only two subsets, the test set might be referred to as the validation set).Deciding the sizes and strategies for data set division in training, test and validation sets is very dependent on the problem and data available.

",2
Technology,Vanishing gradient problem,"In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, during each iteration of training each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. As one example of the problem cause, traditional activation functions such as the hyperbolic tangent function have gradients in the range (0,1], and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the early layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n while the early layers train very slowly.
Back-propagation allowed researchers to train supervised deep artificial neural networks from scratch, initially with little success. Hochreiter's diplom thesis of 1991 formally identified the reason for this failure in the ""vanishing gradient problem"", which not only affects many-layered feedforward networks, but also recurrent networks. The latter are trained by unfolding them into very deep feedforward networks, where a new layer is created for each time step of an input sequence processed by the network. (The combination of unfolding and backpropagation is termed backpropagation through time.)
When activation functions are used whose derivatives can take on larger values, one risks encountering the related exploding gradient problem.

",2
Technology,Version space learning,"Version space learning is a logical approach to machine learning, specifically binary classification. Version space learning algorithms search a predefined space of hypotheses, viewed as a set of logical sentences. Formally, the hypothesis space is a disjunction

  
    
      
        
          H
          
            1
          
        
        ∨
        
          H
          
            2
          
        
        ∨
        .
        .
        .
        ∨
        
          H
          
            n
          
        
      
    
    {\displaystyle H_{1}\lor H_{2}\lor ...\lor H_{n}}
  (i.e., either hypothesis 1 is true, or hypothesis 2, or any subset of the hypotheses 1 through n). A version space learning algorithm is presented with examples, which it will use to restrict its hypothesis space; for each example x, the hypotheses that are inconsistent with x are removed from the space. This iterative refining of the hypothesis space is called the candidate elimination algorithm, the hypothesis space maintained inside the algorithm its version space.

",2
Technology,Weak supervision,"Weak supervision is a branch of machine learning where noisy, limited, or imprecise sources are used to provide supervision signal for labeling large amounts of training data in a supervised learning setting. This approach alleviates the burden of obtaining hand-labeled data sets, which can be costly or impractical. Instead, inexpensive weak labels are employed with the understanding that they are imperfect, but can nonetheless be used to create a strong predictive model.

",2
Technology,Word2vec,"Word2vec is a technique for natural language processing published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that a simple mathematical function (the cosine similarity between the vectors) indicates the level of semantic similarity between the words represented by those vectors.

",2
Technology,Outline of computer security,"The following outline is provided as an overview of and topical guide to computer security:
Computer security is commonly known as security applied to computing devices such as computers and smartphones, as well as computer networks such as private and public networks, including the whole Internet.  The field covers all the processes and mechanisms by which digital equipment, information and services are protected from unintended or unauthorized access, change or destruction, and is of growing importance in line with the increasing reliance on computer systems of most societies worldwide. Computer security includes measures taken to ensure the integrity of files stored on a computer or server as well as measures taken to prevent unauthorized access to stored data, by securing the physical perimeter of the computer equipment, authentication of users or computer accounts accessing the data, and providing a secure method of data transmission.",2
Technology,Access level,"In computer science and computer programming, access level denotes the set of permissions or restrictions provided to a data type. Reducing access level is an effective method for limiting failure modes, reducing debugging time, and simplifying overall system complexity. It restricts variable modification to only the methods defined within the interface to the class. Thus, it is incorporated into many fundamental software design patterns. In general, a given object cannot be created, read, updated or deleted by any function without having a sufficient access level.
The two most common access levels are public and private, which denote, respectively; permission across the entire program scope, or permission only within the corresponding class. A third, protected, extends permissions to all subclasses of the corresponding class. Access levels modifiers are commonly used in Java  as well as C#, which further provides the internal level. In C++, the only difference between a struct and a class is the default access level, which is private for classes and public for structs.To illustrate the benefit: consider a public variable which can be accessed from any part of a program. If an error occurs, the culprit could be within any portion of the program, including various sub-dependencies. In a large code base, this leads to thousands of potential sources. Alternatively, consider a private variable. Due to access restrictions, all modifications to its value must occur via functions defined within the class. Therefore, the error is structurally contained within the class. There is often only a single source file for each class, which means debugging only requires evaluation of a single file. With sufficient modularity and minimal access level, large code bases can avoid many challenges associated with complexity.",2
Technology,Adrozek,"Adrozek is malware that injects fake ads into online search results. Microsoft announced the malware threat on 10 December 2020, and noted that many different browsers are affected, including  Google Chrome, Microsoft Edge, Mozilla Firefox and Yandex Browser. The malware was first detected in May 2020 and, at its peak in August 2020, controlled over 30,000 devices a day. But during the December 2020 announcement, Microsoft claimed ""hundreds of thousands"" of infected devices worldwide between May and September 2020.According to Microsoft, if not detected and blocked, Adrozek adds browser extensions, modifies a specific DLL per target browser, and changes browser settings to insert additional, unauthorized ads into web pages, often on top of legitimate ads from search engines. For each user tricked into clicking on the fake ads, the scammers earn affiliate advertising dollars. The malware has been observed to extract device data and, in some cases, steal credentials, sending them to remote servers.Users may unintentionally install the malware because of a drive-by download, by visiting a tampered website, opening an e-mail attachment, or clicking on a deceptive link or a deceptive pop-up window. The main malware program is downloaded to the “Programs Files” folder using file names such as Audiolava.exe, QuickAudio.exe, and converter.exe. According to PC Magazine, a good way to avoid, or mitigate, infection by Adrozek is to keep browser and related software programs up to date.",2
Technology,Air India data breach,The 2021 Air India cyberattack was a cyberattack that affected more than 4.5 million customers of Air India airlines.,2
Technology,American Innovation and Competitiveness Act,"The American Innovation and Competitiveness Act (AICA) is a United States federal law enacted in 2017 by President Barack Obama that aims to invest in cybersecurity and cryptography research. The legislation was initially introduced in the Senate by Cory Gardner (R-CO) and Gary Peters (D-MI). The legislation serves as a reauthorization of the 2010 America COMPETES Act that expired in 2013.The legislation updates instructions to the National Science Foundation and the National Institute of Standards and Technology (NIST), with a director of security position being created in the latter latter. AICA supports the coordination of citizen science and crowdsourcing by Federal agencies to accomplish their missions.",2
Technology,Anderson's rule (computer science),"In the field of computer security, Anderson's rule refers to a principle formulated by Ross J. Anderson: systems that handle sensitive personal information involve a trilemma of security, functionality and scale, of which you can choose any two. A system that has information on many data subjects and to which many people require access is hard to secure unless its functionality is severely restricted. If it has rich functionality, you may have to restrict the number of people with access, or accept that some information will leak.",2
Technology,Anomaly Detection at Multiple Scales,"Anomaly Detection at Multiple Scales, or ADAMS, is a $35 million DARPA project designed to identify patterns and anomalies in very large data sets. It is under DARPA's Information Innovation office and began in 2011.The project is intended to detect and prevent insider threats such as ""a soldier in good mental health becoming homicidal or
suicidal"", an ""innocent insider becoming malicious"", or ""a government employee [who] abuses access privileges to share classified information"". Specific cases mentioned are Nidal Malik Hasan and WikiLeaks source Chelsea Manning. Commercial applications may include finance. The intended recipients of the system output are operators in the counterintelligence agencies.The Proactive Discovery of Insider Threats Using Graph Analysis and Learning is part of the ADAMS project. The Georgia Tech team includes noted high-performance computing researcher David A. Bader.",2
Technology,Application portfolio attack surface,"In the realm of application security, the term Application Portfolio Attack Surface or APAS, refers to the collective risk to an organization posed by the sum total of the security vulnerabilities found within the set of all mission critical systems or software run by the organization or enterprise.",2
Technology,Aptum Technologies,"Aptum Technologies, formerly Cogeco Peer 1,  is a provider of services for data centers and cloud computing. The company is headquartered in Toronto, Ontario, Canada.",2
Technology,Aurora Generator Test,"Idaho National Laboratory ran the Aurora Generator Test in 2007 to demonstrate how a cyberattack could destroy physical components of the electric grid. The experiment used a computer program to rapidly open and close a diesel generator's circuit breakers out of phase from the rest of the grid, thereby subjecting the engine to abnormal torques and ultimately causing it to explode. This vulnerability is referred to as the Aurora Vulnerability.
This vulnerability is especially a concern because most grid equipment supports using Modbus and other legacy communications protocols that were designed without security in mind. As such, they do not support authentication, confidentiality, or replay protection.  This means that any attacker that can communicate with the device can control it and use the Aurora Vulnerability to destroy it.",2
Technology,Automotive security,"Automotive security refers to the branch of computer security focused on the cyber risks related to the automotive context. The increasingly high number of ECUs in vehicles and, alongside, the implementation of multiple different means of communication from and towards the vehicle in a remote and wireless manner led to the necessity of a branch of cybersecurity dedicated to the threats associated with vehicles. Not to be confused with automotive safety.",2
Technology,Biometric device,"A biometric device is a security identification and authentication device.  Such devices use automated methods of verifying or recognising the identity of a living person based on a physiological  or behavioral characteristic. These characteristics include fingerprints, facial images, iris and voice recognition.

",2
Technology,Blue team (computer security),"A blue team is a group of individuals who perform an analysis of information systems to ensure security, identify security flaws, verify the effectiveness of each security measure, and to make certain all security measures will continue to be effective after implementation.",2
Technology,BlueBorne (security vulnerability),"BlueBorne is a type of security vulnerability with Bluetooth implementations in Android, iOS, Linux and Windows. It affects many electronic devices such as laptops, smart cars, smartphones and wearable gadgets. One example is CVE-2017-14315.  The vulnerabilities were first reported by Armis, an IoT security firm, on 12 September 2017. According to Armis, ""The BlueBorne attack vector can potentially affect all devices with Bluetooth capabilities, estimated at over 8.2 billion devices today [2017].""",2
Technology,BlueHat,"BlueHat (or Blue Hat or Blue-Hat) is a term used to refer to outside computer security consulting firms that are employed to bug test a system prior to its launch, looking for exploits so they can be closed. In particular, Microsoft uses the term to refer to the computer security professionals they invited to find the vulnerability of their products such as Windows.",2
Technology,British Airways data breach,"In 2018 there  was a data breach that affected 380,000 to 500,000 customers of British Airways.",2
Technology,Camfecting,"Camfecting, in the field of computer security, is the process of attempting to hack into a person's webcam and activate it without the webcam owner's permission. The remotely activated webcam can be used to watch anything within the webcam's field of vision, sometimes including the webcam owner themselves. Camfecting is most often carried out by infecting the victim's computer with a virus that can provide the hacker access to their webcam. This attack is specifically targeted at the victim's webcam, and hence the name camfecting, a portmanteau of the words camera and infecting.
Typically, a webcam hacker or a camfecter sends his victim an innocent-looking application which has a hidden Trojan software through which the camfecter can control the victim's webcam. The camfecter virus installs itself silently when the victim runs the original application. Once installed, the camfecter can turn on the webcam and capture pictures/videos. The camfecter software works just like the original webcam software present in the victim computer, the only difference being that the camfecter controls the software instead of the webcam's owner.",2
Technology,Capture the flag (cybersecurity),"Capture the Flag (CTF) in computer security is an exercise in which ""flags"" are secretly hidden in purposefully-vulnerable programs or websites. Competitors steal flags either from other competitors (attack/defense-style CTFs) or from the organizers (jeopardy-style challenges). Several variations exist, including hiding flags in hardware devices. Competitions exist both online and in-person, and can be advanced or entry-level. The game is based on the traditional outdoor sport of the same name.",2
Technology,Centurion guard,"The Centurion Guard is a PC hardware and software based security product developed by Centurion Technologies and released in 1996.  There were many different releases and versions of this product, and many were distributed in the Bill & Melinda Gates Foundation computers that were donated to libraries.",2
Technology,Ciscogate,"Ciscogate, also known as the Black Hat Bug, is the name given to a legal incident that occurred at the Black Hat Briefings security conference in Las Vegas, Nevada, on July 27, 2005.On the morning of the first day of the conference, July 26, 2005, some attendees noticed that 30 pages of text had been physically ripped out of the extensive conference presentation booklet the night before at the request of Cisco Systems and the CD-ROM with presentation slides was not included. It was determined the pages covered a talk to be given by Michael Lynn, a security researcher with Atlanta-based IBM Internet Security Systems (ISS). Instead of the pages with the details, attendees found a photographed copy of a notice from Black Hat saying ""Due to some last minute changes beyond Black Hat's control, and at the request of the presenter, the included materials aren't up to the standards Black Hat tries to meet. Black Hat will be the first to apologize. We hope the vendors involved will follow suit."" According to Lynn's lawyer, his employer had approved of the talk leading up to the conference but changed their minds two days before the scheduled talk, forbidding him from presenting.Lynn's original presentation was to cover a vulnerability in Cisco routers. The presentation was one of four scheduled to follow Jeff Moss' keynote address on the first day of the conference, titled ""Cisco IOS Security Architecture"". After being told by his employer that he could not present on the topic, Lynn chose an alternate topic. Cisco and ISS had offered to give new joint presentation but this was turned down by Black Hat because the original speaking slot was given to Lynn, not Cisco. Lynn's presentation began by covering security issues in services that allow users to make Voice over IP telephone calls. Shortly after beginning the presentation Lynn changed back to his original topic and began disclosing some technical details of the vulnerability he found in Cisco routers stating that he would rather resign from his job at ISS than keep the details private.",2
Technology,Client honeypot,"Honeypots are security devices whose value lie in being probed and compromised. Traditional honeypots are servers (or devices that expose server services) that wait passively to be attacked. Client Honeypots are active security devices in search of malicious servers that attack clients. The client honeypot poses as a client and interacts with the server to examine whether an attack has occurred. Often the focus of client honeypots is on web browsers, but any client that interacts with servers can be part of a client honeypot (for example ftp, ssh, email, etc.).
There are several terms that are used to describe client honeypots. Besides client honeypot, which is the generic classification, honeyclient is the other term that is generally used and accepted. However, there is a subtlety here, as ""honeyclient"" is actually a homograph that could also refer to the first known open source client honeypot implementation (see below), although this should be clear from the context.",2
Technology,Cloud computing security,"Cloud computing security or, more simply, cloud security  refers to a broad set of policies, technologies, applications, and controls utilized to protect virtualized IP, data, applications, services, and the associated infrastructure of cloud computing. It is a sub-domain of computer security, network security, and, more broadly, information security.",2
Technology,CloudPassage,"CloudPassage is a company that provides an automation platform, delivered via software as a service, that improves security for private, public, and hybrid cloud computing environments. CloudPassage is headquartered in San Francisco.

",2
Technology,Co-managed Security,"The Co-Managed IT security service model entails security monitoring, event correlation, incident response, system tuning, and compliance support across an organization's entire IT environment. Co-Management allows organizations to collaborate with their managed security service providers by blending security expertise of the provider with the contextual knowledge of the customer to optimise security posture.Outsourcing all IT security affairs can leave clients in the dark in regards to major security breaches or events.The combined involvement of the client and managed security service providers (MSSP) in relation to SIEM softwares allow for immediate response to security breaches, increased transparency and reduce workload for internal IT security teams.
The cooperative management of SIEM softwares can allow for the sharing of expert knowledge between internal IT security teams and the MSSP.
Co-Managed security services also allow for organizations’ critical data and tools to be remotely managed by a team of certified engineers and security analysts from a 24/7/365 Security Operations Center (SOC). This service approach allows for customer data to remain in-house while the SIEM platform is either on-site or cloud-hosted. The service provider's staff work in conjunction with customer security teams to outline the rules of engagement inside the environment to provide monitoring and response to alerts in real-time. The Security Operation Center also provide the “care and feeding”, and development of a variety of security solutions. The co-managed approach also allows organizations to focus on emerging internal projects and other critical areas of IT.
According to Gartner's How and When to Use Co-managed Security Information and Event Management report, “Co-managed SIEM services enable security and risk management leaders to maximize value from SIEM and enhance security monitoring capabilities, while retaining control and flexibility.” and ""Co-management is on the rise and expected to grow five-fold by 2020.""
SIEM, IDS/IPS, Compliance Automation, Network Configuration Management Tools, Advanced Threat Intelligence, Network Access Control, Endpoint Threat Detection and Response, Application Security, File Integrity Monitoring, Forensic Investigation, and Vulnerability Scanning and Assessment, are all examples of cyber security solutions that co-managed service providers support.",2
Technology,Collaboration-oriented architecture,"Collaboration Oriented Architecture (COA) is a computer system that is designed to collaborate, or use services, from systems that are outside of the operators control. Collaboration Oriented Architecture will often use Service Oriented Architecture to deliver the technical framework.
Collaboration Oriented Architecture is the ability to collaborate between systems that are based on the Jericho Forum principles or ""Commandments"".Bill Gates and Craig Mundie (Microsoft)  clearly articulated the need for people to work outside of their organizations in a secure and collaborative manner in their opening keynote to the RSA Security Conference in February 2007.Successful implementation of a Collaboration Oriented Architecture implies the ability to successfully inter-work securely over the Internet and will typically mean the resolution of the problems that come with de-perimeterisation.",2
Technology,Collateral freedom,"Collateral freedom is an anti-censorship strategy that attempts to make it economically prohibitive for censors to block content on the Internet. This is achieved by hosting content on cloud services that are considered by censors to be ""too important to block,"" and then using encryption to prevent censors from identifying requests for censored information that is hosted among other content, forcing censors to either allow access to the censored information or take down entire services.",2
Technology,Commission on Enhancing National Cybersecurity,"The President's Commission on Enhancing National Cybersecurity is a Presidential Commission formed on April 13, 2016, to develop a plan for protecting cyberspace, and America's economic reliance on it. The commission released its final report in December 2016.  The report made recommendations regarding the intertwining roles of the military, government administration and the private sector in providing cyber security.  Chairman Donilon said of the report that its coverage ""is unusual in the breadth of issues"" with which it deals.",2
Technology,Computer Law & Security Review,"The Computer Law & Security Review is a journal accessible to a wide range of professional legal and IT practitioners, businesses, academics, researchers, libraries and organisations in both the public and private sectors, the Computer Law and Security Review regularly covers:

CLSR Briefing with special emphasis on UK/US developments
European Union update
National news from 10 European jurisdictions
Pacific rim news column
Refereed practitioner and academic papers on topics such as Web 2.0, IT security, Identity management, ID cards, RFID, interference with privacy, Internet law, telecoms regulation, online broadcasting, intellectual property, software law, e-commerce, outsourcing, data protection and freedom of information and many other topics.The Journal's Correspondent Panel includes more than 40 specialists in IT law and security.
Each issue contains articles, case law analysis and current news on information and communications technology.
Special Features

High quality peer reviewed papers from internationally renowned practitioner and academic experts
Latest developments reported in situ by more than 20 leading law firms from around the world
Highly experienced and respected editor and correspondents panel
Online access to all 23 volumes of CLSR with embedded web links to primary sources
Contact details of all authors
A pool of expertise that can collectively identify the key topics that need to be examined.",2
Technology,Computer security,"Computer security, cybersecurity (cyber security), or information technology security (IT security) is the protection of computer systems and networks from information disclosure, theft of, or damage to their hardware, software, or electronic data, as well as from the disruption or misdirection of the services they provide.The field has become significant due to the expanded reliance on computer systems, the Internet, and wireless network standards such as Bluetooth and Wi-Fi, and due to the growth of ""smart"" devices, including smartphones, televisions, and the various devices that constitute the Internet of things (IoT). Cybersecurity is also one of the significant challenges in the contemporary world, due to its complexity, both in terms of political usage and technology. Its primary goal is to ensure the system's dependability, integrity, and data privacy.",2
Technology,Computer security compromised by hardware failure,"Computer security compromised by hardware failure is a branch of computer security applied to hardware.
The objective of computer security includes protection of information and property from theft, corruption, or natural disaster, while allowing the information and property to remain accessible and productive to its intended users. Such secret information could be retrieved by different ways. This article focus on the retrieval of data thanks to misused hardware or hardware failure. Hardware could be misused or exploited to get secret data. This article collects main types of attack that can lead to data theft.
Computer security can be comprised by devices, such as keyboards, monitors or printers (thanks to electromagnetic or acoustic emanation for example) or by components of the computer, such as the memory, the network card or the processor (thanks to time or temperature analysis for example).",2
Technology,Computer security incident management,"In the fields of computer security and information technology, computer security incident management involves the monitoring and detection of security events on a computer or computer network, and the execution of proper responses to those events. Computer security incident management is a specialized form of incident management, the primary purpose of which is the development of a well understood and predictable response to damaging events and computer intrusions.Incident management requires a process and a response team which follows this process.  This definition of computer security incident management follows the standards and definitions described in the National Incident Management System (NIMS). The incident coordinator manages the response to an emergency security incident.  In a Natural Disaster or other event requiring response from Emergency services, the incident coordinator would act as a liaison to the emergency services incident manager.

",2
Technology,Confused deputy problem,"In information security, the confused deputy problem is often cited as an example of why capability-based security is important. A confused deputy is a legitimate, more privileged computer program that is tricked by another program into misusing its authority on the system.  It is a specific type of privilege escalation.Capability systems protect against the confused deputy problem, whereas access control list-based systems do not.",2
Technology,Content Disarm & Reconstruction,"Content Disarm & Reconstruction (CDR) is a computer security technology for removing potentially malicious code from files. Unlike malware analysis, CDR technology does not determine or detect malware's functionality but removes all file components that are not approved within the system's definitions and policies.It is used to prevent cyber security threats from entering a corporate network perimeter. Channels that CDR can be used to protect include email and website traffic. Advanced solutions can also provide similar protection on computer endpoints, or cloud email and file sharing services.
There are three levels of CDR; 1) flattening and converting the original file to a PDF, 2) stripping active content while keeping the original file type, and 3) eliminating all file-borne risk while maintaining file type, integrity and active content. Beyond these three levels, there are also more advanced forms of CDR that is able to perform ""soft conversion"" and ""hard conversion"", based on the user's preference in balancing usability and security.",2
Technology,Content Threat Removal,"Content Threat Removal (CTR) is a cyber security technology intended to defeat the threat posed by handling digital content in cyberspace. Unlike other defences, including antivirus software and sandboxed execution, it does not rely on being able to detect threats. Similarly to Content Disarm and Reconstruction, CTR is designed to remove the threat without knowing whether it has done so, and acts without knowing if data contains a threat or not.
Detection strategies work by detecting unsafe content, and then blocking or removing that content. Content that is deemed safe is delivered to its destination. In contrast, Content Threat Removal assumes all data is hostile and delivers none of it to the destination, regardless of whether it is actually hostile. Although no data is delivered, the business information carried by the data is delivered, but using new data created for the purpose.

",2
Technology,Control-flow integrity,Control-flow integrity (CFI) is a general term for computer security techniques that prevent a wide variety of malware attacks from redirecting the flow of execution (the control flow) of a program.,2
Technology,Cowrie (honeypot),"Cowrie is a medium interaction SSH and Telnet honeypot designed to log brute force attacks and shell interaction performed by an attacker. Cowrie also functions as an SSH and telnet proxy to observe attacker behavior to another system. Cowrie was developed from Kippo.

",2
Technology,Crackme,"A crackme (often abbreviated by cm) is a small program designed to test a programmer's reverse engineering skills.They are programmed by other reversers as a legal way to crack software, since no intellectual property is being infringed upon.
Crackmes, reversemes and keygenmes generally have similar protection schemes and algorithms to those found in proprietary software. However, due to the wide use of packers/protectors in commercial software, many crackmes are actually more difficult as the algorithm is harder to find and track than in commercial software.",2
Technology,Cure53,"Cure53 is a German cybersecurity firm. The company was founded by Dr. Mario Heidrich, a client side security researcher.
After a report from Cure53 on the South Korean security app Smart Sheriff, that described the apps security holes as ""catastrophic"", the South Korean government ordered the Smart Sheriff to be shut down.

",2
Technology,Cyber and Information Domain Service,"The Cyber and Information Domain Service (German: Cyber- und Informationsraum, German pronunciation: [ˈsaɪ̯bɐ ʊnt ɪnfɔʁmaˈt͡si̯oːnsˌʁaʊ̯m]; CIR) is the youngest branch of Germany's military, the Bundeswehr. The decision to form a new  military branch was presented by Defense Minister Ursula von der Leyen on 26 April 2016, becoming operational on 1 April 2017. The headquarter of the Cyber and Information Domain Service is Bonn.",2
Technology,Cyber Discovery,"Cyber Discovery was a United Kingdom initiative to get teenagers interested in cyber security. The initiative was funded £20 million by the UK Department for Digital, Culture, Media and Sport in partnership with SANS Institute Started in 2017, each year the program had followed a similar pattern of 4 (often overlapping) stages.",2
Technology,Cyber Intelligence Sharing and Protection Act,"The Cyber Intelligence Sharing and Protection Act (CISPA H.R. 3523 (112th Congress), H.R. 624 (113th Congress), H.R. 234 (114th Congress)) was a proposed law in the United States which would allow for the sharing of Internet traffic information between the U.S. government and technology and manufacturing companies. The stated aim of the bill is to help the U.S. government investigate cyber threats and ensure the security of networks against cyberattacks.The legislation was introduced on November 30, 2011, by Representative Michael Rogers (R-MI) and 111 co-sponsors. It was passed in the House of Representatives on April 26, 2012, but was not passed by the U.S. Senate. President Barack Obama's advisers have argued that the bill lacks confidentiality and civil liberties safeguards, and the White House said he would veto it.In February 2013, the House reintroduced the bill and it passed in the United States House of Representatives on April 18, 2013, but stalled and was not voted upon by the Senate. On July 10, 2014 a similar bill, the Cybersecurity Information Sharing Act (CISA), was introduced in the Senate.In January 2015, the House reintroduced the bill again. The bill has been referred to the Committee on Intelligence, and as of February 2, 2015 to the Subcommittee on Crime, Terrorism, Homeland Security, and Investigations and Subcommittee on Constitution and Civil Justice to see if it will come to the House for a vote. In December 2015 a version of CISPA was hidden in the total federal budget.
CISPA had garnered favor from corporations and lobbying groups such as Microsoft, Facebook, AT&T, IBM, and the United States Chamber of Commerce, which look on it as a simple and effective means of sharing important cyber threat information with the government. It has however been criticized by advocates of Internet privacy and civil liberties, such as the Electronic Frontier Foundation, the American Civil Liberties Union, Free Press, Fight for the Future, and Avaaz.org, as well as various conservative and libertarian groups including the Competitive Enterprise Institute, TechFreedom, FreedomWorks, Americans for Limited Government, Liberty Coalition, and the American Conservative Union. Those groups argue CISPA contains too few limits on how and when the government may monitor a private individual's Internet browsing information. Additionally, they fear that such new powers could be used to spy on the general public rather than to pursue malicious hackers.Some critics saw wording included in CISPA as a second attempt to protect intellectual property after the Stop Online Piracy Act was taken off the table by Congress after it met opposition. Intellectual property theft was initially listed in the bill as a possible cause for sharing Web traffic information with the government, though it was removed in subsequent drafts.",2
Technology,Cyber Ireland,"Cyber Ireland is a business cluster organisation that connects businesses, educational institutions and the state to support the growing cyber security industry in Ireland, which consisted of over 60 domestic and 40 multinational cyber-security companies in 2018.  Launched in 2019 by the IDA and Enterprise Ireland's Regional Technology Cluster Fund, Cyber Ireland had over 180 members as of the start of 2022  including Johnson Controls International, Dell EMC, IBM, McAfee, McKesson and Trend Micro. Cyber Ireland was the first business cluster to be formed in the country.",2
Technology,Cyber risk quantification,"Cyber risk quantification involves the application of risk quantification techniques to an organization's cybersecurity risk.  Cyber risk quantification is the process of evaluating the cyber risks that have been identified and then validating, measuring and analyzing the available cyber data using mathematical modeling techniques to accurately represent the organization's cybersecurity environment in a manner that can be used to make informed cybersecurity infrastructure investment and risk transfer decisions.  Cyber risk quantification is a supporting activity to cybersecurity risk management; cybersecurity risk management is a component of enterprise risk management and is especially important in organizations and enterprises that are highly dependent upon their information technology (IT) networks and systems for their business operations.
One method of quantifying cyber risk is the value-at-risk (VaR) method that is discussed at the January 2015 World Economic Forum meeting.  At this meeting, VaR was studied and researched and deemed to be a viable method of quantifying cyber risk. 
A well known framework for cyber risk quantification is called FAIRTM (Factor Analysis of Information Risk). The FAIR Institute is a non-profit professional organization committed to furthering the science of cyber and operational risk measurement and management. Cyber-Risk Quantification can be an automated or software supported process allowing Users to construct mathematical models to quantify Cyber-Security risks.",2
Technology,Cyber self-defense,"In cybersecurity, cyber self-defense refers to self-defense against cyberattack. While it generally emphasizes active cybersecurity measures by computer users themselves, cyber self-defense is sometimes used to refer to the self-defense of organizations as a whole, such as corporate entities or entire nations. Surveillance self-defense is a variant of cyber self-defense and largely overlaps with it. Active and passive cybersecurity measures provide defenders with higher levels of cybersecurity, intrusion detection, incident handling and remediation capabilities.  Various sectors and organizations are legally obligated to adhere to cyber security standards.

",2
Technology,Cyber Storm Exercise,"The Cyber Storm exercise is a biennial simulated exercise overseen by the United States Department of Homeland Security that took place February 6 through February 10, 2006 with the purpose of testing the nation's defenses against digital espionage. The simulation was targeted primarily at American security organizations but officials from the United Kingdom, Canada, Australia and New Zealand participated as well.",2
Technology,Cyber Threat Intelligence Integration Center,"The Cyber Threat Intelligence Integration Center (CTIIC) is a new United States federal government agency that will be a fusion center between existing agencies and the private sector for real-time use against cyber attacks. CTIIC was created due to blocked efforts in Congress that were stymied over liability and privacy concerns of citizens.CTIIC was formally announced by Lisa Monaco February 10, 2015 at the Wilson Center. The agency will be within the Office of the Director of National Intelligence.",2
Technology,Cyberbiosecurity,"Cyberbiosecurity is an emerging field at the intersection of cybersecurity and biosecurity. The objective of cyberbiosecurity has been described as addressing ""the potential for or actual malicious destruction, misuse, or exploitation of valuable information, processes, and material at the interface of the life sciences and digital worlds"". Cyberbiosecurity is part of a system of measures that collectively aim to ""Safeguard the Bioeconomy"", an objective described by the National Academies of Sciences, Engineering and Medicine of the United States.",2
Technology,Cybercrime,"Cybercrime is a crime that involves a computer and a network. The computer may have been used in the commission of a crime, or it may be the target. Cybercrime may harm someone's security and financial health.There are many privacy concerns surrounding cybercrime when confidential information is intercepted or disclosed, lawfully or otherwise. Internationally, both governmental and non-state actors engage in cybercrimes, including espionage, financial theft, and other cross-border crimes. Cybercrimes crossing international borders and involving the actions of at least one nation-state are sometimes referred to as cyberwarfare. Warren Buffett describes cybercrime as the ""number one problem with mankind"" and ""poses real risks to humanity.""A report (sponsored by McAfee) published in 2014 estimated that the annual damage to the global economy was $445 billion. A 2016 report by Cybersecurity Ventures predicted that global damages incurred as a result of cybercrime would cost up to $6 trillion annually by 2021 and $10.5 trillion annually by 2025.Approximately $1.5 billion was lost in 2012 to online credit and debit card fraud in the US. In 2018, a study by the Center for Strategic and International Studies (CSIS), in partnership with McAfee, concludes that nearly one percent of global GDP, close to $600 billion, is lost to cybercrime each year. The World Economic Forum 2020 Global Risk report confirmed that organized Cybercrimes bodies are joining forces to perpetrate criminal activities online while estimating the likelihood of their detection and prosecution to be less than 1 percent in the US.",2
Technology,CyberPatriot,"CyberPatriot is a national youth cyber education program created in the United States to help direct students toward careers in cybersecurity or other computer, science, technology, engineering, and mathematics disciplines. The program was created by the Air Force Association (AFA).  It features the annual National Youth Cyber Defense Competition for high school and middle school students. It is similar to its collegiate counterpart, the Collegiate Cyber Defense Competition (CCDC), especially at the CyberPatriot National Finals Competition.
The National Youth Cyber Defense Competition is now in its thirteenth season and is called ""CyberPatriot XIII"" indicating the season's competition.  CyberPatriot XIII is open to all high schools, middle schools, and accredited home school programs around the country. JROTC units of all Services, Civil Air Patrol squadrons, and Naval Sea Cadet Corps divisions may also participate in the competition.  Outside of the regular competition, CyberPatriot also hosts two additional sub-programs: Summer CyberCamps and an Elementary School Cyber Education Initiative. The Northrop Grumman Foundation is the ""presenting sponsor"". A spin off program is run in the UK called Cyber Centurion.",2
Technology,Cybersecurity Information Sharing Act,"The Cybersecurity Information Sharing Act (CISA S. 2588 [113th Congress], S. 754 [114th Congress]) is a United States federal law designed to ""improve cybersecurity in the United States through enhanced sharing of information about cybersecurity threats, and for other purposes"". The law allows the sharing of Internet traffic information between the U.S. government and technology and manufacturing companies. The bill was introduced in the U.S. Senate on July 10, 2014, and passed in the Senate October 27, 2015.  Opponents question CISA's value, believing it will move responsibility from private businesses to the government, thereby increasing vulnerability of personal private information, as well as dispersing personal private information across seven government agencies, including the NSA and local police.
The text of the bill was incorporated by amendment into a consolidated spending bill in the U.S. House on December 15, 2015, which was signed into law by President Barack Obama on December 18, 2015.",2
Technology,Cybersex trafficking,"Cybersex trafficking, live streaming sexual abuse, webcam sex tourism/abuse or ICTs (Information and Communication Technologies)-facilitated sexual exploitation is a cybercrime involving sex trafficking and the live streaming of coerced sexual acts and/or rape on webcam.Cybersex trafficking is distinct from other sex crimes. Victims are transported by traffickers to 'cybersex dens', which are locations with webcams and internet-connected devices with live streaming software. There, victims are forced to perform sexual acts on themselves or other people in sexual slavery or raped by the traffickers or assisting assaulters in live videos. Victims are frequently ordered to watch the paying live distant consumers or purchasers on shared screens and follow their commands. It is often a commercialized, cyber form of forced prostitution. Women, children, and people in poverty are particularly vulnerable to coerced internet sex. The computer-mediated communication images produced during the crime are a type of rape pornography or child pornography that is filmed and broadcast in real time and can be recorded.There is no data about the magnitude of cybersex trafficking in the world. The technology to detect all incidents of the live streaming crime has not been developed yet. Millions of reports of cybersex trafficking are sent to authorities annually. It is a billion-dollar, illicit industry that was brought on with the Digital Age and is connected to globalization. It has surged from the world-wide expansion of telecommunications and global proliferation of the internet and smartphones, particularly in developing countries. It has also been facilitated by the use of software, encrypted communication systems, and network technologies that are constantly evolving, as well as the growth of international online payment systems with wire transfer services and cryptocurrencies that hide the transactor's identities.The transnational nature and global scale of cybersex trafficking necessitate a united response by the nations, corporations, and organizations of the world to reduce incidents of the crime; protect, rescue, and rehabilitate victims; and arrest and prosecute the perpetrators. Some governments have initiated advocacy and media campaigns that focus on awareness of the crime. They have also implemented training seminars held to teach law enforcement, prosecutors, and other authorities, as well as NGO workers, to combat the crime and provide trauma-informed aftercare service. New legislation combating cybersex trafficking is needed in the twenty-first century.",2
Technology,Dancing pigs,"In computer security, ""dancing pigs"" is a term or problem that explains computer users' attitudes towards computer security. It states that users will continue to pick an amusing graphic even if they receive a warning from security software that it is potentially dangerous. In other words, users choose their primary desire features without considering the security. ""Dancing pigs"" is generally used by tech experts and can be found in IT articles.",2
Technology,Data commingling,"Data commingling, in computer science, occurs when different items or kinds of data are stored in such a way that they become commonly accessible when they are supposed to remain separated. In cloud computing, this can occur where different customer data sits on the same server. Data that is commingled can present a security vulnerability.Data commingling can also occur due to high speed data transmission mixing. In this situation, data of one security level can inadvertently or purposely be mixed with data of a lower or higher security level on the same transmission portal. Portal vehicles can be wire, fiber optics, microwave or various radio frequency transmission portals. This commingling can cause breaches of security and become a source of legal issues to any entity, corporation or individual.
Data commingling can also occur when personal computers and personal software programs are used for business, security, government, etc. uses. In the early formulation stages of entities, non-profit or profit corporations, LLC's, LLP's, etc., the creation and use of stand-alone computers and stand-alone networks, ""absolutely unconnected"" to involved individuals, is the easiest and safest way to prevent Data Commingling.",2
Technology,Data remanence,"Data remanence is the residual representation of digital data that remains even after attempts have been made to remove or erase the data.  This residue may result from data being left intact by a nominal file deletion operation, by reformatting of storage media that does not remove data previously written to the media, or through physical properties of the storage media that allow previously written data to be recovered.  Data remanence may make inadvertent disclosure of sensitive information possible should the storage media be released into an uncontrolled environment (e.g., thrown in the bin (trash) or lost). 
Various techniques have been developed to counter data remanence.  These techniques are classified as clearing,  purging/sanitizing, or destruction.  Specific methods include overwriting, degaussing, encryption, and media destruction.
Effective application of countermeasures can be complicated by several factors, including media that are inaccessible, media that cannot effectively be erased, advanced storage systems that maintain histories of data throughout the data's life cycle, and persistence of data in memory that is typically considered volatile.
Several standards exist for the secure removal of data and the elimination of data remanence.",2
Technology,Data-centric security,"Data-centric security is an approach to security that emphasizes the dependability of the data itself rather than the security of networks, servers, or applications. Data-centric security is evolving rapidly as enterprises increasingly rely on digital information to run their business and big data projects become mainstream.
Data-centric security also allows organizations to overcome the disconnect between IT security technology and the objectives of business strategy by relating security services directly to the data they implicitly protect; a relationship that is often obscured by the presentation of security as an end in itself.",2
Technology,Deception technology,"Deception technology is a category of cyber security defense. Deception technology products can detect, analyze, and defend against zero-day and advanced attacks, often in real time. They are automated, accurate, and provide insight into malicious activity within internal networks which may be unseen by other types of cyber defense. Deception technology enables a more proactive security posture by seeking to deceive the attackers, detect them and then defeat them, allowing the enterprise to return to normal operations.
Existing defense-in-depth cyber technologies have struggled against the increasing wave of sophisticated and persistent human attackers. These technologies seek primarily to defend a perimeter, but both firewalls and endpoint security cannot defend a perimeter with 100% certainty. Cyber-attackers can penetrate these networks and move unimpeded for months, stealing data and intellectual property. Heuristics may find an attacker within the network, but often generate so many alerts that critical alerts are missed. Since 2014, attacks have accelerated and there is evidence that cyber-attackers are penetrating traditional defenses at a rapidly increasing rate.
Deception technology considers the human attacker's point of view and method for exploiting and navigating networks to identify and exfiltrate data. It integrates with existing technologies to provide new visibility into the internal networks, share high probability alerts and threat intelligence with the existing infrastructure.

",2
Technology,Defense strategy (computing),"In computing, defense strategy is a concept and practice used by computer designers, users, and IT personnel to reduce computer security risks.",2
Technology,Defensive computing,"Defensive computing is a form of practice for computer users to help reduce the risk of computing problems, by avoiding dangerous computing practices. The primary goal of this method of computing is to be able to anticipate and prepare for potentially problematic situations prior to their occurrence, despite any adverse conditions of a computer system or any mistakes made by other users. This can be achieved through adherence to a variety of general guidelines, as well as the practice of specific computing techniques.
Strategies for defensive computing could be divided into two categories, network security and the backup and restoration of data.",2
Technology,Democratic Congressional Campaign Committee cyber attacks,"On Friday July 29, 2016 the Democratic Congressional Campaign Committee reported that its computer systems had been infiltrated. It is strongly believed by US intelligence sources that the infiltrator groups are Russian foreign intelligence groups that breached the Democratic National Committee's computer systems. These groups are known as Fancy Bear and Cozy Bear (or ""Sofacy"").CrowdStrike assisted with efforts to deal with the DCCC breach. There was significant concern that the Russian Government was attempting to influence the 2016 Presidential campaign. Russian cyber intrusions into United States government and private sector computer systems significantly increased after the U.S, imposed sanctions on Russia after its invasion of the Crimea in Ukraine. It was President Obama's preference to publicize cyber attacks.",2
Technology,Democratic National Committee cyber attacks,"The Democratic National Committee cyber attacks took place in 2015 and 2016, in which two groups of Russian computer hackers infiltrated the Democratic National Committee (DNC) computer network, leading to a data breach. Cybersecurity experts, as well as the U.S. government, determined that the cyberespionage was the work of Russian intelligence agencies.
Forensic evidence analyzed by several cybersecurity firms, CrowdStrike, Fidelis, and Mandiant (or FireEye), strongly indicates that two Russian intelligence agencies separately infiltrated the DNC computer systems. The American cybersecurity firm CrowdStrike, which removed the hacking programs, revealed a history of encounters with both groups and had already named them, calling one of  them Cozy Bear and the other Fancy Bear, names which are used in the media.On December 9, 2016, the CIA told U.S. legislators the U.S. Intelligence Community concluded Russia conducted the cyberattacks and other operations during the 2016 U.S. election to assist Donald Trump in winning the presidency. Multiple U.S. intelligence agencies concluded that specific individuals tied to the Russian government provided WikiLeaks with the stolen emails from the DNC, as well as stolen emails from Hillary Clinton's campaign chairman, who was also the target of a cyberattack. These intelligence organizations additionally concluded Russia hacked the Republican National Committee (R.N.C.) as well as the D.N.C., but chose not to leak information obtained from the R.N.C.

",2
Technology,Digital self-defense,"Digital self-defense is the use of self-defense strategies by Internet users to ensure digital security; that is to say, the protection of confidential personal electronic information.  Internet security software provides initial protection by setting up a firewall, as well as scanning computers for malware, viruses, Trojan horses, worms and spyware.  However information at most risk includes personal details such as birthdates, phone numbers, bank account, schooling details, sexuality, religious affiliations, email addresses and passwords.  This information is often openly revealed in social networking sites, leaving Internet users vulnerable to social engineering and possibly Internet crime. Mobile devices, especially those with Wi-Fi, allow this information to be shared inadvertently.Digital self-defense requires Internet users to take an active part in guarding their own personal information. Four key strategies are frequently suggested to assist that protection.

",2
Technology,Dolev–Yao model,"The Dolev–Yao model, named after its authors Danny Dolev and Andrew Yao, is a formal model used to prove properties of interactive  cryptographic protocols.",2
Technology,Domain fronting,"Domain fronting is a technique for Internet censorship circumvention that uses different domain names in different communication layers of an HTTPS connection to discreetly connect to a different target domain than is discernable to third parties monitoring the requests and connections.
Due to quirks in security certificates, the redirect systems of the content delivery networks (CDNs) used as 'domain fronts', and the protection provided by HTTPS, censors are typically unable to differentiate circumvention (""domain-fronted"") traffic from overt non-fronted traffic for any given domain name. As such they are forced to either allow all traffic to the domain front—including circumvention traffic—or block the domain front entirely, which may result in expensive collateral damage and has been likened to ""blocking the rest of the Internet"".Domain fronting does not conform to HTTP standards that require the SNI extension and HTTP Host header to contain the same domain. Large cloud service providers, including Amazon and Google, now actively prohibit domain fronting, which has made it ""largely non-viable"" as a censorship bypass technique.

",2
Technology,DREAD (risk assessment model),"DREAD is part of a system for risk-assessing computer security threats previously used at Microsoft, it was abandoned by its creators.  It provides a mnemonic for risk rating security threats using five categories.
The categories are:

Damage – how bad would an attack be?
Reproducibility – how easy is it to reproduce the attack?
Exploitability – how much work is it to launch the attack?
Affected users – how many people will be impacted?
Discoverability – how easy is it to discover the threat?The DREAD name comes from the initials of the five categories listed. It was initially proposed for threat modeling, but it was discovered that the ratings are not very consistent and are subject to debate.  It was out of use at Microsoft by 2008.When a given threat is assessed using DREAD, each category is given a rating from 1 to 10. The sum of all ratings for a given issue can be used to prioritize among different issues.",2
Technology,EasyJet data breach,The EasyJet data breach was a cyberattack on the computer systems of British airline EasyJet.,2
Technology,Economics of security,"The economics of information security addresses the economic aspects of privacy and computer security. Economics of information security includes models of the strictly rational “homo economicus” as well as behavioral economics. Economics of security addresses individual and organizational decisions and behaviors with respect to security and privacy as market decisions.
Economics of security addresses a core question: why do agents choose technical risks when there exists technical solutions to mitigate security and privacy risks?  Economics addresses not only this question, but also inform design decisions in security engineering.",2
Technology,Electric grid security,"Electric grid security in the US refer to the activities that utilities, regulators, and other stakeholders play in securing the national electricity grid. The American electrical grid is going through one of the largest changes in its history, which is the move to smart grid technology. The smart grid allows energy customers and energy providers to more efficiently manage and generate electricity. Similar to other new technologies, the smart grid also introduces new concerns about security.Utility owners and operators (whether investor-owned, municipal, or cooperative) typically are responsible for implementing system improvements with regards to cybersecurity. Executives in the utilities industry are beginning to recognize the business impact of cybersecurity.The electric utility industry in the U.S. leads a number of initiatives to help protect the national electric grid from threats. The industry partners with the federal government, particularly the National Institute of Standards and Technology, the North American Electric Reliability Corporation, and federal intelligence and law enforcement agencies.Electric grids can be targets of military or terrorist activity. When American military leaders created their first air war plan against the Axis in 1941, Germany's electric grid was at the top of the target list.

",2
Technology,Enrollment over Secure Transport,"The Enrollment over Secure Transport, or EST is a cryptographic protocol that describes an X.509 certificate management protocol targeting public key infrastructure (PKI) clients that need to acquire client certificates and associated certificate authority (CA) certificates. EST is described in RFC 7030. EST has been put forward as a replacement for SCEP, being easier to implement on devices already having an HTTPS stack. EST uses HTTPS as transport and leverages TLS for many of its security attributes. EST has described standardized URLs and uses the well-known Uniform Resource Identifiers (URIs) definition codified in RFC 5785.",2
Technology,Enterprise information security architecture,"Enterprise information security architecture (ZBI) is a part of enterprise architecture focusing on information security throughout the enterprise.  The name implies a difference that may not exist between small/medium-sized businesses and larger organizations.

",2
Technology,ERP security,"ERP Security is a wide range of measures aimed at protecting Enterprise resource planning (ERP) systems from illicit access ensuring accessibility and integrity of system data. ERP system is a computer software that serves to unify the information intended to manage the organization including Production, Supply Chain Management, Financial Management, Human Resource Management, Customer Relationship Management, Enterprise Performance Management. Common ERP systems are SAP, Oracle E-Business Suite, Microsoft Dynamics, Priority Software.

",2
Technology,Fabric of Security,"The Fabric of Security, also known as Cyber Security Fabric or Federated Security, refers to systems designed to protect the Information Systems infrastructure of the home, a corporation or government from malicious attackers. Protection in this sense means guaranteeing the confidentiality, integrity, and the availability of the information stored in the system (""SYSTEM""), and its elements or components.
Unlike endpoint security, network security, web application security, and Internet security, Fabric of Security Systems assume that attacks will be successful, and cannot be averted. Therefore, the emphasis shifts from attempting to prevent unauthorized access to that of minimizing the time to detect the unauthorized access, the time to isolate the unauthorized access from doing harm, and finally, the time to remove the offending process and reconfiguration of the system back into a ""SAFE"" state.",2
Technology,Fail-stop,"A fail-stop subset of a computer language is one that has the same semantics as the original, except in the case where an exceptional condition arises.  The fail-stop subset must report an exceptional condition whenever the superset language reports one, but may additionally report an exceptional condition in other cases.
Fail-stop languages are often used in computer systems where correctness is very important, since it is easier to make such systems fail-fast.  For example, the ""+"" operator in many programming languages is not associative because of the possibility of floating-point overflow.  Repairing these languages to fail fast when commonly assumed properties do not hold makes it much easier to write and verify correct code.",2
Technology,Federal Desktop Core Configuration,"The Federal Desktop Core Configuration is a list of security settings recommended by the National Institute of Standards and Technology for general-purpose microcomputers that are connected directly to the network of a United States government agency.
The FDCC is a list of agreed upon Microsoft Windows operating system common core system functions, applications, files, and services that are changed in their configuration around which a framework for a more secure, and security-reliable MS Windows operating system was created. The standards were then made mandatory for every federal government computer effective Feb 1, 2008. If you wanted to connect to a federal office computer network your system had to meet or exceed the FDCC standard or you were denied access.
FDCC applied only to Windows XP and Vista desktop and laptop computers and was replaced by the United States Government Configuration Baseline (USGCB), which included settings for Windows 7 and Red Hat Enterprise Linux 5.
For Windows 7, the NIST changed the naming convention to the US Government Computer Baseline (USGCB ver 2.0). In addition to un-classifying a general Windows settings guide, the NIST also publishes guides specifically for Windows Firewall, Internet Explorer, and a guide (Vista-Energy, for example) created to capture settings that adhere to energy conservation policies.",2
Technology,Footprinting,"Footprinting (also known as reconnaissance) is the technique used for gathering information about computer systems and the entities they belong to. To get this information, a hacker might use various tools and technologies. This information is very useful to a hacker who is trying to crack a whole system.When used in the computer security lexicon, ""Footprinting"" generally refers to one of the pre-attack phases; tasks performed before doing the actual attack.  Some of the tools used for Footprinting are Sam Spade, nslookup, traceroute, Nmap and neotrace.

",2
Technology,Four Horsemen of the Infocalypse,"The Four Horsemen of the Infocalypse refers to those who use the internet to facilitate crime, or (pejoratively) to rhetorical approaches evoking such criminals.
The phrase is a play on Four Horsemen of the Apocalypse. There does not appear to be
a universally agreed definition of who
the Horsemen are, but they are usually listed as terrorists, drug dealers, pedophiles/child molesters, and organized crime.   One of the most
famous definitions is in the Cypherpunk FAQ, which states:

8.3.4. ""How will privacy and anonymity be attacked?""
[...]
like so many other ""computer hacker"" items, as a tool for the ""Four Horsemen"": drug-dealers, money-launderers,  terrorists, and pedophiles.17.5.7. ""What limits on the Net are being proposed?""
[...]
Newspapers are complaining about the Four Horsemen of the Infocalypse:
terrorists, pedophiles, drug dealers, and money launderers
Other sources use slightly different descriptions but generally refer to
similar activities.",2
Technology,Gifar,"Graphics Interchange Format Java Archives (GIFAR) is a term for GIF files combined with the JAR file format. GIFARs could be uploaded to Web sites that allow image uploading, and then run as though they were part of the legitimate code of that site. Java was patched in JRE 6 Update 11, with a CVE published on December 4 2008.In this attack, GIF Java archive files (GIFARs) were uploaded to Web sites on the understanding that they are GIFs, and the file was then interpreted as a JAR file when viewed and executed. This circumvented the same-origin policy that browsers impose; bypassing the content validation usually used. Attackers reference this malicious image in the applet code on the hosted site, establishing cross-domain communication with the (your) target domain.
This technique worked because GIF images store their header in the beginning of the file, and JAR files (as with any ZIP archive-based format) store their data at the tail. This attack is not unique to GIFs and JARs; there is a general class of vulnerabilities of file type combinations such as .doc, .jpg, etc.
A GIFAR allowed an attacker to access the victim's HTTP cookies. This allows session hijacking, where the victim's logged-in user accounts can be accessed.
GIFARs should not have been executed if the user is viewing the image; it had to be interpreted as a JAR not a GIF to run.
For the attack to work, the victim must be logged into the Web site that is hosting the image.
Any site that includes login sessions with user-uploaded pictures can be vulnerable.",2
Technology,Security hacker,"A security hacker is someone who explores methods for breaching defenses and exploiting weaknesses in a computer system or network. Hackers may be motivated by a multitude of reasons, such as profit, protest, information gathering, challenge, recreation, or evaluation of a system weaknesses to assist in formulating defenses against potential hackers. The subculture that has evolved around hackers is often referred to as the ""computer underground"".Longstanding controversy surrounds the meaning of the term ""hacker"". In this controversy, computer programmers reclaim the term hacker, arguing that it refers simply to someone with an advanced understanding of computers and computer networks and that cracker is the more appropriate term for those who break into computers, whether computer criminals (black hats) or computer security experts (white hats). A 2014 article noted that ""the black-hat meaning still prevails among the general public"".",2
Technology,Hacker Bible,"The Hacker Bible is a publication of the German hacker organization Chaos Computer Club (CCC). It has been published in two editions to date, 1985 and 1988. Both were edited by Wau Holland and published on the Grüne Kraft press.
The Hacker Bible is a compendium of documents and stories from the hacker scene, for example the instruction guide to the acoustic coupler named “Data-loo”(Germ.:Datenklo). Furthermore, it offers manuals and other technical explanations. The first edition appeared in 1985 with the subtitle “Cable salad is good for you"" (""Kabelsalat ist gesund”) and had sold 25,000 copies by mid-1988. The second edition in 1988 was given the additional name “The New Testament”. The comic images on the cover sleeve are a creation of German comic artists Mali Beinhorn and Werner Büsch from the comic workshop Büsch-Beinhorn. The production and distribution of the Hacker Bible was discontinued by 1990. Since 1999, the CCC has offered a scanned and full-text version online (in German) with further materials such as texts from Peter Glaser, a documentation on Karl Koch and works from Tron from the Chaos-CD.

",2
Technology,Hardware security,"A hardware security module (HSM) is a physical computing device that safeguards and manages digital keys, performs encryption and decryption functions for digital signatures, strong authentication and other cryptographic functions. These modules traditionally come in the form of a plug-in card or an external device that attaches directly to a computer or network server. A hardware security module contains one or more secure cryptoprocessor chips.",2
Technology,HDIV,"The Human Development Index (HDI) is a statistic composite index of life expectancy, education (mean years of schooling completed and expected years of schooling upon entering the education system), and per capita income indicators, which are used to rank countries into four tiers of human development. A country scores a higher level of HDI when the lifespan is higher, the education level is higher, and the gross national income GNI (PPP) per capita is higher. It was developed by  Pakistani economist Mahbub ul Haq and was further used to measure a country's development by the United Nations Development Programme (UNDP)'s Human Development Report Office.The 2010 Human Development Report introduced an Inequality-adjusted Human Development Index (IHDI). While the simple HDI remains useful, it stated that ""the IHDI is the actual level of human development (accounting for inequality), while the HDI can be viewed as an index of 'potential' human development (or the maximum level of HDI) that could be achieved if there were no inequality.""The index is based on the human development approach, developed by Mahbub ul Haq, anchored in Amartya Sen's work on human capabilities, often framed in terms of whether people are able to ""be"" and ""do"" desirable things in life. Examples include – being: well fed, sheltered, healthy; doing: work, education, voting, participating in community life. The freedom of choice is central – someone choosing to be hungry (e.g. when fasting for religious reasons) is quite different from someone who is hungry because they cannot afford to buy food, or because the country is in a famine.The index does not take into account several factors, such as the net wealth per capita or the relative quality of goods in a country. This situation tends to lower the ranking for some of the most advanced countries, such as the G7 members and others.",2
Technology,HEAT LANrev,"HEAT LANrev (formerly Absolute Manage) is systems lifecycle management software used by system administrators to automate IT administration tasks. The product includes server and client (""agent"") software that runs on Windows and macOS.

",2
Technology,High Assurance Guard,"A High Assurance Guard (HAG) is a Multilevel security computer device which is used to communicate between different Security Domains, such as NIPRNet to SIPRNet. A HAG is one example of a Controlled Interface between security levels. HAGs are approved through the Common Criteria process.

",2
Technology,Homeland Open Security Technology,"Homeland Open Security Technology (HOST) is a five-year, $10 million program by the Department of Homeland Security's Science and Technology Directorate to promote the creation and use of open security and open-source software in the United States government and military, especially in areas pertaining to computer security.Proponent David A. Wheeler claims that open-source security could also extend to hardware and written documents. In October 2011, the project won the Open Source for America 2011 Government Deployment Open Source Award.",2
Technology,Host Based Security System,"The Host Based Security System (HBSS) is the official name given to the United States Department of Defense (DOD) commercial off-the-shelf (COTS) suite of software applications used within the DOD to monitor, detect, and defend the DOD computer networks and systems.  The Enterprise-wide Information Assurance and computer Network Defense Solutions Steering Group (ESSG) sponsored the acquisition of the HBSS System for use within the DOD Enterprise Network.  HBSS is deployed on both the Non-Classified Internet Protocol Routed Network (NIPRNet) and Secret Internet Protocol Routed Network (SIPRNet) networks, with priority given to installing it on the NIPRNet.  HBSS is based on McAfee, Inc's ePolicy Orchestrator (ePO) and other McAfee point product security applications such as Host Intrusion Prevention System (HIPS).

",2
Technology,HTTP tunnel,"HTTP tunneling is used to create a network link between two computers in conditions of restricted network connectivity including firewalls, NATs and ACLs, among other restrictions. The tunnel is created by an intermediary called a proxy server which is usually located in a DMZ.
Tunneling can also allow communication using a protocol that normally wouldn’t be supported on the restricted network.",2
Technology,Human–computer interaction (security),"HCISec is the study of interaction between humans and computers, or human–computer interaction, specifically as it pertains to information security.  Its aim, in plain terms, is to improve the usability of security features in end user applications.  
Unlike HCI, which has roots in the early days of Xerox PARC during the 1970s, HCISec is a nascent field of study by comparison. Interest in this topic tracks with that of Internet security, which has become an area of broad public concern only in very recent years.
When security features exhibit poor usability, the following are common reasons:

they were added in casual afterthought
they were hastily patched in to address newly discovered security bugs
they address very complex use cases without the benefit of a software wizard
their interface designers lacked understanding of related security concepts
their interface designers were not usability experts (often meaning they were the application developers themselves)",2
Technology,Information Exchange Gateway,"NATO has defined the concept of an Information Exchange Gateway (IEG) to facilitate secure communication between different security and management domains. The IEG is designed to provide a standard and secure method of communication between NATO, NATO nations, non-NATO nations, coalition forces, Non Government Organisations (NGOs), and other International Organisations (IOs).An Information Exchange Gateway provides Information Exchange Services (IES) to facilitate the exchange of information between networks, including data and protocol translation where necessary, and Information Protection Services (IPS) that ensure only intended information is exchanged. In addition, Node Protection Services (NPS) ensure information is exchanged in a safe and secure way and that only intended information which has been validated by the IPS can be exchanged.NATO has defined five main IEG scenarios each with scenario variants for transferring classified information. The scenarios take account of the security classifications of the domains that they connect, as well as the security policy, the owners and the administrators of those domains.
In some scenarios, an IEG must include a guard to provide the IPS and NPS functions. When used with a guard, the IEG's DMZ is split into two.",2
Technology,Information Security Automation Program,"The Information Security Automation Program (ISAP, pronounced “I Sap”) is a U.S. government multi-agency initiative to enable automation and standardization of technical security operations. While a U.S. government initiative, its standards based design can benefit all information technology security operations. The ISAP high level goals include standards based automation of security checking and remediation as well as automation of technical compliance activities (e.g. FISMA). ISAP's low level objectives include enabling standards based communication of vulnerability data, customizing and managing configuration baselines for various IT products, assessing information systems and reporting compliance status, using standard metrics to weight and aggregate potential vulnerability impact, and remediating identified vulnerabilities.
ISAP's technical specifications are contained in the related Security Content Automation Protocol (SCAP). ISAP's security automation content is either contained within, or referenced by, the National Vulnerability Database. 
ISAP is being formalized through a trilateral memorandum of agreement (MOA) between Defense Information Systems Agency (DISA), the National Security Agency (NSA), and the National Institute of Standards and Technology (NIST).  The Office of the Secretary of Defense (OSD) also participates and the Department of Homeland Security (DHS) funds the operation infrastructure on which ISAP relies (i.e., the National Vulnerability Database).",2
Technology,Information security operations center,"An information security operations center (ISOC or SOC) is a facility where enterprise information systems (web sites, applications, databases, data centers and servers, networks, desktops and other endpoints) are monitored, assessed, and defended.

",2
Technology,Insider threat,"An insider threat is a malicious threat to an organization that comes from people within the organization, such as employees, former employees, contractors or business associates, who have inside information concerning the organization's security practices, data and computer systems. The threat may involve fraud, the theft of confidential or commercially valuable information, the theft of intellectual property, or the sabotage of computer systems. The insider threat comes in three categories:

Malicious insiders, which are people who take advantage of their access to inflict harm on an organization;
Negligent insiders, which are people who make errors and disregard policies, which place their organizations at risk; and
Infiltrators, who are external actors that obtain legitimate access credentials without authorization.

",2
Technology,Intel Management Engine,"The Intel Management Engine (ME), also known as the Intel Manageability Engine, is an autonomous subsystem that has been incorporated in virtually all of Intel's processor chipsets since 2008. It is located in the Platform Controller Hub of modern Intel motherboards.
The Intel Management Engine always runs as long as the motherboard is receiving power, even when the computer is turned off. This issue can be mitigated with deployment of a hardware device, which is able to disconnect mains power.
The Intel ME is an attractive target for hackers, since it has top level access to all devices and completely bypasses the operating system. The Electronic Frontier Foundation has voiced concern about Intel ME and some security researchers have voiced concern that it is a backdoor.
Intel's main competitor AMD has incorporated the equivalent AMD Secure Technology (formally called Platform Security Processor) in virtually all of its post-2013 CPUs.

",2
Technology,Internet Security Awareness Training,"Internet Security Awareness Training (ISAT) is the training given to members of an organization regarding the protection of various information assets of that organization. ISAT is a subset of general security awareness training (SAT).
Even small and medium enterprises are generally recommended to provide such training, but organizations that need to comply with government regulations (e.g., the Gramm–Leach–Bliley Act, the Payment Card Industry Data Security Standard, Health Insurance Portability and Accountability Act, Sarbox) normally require formal ISAT for annually for all employees. Often such training is provided in the form of online courses.

ISAT, also referred to as Security Education, Training, and Awareness (SETA), organizations train and create awareness of information security management within their environment. It is beneficial to organizations when employees are well trained and feel empowered to take important actions to protect themselves and organizational data. The SETA program target must be based on user roles within organizations and for positions that expose the organizations to increased risk levels, specialized courses must be required.",2
Technology,Intrusion tolerance,"Intrusion tolerance is a fault-tolerant design approach to defending information systems against malicious attacks. In that sense, it is also a computer security approach. Abandoning the conventional aim of preventing all intrusions, intrusion tolerance instead calls for triggering mechanisms that prevent intrusions from leading to a system security failure. There are two major variants of intrusion tolerance mechanisms: mechanisms based on redundancy (e.g., as in Byzantine fault tolerance); mechanisms based on intrusion detection (e.g., with an intrusion detection system) and reaction.",2
Technology,IT baseline protection,"The IT baseline protection (German: IT-Grundschutz) approach from the German Federal Office for Information Security (BSI) is a methodology to identify and implement computer security measures in an organization. The aim is the achievement of an adequate and appropriate level of security for IT systems. To reach this goal the BSI recommends ""well-proven technical, organizational, personnel, and infrastructural safeguards"". Organizations and federal agencies show their systematic approach to secure their IT systems (e.g. Information Security Management System) by obtaining an ISO/IEC 27001 Certificate on the basis of IT-Grundschutz.

",2
Technology,ITIL security management,"ITIL security management describes the structured fitting of security into an organization. ITIL security management is based on the ISO 27001 standard. ""ISO/IEC 27001:2005 covers all types of organizations (e.g. commercial enterprises, government agencies, not-for profit organizations). ISO/IEC 27001:2005 specifies the requirements for establishing, implementing, operating, monitoring, reviewing, maintaining and improving a documented Information Security Management System within the context of the organization's overall business risks. It specifies requirements for the implementation of security controls customized to the needs of individual organizations or parts thereof. ISO/IEC 27001:2005 is designed to ensure the selection of adequate and proportionate security controls that protect information assets and give confidence to interested parties.""
A basic concept of security management is information security. The primary goal of information security is to control access to information. The value of the information is what must be protected. These values include confidentiality, integrity and availability. Inferred aspects are privacy, anonymity and verifiability.
The goal of security management comes in two parts:

Security requirements defined in service level agreements (SLA) and other external requirements that are specified in underpinning contracts, legislation and possible internal or external imposed policies.
Basic security that guarantees management continuity. This is necessary to achieve simplified service-level management for information security.SLAs define security requirements, along with legislation (if applicable) and other contracts. These requirements can act as key performance indicators (KPIs) that can be used for process management and for interpreting the results of the security management process.
The security management process relates to other ITIL-processes. However, in this particular section the most obvious relations are the relations to the service level management, incident management and change management processes.

",2
Technology,Kill Pill,"In computing, kill pill is a term given to mechanisms and technologies designed to render systems useless either by user command, or under a predefined set of circumstances. Kill pill technology is most commonly used to disable lost or stolen devices for security purposes, but can also be used for the enforcement of rules and contractual obligations.",2
Technology,Krebs on Security,"Brian Krebs (born 1972) is an American journalist and investigative reporter. He is best known for his coverage of profit-seeking cybercriminals. Krebs is the author of a daily blog, KrebsOnSecurity.com,  covering computer security and cybercrime. From 1995 to 2009, Krebs was a reporter for The Washington Post and covered tech policy, privacy and computer security as well as authoring the Security Fix blog. He is also known for interviewing hacker 0x80 and doxxing security researcher.",2
Technology,Language-based security,"In computer science, language-based security (LBS) is a set of techniques that may be used to strengthen the security of applications on a high level by using the properties of programming languages.
LBS is considered to enforce computer security on an application-level, making it possible to prevent vulnerabilities which traditional operating system security is unable to handle.
Software applications are typically specified and implemented in certain programming languages, and in order to protect against attacks, flaws and bugs an application's source code might be vulnerable to, there is a need for application-level security; security evaluating the applications behavior with respect to the programming language. This area is generally known as language-based security.",2
Technology,Linked timestamping,Linked timestamping is a type of trusted timestamping where issued time-stamps are related to each other.,2
Technology,List of security assessment tools,This is a list of available software and hardware tools that are designed for or are particularly suited to various kinds of security assessment and security testing.,2
Technology,List of security hacking incidents,The list of security hacking incidents covers important or noteworthy events in the history of security hacking and cracking.,2
Technology,Macro and security,"A macro in computer science is a rule or pattern that specifies how a certain input sequence (often a sequence of characters) should be mapped to a replacement input sequence (also often a sequence of characters) according to a defined procedure.
A macro is used to define variables or procedures, to allow code reuse, or to design domain-specific languages.
Macros can be separated into several types: 

Text substitution macros as in the C language.
Macros in software. In some software, a sequence of instructions can be associated to a keyboard or mouse action. Some software can include a programming language (like VBA in Microsoft Office) allowing the control of software features.
Other types of macros that are not covered in this article.Macros can be very useful to software users. They simplify regularly used actions (repetitive code for a programmer, or a sequence of actions in a program) so that the productivity of the user is increased. However, many problems exist, they will be tackled subsequently.",2
Technology,Michigan Cyber Range,"The Michigan Cyber Range was established by Merit Network in the summer of 2012 to teach cybersecurity certification courses, provide cybersecurity-related services, and develop a virtual cybersecurity training environment, known as Alphaville. Merit Network staffs and operates the Michigan Cyber Range in Ann Arbor, Michigan. The Range's infrastructure contains virtual servers placed on Merit's fiber-optic network, the largest fiber-optic network in Michigan. The harnessing of Merit’s fiber-optic network allows the Michigan Cyber Range to scale the virtual environment (Alphaville) to an unprecedented size. The Michigan Cyber Range currently has five physical locations in Michigan, on the campuses of Northern Michigan University in Marquette, Michigan, Wayne State University in Warren, Pinckney High School in Pinckney, WMCAT in Grand Rapids, and University of Michigan in Flint. On the Range, exercises such as a vast Capture the Flag with nearly 500 challenges, Paintball where teams of 5 via over control of Alphaville, and Cyber Sentinel where incident response plans and skills can be tested. These exercises are coupled with cyber security educational classes, workshops, and private secure sandboxes to round out the Michigan Cyber Range’s service offerings.
Michigan Cyber Range partners include Merit Network, the U.S. Department of Homeland Security, U.S. Department of Energy, National Institute of Standards and Technology, DTE Energy, Consumers Energy, Plante and Moran PLLC, Juniper Networks, Eastern Michigan University, Michigan State Police, Michigan's Department of Military and Veterans Affairs, Michigan Economic Development Corporation, and the Michigan Department of Technology, Management and Budget.",2
Technology,MinID,"MinID is an electronic login system used to secure a range of internet services in the Norwegian public sector. The communication done with MinID is encrypted to secure information from unauthorized usage. Everyone registered in the Norwegian Population Register over the age of 13 years can create a public ID with MinID.
As of April 2010, more than 2 million people living in Norway had created user accounts with MinID. To create a public ID, PIN-codes from the Norwegian Tax Administration are needed.",2
Technology,Model-driven security,Model-driven security (MDS) means applying model-driven approaches (and especially the concepts behind model-driven software development)  to security.,2
Technology,National Collegiate Cyber Defense Competition,"The National Collegiate Cyber Defense Competition (NCCDC) is the championship event for the Collegiate Cyber Defense Competition system – the largest college-level cyber defense competition in the USA. The event is held annually in the San Antonio area.
In an effort to help facilitate the development of a regular, national level cyber security exercise, the Center for Infrastructure Assurance and Security at the University of Texas at San Antonio (UTSA) hosted the first Collegiate Cyber Defense Competition for the Southwestern region in May 2005.  On June 29, 2010, United States House legislature passed recognizing the National CCDC for promoting cyber security curriculum.While similar to other cyber defense competitions in many aspects, the NCCDC, is unique in that it focuses on the operational aspect of managing and protecting an existing network infrastructure. While other exercises examine the abilities of a group of students to design, configure, and protect a network over the course of an entire semester, this competition is focused on the more operational task of assuming administrative and protective duties for an existing commercial network. Teams are assessed based on their ability to detect and respond to outside threats, maintain availability of existing services such as mail servers and web servers, respond to business requests such as the addition or removal of additional services, and balance security needs against business needs.",2
Technology,National Cyber Security Awareness Month,"National Cyber Security Awareness Month (NCSAM) is observed in October in the United States of America. Started by the National Cyber Security Division within the Department of Homeland Security and the nonprofit National Cyber Security Alliance, the month raises awareness about the importance of cybersecurity.",2
Technology,National Cyber Security Policy 2013,"National Cyber Security Policy is a policy framework by Department of Electronics and Information Technology (DeitY) It aims at protecting the public and private infrastructure from cyber attacks. The policy also intends to safeguard ""information, such as personal information (of web users), financial and banking information and sovereign data"". This was particularly relevant in the wake of US National Security Agency (NSA) leaks that suggested the US government agencies are spying on Indian users, who have no legal or technical safeguards against it. Ministry of Communications and Information Technology (India) defines Cyberspace as a complex environment consisting of interactions between people, software services supported by worldwide distribution of information and communication technology.",2
Technology,Network eavesdropping,"Network eavesdropping, also known as eavesdropping attack, sniffing attack, or snooping attack, is a method that retrieves user information through the internet. This attack happens on electronic devices like computers and smartphones. This network attack typically happens under the usage of unsecured networks, such as public wifi connections or shared electronic devices. Eavesdropping attacks through the network is considered one of the most urgent threats in industries that rely on collecting and storing data.A typical network eavesdropper may be called a Black-hat hacker and is considered a low-level hacker as it is simple to network eavesdrop successfully. The threat of network eavesdroppers is a growing concern. Research and discussions are brought up in the public's eye, for instance, types of eavesdropping, open-source tools, and commercial tools to prevent eavesdropping. Models against network eavesdropping attempts are built and developed as privacy is increasingly valued. Sections on cases of successful network eavesdropping attempts and its laws and policies in the National Security Agency are mentioned. Some laws include the Electronic Communications Privacy Act and the Foreign Intelligence Surveillance Act.",2
Technology,Nobody (username),"In many Unix variants, ""nobody"" is the conventional name of a user identifier which owns no files, is in no privileged groups, and has no abilities except those which every other user has. It is normally not enabled as a user account, i.e. has no home directory or login credentials assigned. Some systems also define an equivalent group ""nogroup"".

",2
Technology,Opal Storage Specification,"The Opal Storage Specification is a set of specifications for features of data storage devices (such as disk drives) that enhance their security.  For example, it defines a way of encrypting the stored data so that an unauthorized person who gains possession of the device cannot see the data.  That is, it is a specification for self-encrypting drives (SED).
The specification is published by the Trusted Computing Group Storage Workgroup.",2
Technology,Open security,"Open security is the use of open source philosophies and methodologies to approach computer security and other information security challenges. Traditional application security is based on the premise that any application or service (whether it is malware or desirable) relies on security through obscurity.Open source approaches have created technology such as Linux (and to some extent, the Android operating system). Additionally, open source approaches applied to documents have inspired wikis and their largest example, Wikipedia. Open security suggests that security breaches and vulnerabilities can be better prevented or ameliorated when users facing these problems collaborate using open source philosophies.This approach requires that users be legally allowed to collaborate, so relevant software would need to be released under a license that is widely accepted to be open source; examples include the Massachusetts Institute of Technology (MIT) license, the Apache 2.0 license, the GNU Lesser General Public License (LGPL), and the GNU General Public License (GPL). Relevant documents would need to be under a generally accepted ""open content"" license; these include Creative Commons Attribution (CC-BY) and Attribution Share Alike (CC-BY-SA) licenses, but not Creative Commons ""non-commercial"" licenses or ""no-derivative"" licenses.On the developer side, legitimate software and service providers can have independent verification and testing of their source code. On the information technology side, companies can aggregate common threats, patterns, and security solutions to a variety of security issues.

",2
Technology,Open-source software security,Open-source software security is the measure of assurance or guarantee in the freedom from danger and risk inherent to an open-source software system.,2
Technology,Open Threat Exchange,"Open Threat Exchange (OTX) is a crowd-sourced computer-security platform. It has more than 180,000 participants in 140 countries who share more than 19 million potential threats daily. It is free to use.Founded in 2012, OTX was created and is run by AlienVault (now AT&T Cybersecurity), a developer of commercial and open source solutions to manage cyber attacks.  The collaborative threat exchange was created partly as a counterweight to criminal hackers successfully working together and sharing information about viruses, malware and other cyber attacks.",2
Technology,Operations security,"Operations security (OPSEC) is a process that identifies critical information to determine if friendly actions can be observed by enemy intelligence, determines if information obtained by adversaries could be interpreted to be useful to them, and then executes selected measures that eliminate or reduce adversary exploitation of friendly critical information.

In a more general sense, OPSEC is the process of protecting individual pieces of data that could be grouped together to give the bigger picture (called aggregation). OPSEC is the protection of critical information deemed mission-essential from military commanders, senior leaders, management or other decision-making bodies. The process results in the development of countermeasures, which include technical and non-technical measures such as the use of email encryption software, taking precautions against eavesdropping, paying close attention to a picture you have taken (such as items in the background), or not talking openly on social media sites about information on the unit, activity or organization's Critical Information List.
The term ""operations security"" was coined by the United States military during the Vietnam War.",2
Technology,Parasitic computing,"Parasitic computing is programming technique where a program in normal authorized interactions with another program manages to get the other program to perform computations of a complex nature. It is, in a sense, a security exploit in that the program implementing the parasitic computing has no authority to consume resources made available to the other program.
It was first proposed by Albert-Laszlo Barabasi, Vincent W. Freeh, Hawoong Jeong & Jay B. Brockman from University of Notre Dame, Indiana, USA, in 2001. The example given by the original paper was two computers communicating over the Internet, under disguise of a standard communications session. The first computer is attempting to solve a large and extremely difficult 3-SAT problem; it has decomposed the original 3-SAT problem in a considerable number of smaller problems. Each of these smaller problems is then encoded as a relation between a checksum and a packet such that whether the checksum is accurate or not is also the answer to that smaller problem. The packet/checksum is then sent to another computer. This computer will, as part of receiving the packet and deciding whether it is valid and well-formed, create a checksum of the packet and see whether it is identical to the provided checksum. If the checksum is invalid, it will then request a new packet from the original computer. The original computer now knows the answer to that smaller problem based on the second computer's response, and can transmit a fresh packet embodying a different sub-problem. Eventually, all the sub-problems will be answered and the final answer easily calculated.
The example is based on an exploit of the Transmission Control Protocol (TCP), used for internet connections, so in the end, the target computer(s) is unaware that it has performed computation for the benefit of the other computer, or even done anything besides have a normal TCP/IP session.
The proof-of-concept is obviously extremely inefficient as the amount of computation necessary to merely send the packets in the first place easily exceeds the computations leeched from the other program; the 3-SAT problem would be solved much more quickly if just analyzed locally. In addition, in practice packets would probably have to be retransmitted occasionally when real checksum errors and network problems occur. However, parasitic computing on the level of checksums is a demonstration of the concept. The authors suggest that as one moves up the application stack, there might come a point where there is a net computational gain to the parasite - perhaps one could break down interesting problems into queries of complex cryptographic protocols using public keys. If there was a net gain, one could in theory use a number of control nodes for which many hosts on the Internet form a distributed computing network completely unawares.
Students of the University of Applied Sciences, Bern, Switzerland, extended this concept into a programmable virtual machine in 2002.",2
Technology,Parkerian Hexad,"The Parkerian hexad is a set of six elements of information security proposed by Donn B. Parker in 1998. The Parkerian hexad adds three additional attributes to the three classic security attributes of the CIA triad (confidentiality, integrity, availability).
The Parkerian Hexad attributes are the following:

Confidentiality
Possession or Control
Integrity
Authenticity
Availability
UtilityThese attributes of information are atomic in that they are not broken down into further constituents; they are non-overlapping in that they refer to unique aspects of information. Any information security breach can be described as affecting one or more of these fundamental attributes of information.

",2
Technology,Physical access,"Physical access is a term in computer security that refers to the ability of people to physically gain access to a computer system. According to Gregory White, ""Given physical access to an office, the knowledgeable attacker will quickly be able to find the information needed to gain access to the organization's computer systems and network.""",2
Technology,Physical information security,"Physical information security is the intersection, the common ground between physical security and information security.  It primarily concerns the protection of tangible information-related assets such as computer systems and storage media against physical, real-world threats such as unauthorized physical access, theft, fire and flood.  It typically involves physical controls such as protective barriers and locks, uninterruptible power supplies, and shredders.  Information security controls in the physical domain complement those in the logical domain (such as encryption), and procedural or administrative controls (such as information security awareness and compliance with policies and laws).",2
Technology,Principal (computer security),"A principal in computer security is an entity that can be authenticated by a computer system or network. It is referred to as a security principal in Java and Microsoft literature.Principals can be individual people, computers, services, computational entities such as processes and threads, or any group of such things. They need to be identified and authenticated before they can be assigned rights and privileges over resources in the network. A principal typically has an associated identifier (such as a security identifier) that allows it to be referenced for identification or assignment of properties and permissions.

",2
Technology,Principle of least privilege,"In information security, computer science, and other fields, the principle of least privilege (PoLP), also known as the principle of minimal privilege or the principle of least authority, requires that in a particular abstraction layer of a computing environment, every module (such as a process, a user, or a program, depending on the subject) must be able to access only the information and resources that are necessary for its legitimate purpose.",2
Technology,PRODIGAL (computer system),"PRODIGAL (proactive discovery of insider threats using graph analysis and learning) is a computer system for predicting anomalous behavior among humans, by data mining network traffic such as emails, text messages and server log entries. It is part of DARPA's Anomaly Detection at Multiple Scales (ADAMS) project. The initial schedule is for two years and the budget $9 million.It uses graph theory, machine learning, statistical anomaly detection, and high-performance computing to scan larger sets of data more quickly than in past systems. The amount of data analyzed is in the range of terabytes per day. The targets of the analysis are employees within the government or defense contracting organizations; specific examples of behavior the system is intended to detect include the actions of Nidal Malik Hasan and WikiLeaks source Chelsea Manning. Commercial applications may include finance. The results of the analysis, the five most serious threats per day, go to agents, analysts, and operators working in counterintelligence.",2
Technology,Proof-carrying code,"Proof-carrying code (PCC) is a software mechanism that allows a host system to verify properties about an application via a formal proof that accompanies the application's executable code.  The host system can quickly verify the validity of the proof, and it can compare the conclusions of the proof to its own security policy to determine whether the application is safe to execute.  This can be particularly useful in ensuring memory safety (i.e. preventing issues like buffer overflows).
Proof-carrying code was originally described in 1996 by George Necula and Peter Lee.",2
Technology,Public computer,"A public computer (or public access computer) is any of various computers available in public areas. Some places where public computers may be available are libraries, schools, or dedicated facilities run by government.
Public computers share similar hardware and software components to personal computers, however, the role and function of a public access computer is entirely different. A public access computer is used by many different untrusted individuals throughout the course of the day. The computer must be locked down and secure against both intentional and unintentional abuse. Users typically do not have authority to install software or change settings. A personal computer, in contrast, is typically used by a single responsible user, who can customize the machine's behavior to their preferences.
Public access computers are often provided with tools such as a PC reservation system to regulate access.
The world's first public access computer center was the Marin Computer Center in California, co-founded by David and Annie Fox in 1977.",2
Technology,Pwnie Awards,The Pwnie Awards recognize both excellence and incompetence in the field of information security. Winners are selected by a committee of security industry professionals from nominations collected from the information security community. The awards are presented yearly at the Black Hat Security Conference.,2
Technology,Relying party,"A relying party (RP) is a computer term used to refer to a server providing access to a secure software application.
Claims-based applications, where a claim is a statement an entity makes about itself in order to establish access, are also called relying party (RP) applications. RPs can also be called “claims aware applications” and “claims-based applications”. Web applications and services can both be RPs.With a Security Token Service (STS), the RP redirects clients to an STS which authenticates the client and issues it a security token containing a set of claims about the client's identity, which it can present to the RP. Instead of the application authenticating the user directly, the RP can extract these claims from the token and use them for identity related tasks.The OpenID standard defines a situation whereby a cooperating site can act as an RP, allowing the user to log into multiple sites using one set of credentials.  The user benefits from not having to share their login credentials with multiple sites, and the operators of the cooperating site avoid having to develop their own login mechanism.An application demonstrating the concept of relying party is software running on mobile devices, which can be used not only for granting user access to software applications, but also for secure building access, without the user having to enter their credentials each time.",2
Technology,Resilient control systems,"In our modern society, computerized or digital control systems have been used to reliably automate many of the industrial operations that we take for granted, from the power plant to the automobiles we drive. However, the complexity of these systems and how the designers integrate them, the roles and responsibilities of the humans that interact with the systems, and the cyber security of these highly networked systems have led to a new paradigm in research philosophy for next-generation control systems. Resilient Control Systems consider all of these elements and those disciplines that contribute to a more effective design, such as cognitive psychology, computer science, and control engineering to develop interdisciplinary solutions. These solutions consider things such as how to tailor the control system operating displays to best enable the user to make an accurate and reproducible response, how to design in cybersecurity protections such that the system defends itself from attack by changing its behaviors, and how to better integrate widely distributed computer control systems to prevent cascading failures that result in disruptions to critical industrial operations. In the context of cyber-physical systems, resilient control systems are an aspect that focuses on the unique interdependencies of a control system, as compared to information technology computer systems and networks, due to its importance in operating our critical industrial operations.",2
Technology,Reverse engineering,"Reverse engineering (also known as backwards engineering or back engineering) is a process or method through which one attempts to understand through deductive reasoning how a previously made device, process, system, or piece of software accomplishes a task with very little (if any) insight into exactly how it does so.
Reverse engineering is applicable in the fields of computer engineering, mechanical engineering, design, electronic engineering, software engineering, chemical engineering, and systems biology.",2
Technology,RFPolicy,"The RFPolicy states a method of contacting vendors about security vulnerabilities found in their products.  It was originally written by hacker and security consultant Rain Forest Puppy.The policy gives the vendor five working days to respond to the reporter of the bug. If the vendor fails to contact the reporter in those five days, the issue is recommended to be disclosed to the general community. The reporter should help the vendor reproduce the bug and work out a fix. The reporter should delay notifying the general community about the bug if the vendor provides feasible reasons for requiring so.
If the vendor fails to respond or shuts down communication with the reporter of the problem in more than five working days, the reporter should disclose the issue to the general community. When issuing an alert or fix, the vendor should give the reporter proper credits about reporting the bug.",2
Technology,Risk factor (computing),"In information security, risk factor is a collective name for circumstances affecting the likelihood or impact of a security risk.",2
Technology,Runtime application self-protection,"Runtime application self-protection (RASP) is a security technology that uses runtime instrumentation to detect and block computer attacks by taking advantage of information from inside the running software. The technology differs from perimeter-based protections such as firewalls, that can only detect and block attacks by using network information without contextual awareness. RASP technology is said to improve the security of software by monitoring its inputs, and blocking those that could allow attacks, while protecting the runtime environment from unwanted changes and tampering. RASP-protected applications rely less on external devices like firewalls to provide runtime security protection. When a threat is detected RASP can prevent exploitation and possibly take other actions, including terminating a user's session, shutting the application down, alerting security personnel and sending a warning to the user. RASP aims to close the gap left by application security testing and network perimeter controls, neither of which have enough insight into real-time data and event flows to either prevent vulnerabilities slipping through the review process or block new threats that were unforeseen during development.",2
Technology,SafetyNet API,"The SafetyNet API is a security feature of Google Play Services to provide security sensitive applications verification that the integrity of the device is not compromised, using an application programming interface (API).SafetyNet API is deprecated by Google and will be replaced with Play Integrity soon",2
Technology,SCADA Strangelove,"SCADA Strangelove is an independent group of information security researchers founded in 2012, focused on security assessment of industrial control systems (ICS) and SCADA.

",2
Technology,Seccomp,"seccomp (short for secure computing mode) is a computer security facility in the Linux kernel. seccomp allows a process to make a one-way transition into a ""secure"" state where it cannot make any system calls except exit(), sigreturn(), read() and write() to already-open file descriptors.  Should it attempt any other system calls, the kernel will either just log the event or terminate the process with SIGKILL or SIGSYS. In this sense, it does not virtualize the system's resources but isolates the process from them entirely.
seccomp mode is enabled via the prctl(2) system call using the PR_SET_SECCOMP argument, or (since Linux kernel 3.17) via the seccomp(2) system call. seccomp mode used to be enabled by writing to a file, /proc/self/seccomp, but this method was removed in favor of prctl(). In some kernel versions, seccomp disables the RDTSC x86 instruction, which returns the number of elapsed processor cycles since power-on, used for high-precision timing.seccomp-bpf is an extension to seccomp that allows filtering of system calls using a configurable policy implemented using Berkeley Packet Filter rules. It is used by OpenSSH and vsftpd as well as the Google Chrome/Chromium web browsers on Chrome OS and Linux. (In this regard seccomp-bpf achieves similar functionality, but with more flexibility and higher performance, to the older systrace—which seems to be no longer supported for Linux.)
Some consider seccomp comparable to OpenBSD pledge(2) and FreeBSD capsicum(4).

",2
Technology,Secure coding,"Secure coding is the practice of developing computer software in such a way that guards against the accidental introduction of security vulnerabilities. Defects, bugs and logic flaws are consistently the primary cause of commonly exploited software vulnerabilities. Through the analysis of thousands of reported vulnerabilities, security professionals have discovered that most vulnerabilities stem from a relatively small number of common software programming errors. By identifying the insecure coding practices that lead to these errors and educating developers on secure alternatives, organizations can take proactive steps to help significantly reduce or eliminate vulnerabilities in software before deployment.",2
Technology,Secure environment,"In computing, a secure environment is any system which implements the controlled storage and use of information.  In the event of computing data loss, a secure environment is used to protect personal and/or confidential data.
Often, secure environments employ cryptography as a means to protect information.
Some secure environments employ cryptographic hashing, simply to verify that the information has not been altered since it was last modified.",2
Technology,Secure state,"A secure state is an information systems security term to describe where entities in a computer system are divided into subjects and objects, and it can be formally proven that each state transition preserves security by moving from one secure state to another secure state. Thereby it can be inductively proven that the system is secure. As defined in the Bell–LaPadula model, the secure state is built on the concept of a state machine with a set of allowable states in a system. The transition from one state to another state is defined by transition functions.
A system state is defined to be ""secure"" if the only permitted access modes of subjects to objects are in accordance with a security policy.",2
Technology,Secure transmission,"In computer science, secure transmission refers to the transfer of data such as confidential or proprietary information over a secure channel. Many secure transmission methods require a type of encryption. The most common email encryption is called PKI. In order to open the encrypted file, an exchange of key is done.
Many infrastructures such as banks rely on secure transmission protocols to prevent a catastrophic breach of security. Secure transmissions are put in place to prevent attacks such as ARP spoofing and general data loss. Software and hardware implementations which attempt to detect and prevent the unauthorized transmission of information from the computer systems to an organization on the outside may be referred to as Information Leak Detection and Prevention (ILDP), Information Leak Prevention (ILP), Content Monitoring and Filtering (CMF) or Extrusion Prevention systems and are used in connection with other methods to ensure secure transmission of data.

",2
Technology,Security and Privacy in Computer Systems,"Security and Privacy in Computer Systems is a paper by Willis Ware that was first presented to the public at the 1967 Spring Joint Computer Conference.

",2
Technology,Security awareness,"Security awareness is the knowledge and attitude members of an organization possess regarding the protection of the physical, and especially informational, assets of that organization. Many organizations require formal security awareness training for all workers when they join the organization and periodically thereafter, usually annually.",2
Technology,Security breach notification laws,"Security breach notification laws or data breach notification laws are laws that require individuals or entities affected by a data breach, unauthorized access to data, to notify their customers and other parties about the breach, as well as take specific steps to remedy the situation based on state legislature. Data breach notification laws have two main goals. The first goal is to allow individuals a chance to mitigate risks against data breaches. The second goal is to promote company incentive to strengthen data security.Together, these goals work to minimize consumer harm from data breaches, including impersonation, fraud, and identity theft.Such laws have been irregularly enacted in all 50 U.S. states since 2002. Currently, all 50 states have enacted forms of data breach notification laws. It should be noted though, that there is no federal data breach notification law, despite previous legislative attempts. These laws were enacted in response to an escalating number of breaches of consumer databases containing personally identifiable information. Similarly, multiple other countries, like the European Unions's General Data Protection Regulation (GDPR) and Australia's Privacy Amendment (Notifiable Data Breaches) Act 2017 (Cth), have added data breach notification laws to combat the increasing occurrences of data breaches.The rise in data breaches conducted by both countries and individuals is evident and alarming, as the number of reported data breaches has increased from 421 in 2011, to 1,091 in 2016, and 1,579 in 2017 according to the Identity Theft Resource Center (ITRC). It has also impacted millions of people and gained increasing public awareness due to large data breaches such as the October 2017 Equifax breach that exposed almost 146 million individual's personal information.

",2
Technology,Security bug,"A security bug or security defect is a software bug that can be exploited to gain unauthorized access or privileges on a computer system. Security bugs introduce security vulnerabilities by compromising one or more of:

Authentication of users and other entities
Authorization of access rights and privileges
Data confidentiality
Data integritySecurity bugs do not need be identified nor exploited to be qualified as such and are assumed to be much more common than known vulnerabilities in almost any system.",2
Technology,Security information management,"Information security management (ISM) defines and manages controls that an organization needs to implement to ensure that it is sensibly protecting the confidentiality, availability, and integrity of assets from threats and vulnerabilities. The core of ISM includes information risk management, a process that involves the assessment of the risks an organization must deal with in the management and protection of assets, as well as the dissemination of the risks to all appropriate stakeholders. This requires proper asset identification and valuation steps, including evaluating the value of confidentiality, integrity, availability, and replacement of assets. As part of information security management, an organization may implement an information security management system and other best practices found in the ISO/IEC 27001, ISO/IEC 27002, and ISO/IEC 27035 standards on information security.

",2
Technology,Security log,"A security log is used to track security-related information on a computer system. Examples include:

Windows Security Log
Internet Connection Firewall security logAccording to Stefan Axelsson, ""Most UNIX installations do not run any form of security logging software, mainly because the security logging facilities are expensive in terms of disk storage, processing time, and the cost associated with analyzing the audit trail, either manually or by special software.""",2
Technology,Security of smart meters,"A smart meter is an electronic device that records information such as consumption of electric energy, voltage levels, current, and power factor. Smart meters communicate the information to the consumer for greater clarity of consumption behavior, and  electricity suppliers for system monitoring and customer billing. Smart meters typically record energy near real-time, and report regularly, short intervals throughout the day. Smart meters enable two-way communication between the meter and the central system. Such an advanced metering infrastructure (AMI) differs from automatic meter reading (AMR) in that it enables two-way communication between the meter and the supplier. Communications from the meter to the network may be wireless, or via fixed wired connections such as power line carrier (PLC). Wireless communication options in common use include cellular communications, Wi-Fi (readily available), wireless ad hoc networks over Wi-Fi, wireless mesh networks, low power long-range wireless (LoRa), Wize (high radio penetration rate, open, using the frequency 169 MHz)  ZigBee (low power, low data rate wireless), and Wi-SUN (Smart Utility Networks).",2
Technology,Security testing,"Security testing is a process intended to reveal flaws in the security mechanisms of an information system that protect data and maintain functionality as intended. Due to the logical limitations of security testing, passing the security testing process is not an indication that no flaws exist or that the system adequately satisfies the security requirements.
Typical security requirements may include specific elements of confidentiality, integrity, authentication, availability, authorization and non-repudiation. Actual security requirements tested depend on the security requirements implemented by the system. Security testing as a term has a number of different meanings and can be completed in a number of different ways. As such, a Security Taxonomy helps us to understand these different approaches and meanings by providing a base level to work from.

",2
Technology,Security type system,"In computer science, a type system can be described as a syntactic framework which contains a set of rules that are used to assign a type property (int, boolean, char etc.) to various components of a computer program, such as variables or functions. A security type system works in a similar way, only with a main focus on the security of the computer program, through information flow control. Thus, the various components of the program are assigned security types, or labels. The aim of a such system is to ultimately be able to verify that a given program conforms to the type system rules and satisfies non-interference. Security type systems is one of many security techniques used in the field of language-based security, and is tightly connected to information flow and information flow policies.
In simple terms, a security type system can be used to detect if there exists any kind of violation of confidentiality or integrity in a program, i.e. the programmer wants to detect if the program is in line with the information flow policy or not.",2
Technology,Security Vision,"Security Vision – software meant for automation of information security management system (ISMS) organisation.
Software of this kind is a representative of security operations center (SOC).",2
Technology,Security.txt,"security.txt is a proposed standard for websites' security information that is meant to allow security researchers to easily report security vulnerabilities. The standard prescribes a text file called ""security.txt"" in the well known location, similar in syntax to robots.txt but intended to be machine- and human-readable, for those wishing to contact a website's owner about security issues. security.txt files have been adopted by Google, GitHub, LinkedIn, and Facebook.",2
Technology,Separation of protection and security,"In computer sciences the separation of protection and security is a design choice. Wulf et al. identified protection as a mechanism and security as a policy, therefore making the protection-security distinction a particular case of the separation of mechanism and policy principle.  Many frameworks consider both as Security controls of varying types. For example, protection mechanisms would be considered technical controls, while a policy would be considered an administrative control.",2
Technology,Shell Control Box,"Shell Control Box (SCB) is a network security appliance that controls privileged access to remote IT systems, records activities in replayable audit trails, and prevents malicious actions. For example, it records as a system administrator updates a file server or a third-party network operator configures a router. The recorded audit trails can be replayed like a movie to review the events as they occurred. The content of the audit trails is indexed to make searching for events and automatic reporting possible.
SCB is a Linux-based device developed by Balabit. It is an application level proxy gateway. In 2017, Balabit changed the name of the product to Privileged Session Management (PSM) and repositioned it as the core module of its Privileged Access Management solution.",2
Technology,Sherwood Applied Business Security Architecture,"SABSA (Sherwood Applied Business Security Architecture) is a model and methodology for developing a risk-driven enterprise information security architecture and service management, to support critical business processes. It was developed independently from the Zachman Framework, but has a similar structure. The primary characteristic of the SABSA model is that everything must be derived from an analysis of the business requirements for security, especially those in which security has an enabling function through which new business opportunities can be developed and exploited.
The process analyzes the business requirements at the outset, and creates a chain of traceability through the strategy and concept, design, implementation, and ongoing ‘manage and measure’ phases of the lifecycle to ensure that the business mandate is preserved.  Framework tools created from practical experience further support the whole methodology.  
The model is layered, with the top layer being the business requirements definition stage.  At each lower layer a new level of abstraction and detail is developed, going through the definition of the conceptual architecture, logical services architecture, physical infrastructure architecture and finally at the lowest layer, the selection of technologies and products (component architecture). 
The SABSA model itself is generic and can be the starting point for any organization, but by going through the process of analysis and decision-making implied by its structure, it becomes specific to the enterprise, and is finally highly customized to a unique business model.  It becomes in reality the enterprise security architecture, and it is central to the success of a strategic program of information security management within the organization. 
SABSA is a particular example of a methodology that can be used both for IT (information technology) and OT (operational technology) environments.",2
Technology,Site Security Handbook,"The Site Security Handbook, RFC 2196, is a guide on setting computer security policies and procedures for sites that have systems on the Internet (however, the information provided should also be useful to sites not yet connected to the Internet).  The guide lists issues and factors that a site must consider when setting their own policies.  It makes a number of recommendations and provides discussions of relevant areas.
This guide is only a framework for setting security policies and procedures.  In order to have an effective set of policies and procedures, a site will have to make many decisions, gain agreement, and then communicate and implement these policies.
The guide is a product of the IETF SSH working group, and was published in 1997, obsoleting the earlier RFC 1244 from 1991.",2
Technology,SMBGhost,"SMBGhost (or SMBleedingGhost or CoronaBlue) is a type of security vulnerability, with wormlike features, that affects Windows 10 computers and was first reported publicly on 10 March 2020. A Proof-of-Concept (PoC) exploit code was published 1 June 2020 on GitHub by a security researcher. The code could possibly spread to millions of unpatched computers, resulting in as much as tens of billions of dollars in losses.Microsoft recommends all users of Windows 10 versions 1903 and 1909 and Windows Server versions 1903 and 1909 to install patches, and states, ""We recommend customers install updates as soon as possible as publicly disclosed vulnerabilities have the potential to be leveraged by bad actors ... An update for this vulnerability was released in March [2020], and customers who have installed the updates, or have automatic updates enabled, are already protected.""  Workarounds, according to Microsoft, such as disabling SMB compression and blocking port 445, may help but may not be sufficient.According to the advisory division of Homeland Security, ""Malicious cyber actors are targeting unpatched systems with the new [threat], ... [and] strongly recommends using a firewall to block server message block ports from the internet and to apply patches to critical- and high-severity vulnerabilities as soon as possible.""",2
Technology,Software Guard Extensions,"Intel Software Guard Extensions (SGX) is a set of security-related instruction codes that are built into some Intel central processing units (CPUs). They allow user-level and operating system code to define private regions of memory, called enclaves, whose contents is inaccessible from the outside. SGX is designed to be useful for implementing secure remote computation, secure web browsing, and digital rights management (DRM). Other applications include concealment of proprietary algorithms and of encryption keys.SGX involves encryption by the CPU of a portion of memory (the enclave). Data and code originating in the enclave are decrypted on the fly within the CPU, protecting them from being examined or read by other code, including code running at higher privilege levels such the operating system and any underlying hypervisors. While this can mitigate many kinds of attacks, it does not protect against side-channel attacks.A pivot by Intel in 2021 resulted in the deprecation of SGX from the 11th and 12th generation Intel Core Processors, but development continues on Intel Xeon for cloud and enterprise use.

",2
Technology,Software-defined perimeter,"A software-defined perimeter (SDP), also called a ""black cloud"", is an approach to computer security which evolved from the work done at the Defense Information Systems Agency (DISA) under the Global Information Grid (GIG) Black Core Network initiative around 2007.  Software-defined perimeter (SDP) framework was developed by the Cloud Security Alliance (CSA) to control access to resources based on identity. Connectivity in a Software Defined Perimeter is based on a need-to-know model, in which device posture and identity are verified before access to application infrastructure is granted. Application infrastructure is effectively “black” (a DoD term meaning the infrastructure cannot be detected), without visible DNS information or IP addresses.  The inventors of these systems claim that a Software Defined Perimeter mitigates the most common network-based attacks, including: server scanning, denial of service, SQL injection, operating system and application vulnerability exploits, man-in-the-middle, pass-the-hash, pass-the-ticket, and other attacks by unauthorized users.",2
Technology,Spanish Cybersecurity Research Conference,"The Spanish Cybersecurity Research Conference  (Spanish: Jornadas Nacionales de Investigación en Ciberseguridad (JNIC)), is a scientific congress that works as a meeting point where different actors working in the field of cybersecurity research (universities, technological and research centres, companies and public authorities) can exchange knowledge and experience with the shared goal of strengthening research in the Cybersecurity field at the national level.",2
Technology,Spanish Network of Excellence on Cybersecurity Research,"The Spanish Network of Excellence on Cybersecurity Research (RENIC), is a research initiative to promote cybersecurity interests in Spain.",2
Technology,Stegomalware,"Stegomalware is a type of malware that uses steganography to hinder detection. Steganography is the practice of concealing a file, message, image, or video within another file, message, image, video or network traffic. This type of malware operates by building a steganographic system to hide malicious data within its resources and then extracts and executes them dynamically. It is considered one of the most sophisticated and stealthy ways of obfuscation.
The term of `stegomalware' was introduced by researchers in the context of mobile malware and presented at Inscrypt conference in 2014. However, the fact that (mobile) malware could potentially utilize steganography was already presented in earlier works: the use of steganography in malware was first applied to botnets communicating over probabilistically unobservable channels, mobile malware based on covert channels was proposed in the same year. Steganography was later applied to other components of malware engineering such as return-oriented programming and compile-time obfuscation, among others.The Europol-supported CUING initiative monitors the use of steganography in malware.",2
Technology,STRIDE (security),"STRIDE is a model for identifying computer security threats developed by Praerit Garg and Loren Kohnfelder at Microsoft.   It provides a mnemonic for security threats in six categories.The threats are:

Spoofing
Tampering
Repudiation
Information disclosure (privacy breach or data leak)
Denial of service
Elevation of privilegeThe STRIDE was initially created as part of the process of threat modeling.  STRIDE is a model of threats, used to help reason and find threats to a system.  It is used in conjunction with a model of the target system that can be constructed in parallel.  This includes a full breakdown of processes, data stores, data flows, and trust boundaries.Today it is often used by security experts to help answer the question ""what can go wrong in this system we're working on?""
Each threat is a violation of a desirable property for a system:",2
Technology,Supervisor Mode Access Prevention,"Supervisor Mode Access Prevention (SMAP) is a feature of some CPU implementations such as the Intel Broadwell microarchitecture that allows supervisor mode programs to optionally set user-space memory mappings so that access to those mappings from supervisor mode will cause a trap. This makes it harder for malicious programs to ""trick"" the kernel into using instructions or data from a user-space program.",2
Technology,Digital supply chain security,"Digital supply chain security refers to efforts to enhance cyber security within the supply chain. It is a subset of supply chain security and is  focused on the management of cyber security requirements for information technology systems, software and networks, which are driven by threats such as cyber-terrorism, malware, data theft and the advanced persistent threat (APT). Typical supply chain cyber security activities for minimizing risks include buying only from trusted vendors, disconnecting critical machines from outside networks, and educating users on the threats and protective measures they can take.
The acting deputy undersecretary for the National Protection and Programs Directorate for the United States Department of Homeland Security, Greg Schaffer, stated at a hearing that he is aware that there are instances where malware has been found on imported electronic and computer devices sold within the United States.",2
Technology,System integrity,"In telecommunications, the term system integrity has the following meanings: 

That condition of a system wherein its mandated operational and technical parameters are within the prescribed limits.
The quality of an AIS when it performs its intended function in an unimpaired manner, free from deliberate or inadvertent unauthorized manipulation of the system.
The state that exists when there is complete assurance that under all conditions an IT system is based on the logical correctness and reliability of the operating system, the logical completeness of the hardware and software that implement the protection mechanisms, and data integrity.",2
Technology,System Service Descriptor Table,The System Service Descriptor Table (SSDT) is an internal dispatch table within Microsoft Windows.,2
Technology,Thermal attack,"A thermal attack (aka thermal imaging attack) is an approach that exploits heat traces to uncover the entered credentials. These attacks rely on the phenomenon of heat transfer from one object to another. During authentication, heat transfers from the users' hands to the surface they are interacting with, leaving heat traces behind that can be analyzed using thermal cameras that operate in the far-infrared spectrum. These traces can be recovered and used to reconstruct the passwords. In some cases, the attack can be successful even 30 seconds after the user has authenticated.Thermal attacks can be performed after the victim had authenticated, alleviating the need for in-situ observation attacks (e.g., shoulder surfing attacks) that can be affected by hand occlusions. While smudge attacks can reveal the order of entries of graphical passwords, such as the Android Lock Patterns, thermal attacks can reveal the order of entries even in the case of PINs or alphanumeric passwords. The reason thermal attacks leak information about the order of entry is because keys and buttons that the user touches first lose heat over time, while recently touched ones maintain the heat signature for a longer time. This results in distinguishable heat patterns that can tell the attacker which entry was entered first.
Thermal attacks were shown to be effective against plastic keypads, such as the ones used to enter credit card's PINs in supermarkets and restaurants, and on handheld mobile devices such as smartphones and tablets.In their paper published at the Conference on Human Factors in Computing Systems (CHI 2017), Abdelrahman et al. showed that the attack is feasible on today's smartphones. They also proposed some ways to mitigate the attack, such as swiping randomly on the screen to distort the heat traces, or forcing maximum CPU usage for a few seconds.",2
Technology,Thunderspy,"Thunderspy is a type of security vulnerability, based on the Intel Thunderbolt port, first reported publicly on 10 May 2020, that can result in an evil maid (ie, attacker of an unattended device) attack gaining full access to a computer's information in about five minutes, and may affect millions of Apple, Linux and Windows computers, as well as any computers manufactured before 2019, and some after that. According to Björn Ruytenberg, the discoverer of the vulnerability, ""All the evil maid needs to do is unscrew the backplate, attach a device momentarily, reprogram the firmware, reattach the backplate, and the evil maid gets full access to the laptop. All of this can be done in under five minutes.""",2
Technology,Trademark (computer security),"A Trademark in computer security is a contract between code that verifies security properties of an object and code that requires that an object have certain security properties. As such it is useful in ensuring secure information flow.  In object-oriented languages, trademarking is analogous to signing of data but can often be implemented without cryptography.",2
Technology,Trust boundary,"Trust boundary is a term used in computer science and security which describes a boundary where program data or execution changes its level of ""trust,"" or where two principals with different capabilities exchange data or commands. The term refers to any distinct boundary where within a system all sub-systems (including data) have equal trust. An example of an execution trust boundary would be where an application attains an increased privilege level (such as root). A data trust boundary is a point where data comes from an untrusted source--for example, user input or a network socket.A ""trust boundary violation"" refers to a vulnerability where computer software trusts data that has not been validated before crossing a boundary.",2
Technology,Trust on first use,"Trust on first use (TOFU), or trust upon first use (TUFU), is an authentication scheme used by client software which needs to establish a trust relationship with an unknown or not-yet-trusted endpoint. In a TOFU model, the client will try to look up the endpoint's identifier, usually either the public identity key of the endpoint, or the fingerprint of said identity key, in its local trust database. If no identifier exists yet for the endpoint, the client software will either prompt the user to confirm they have verified the purported identifier is authentic, or if manual verification is not assumed to be possible in the protocol, the client will simply trust the identifier which was given and record the trust relationship into its trust database. If in a subsequent connection a different identifier is received from the opposing endpoint, the client software will consider it to be untrusted.

",2
Technology,Trusted client,"In computing, a trusted client is a device or program controlled by the user of a service, but with restrictions designed to prevent its use in ways not authorized by the provider of the service. That is, the client is a device that vendors trust and then sell to the consumers, whom they do not trust. Examples include video games played over a computer network or the Content Scramble System (CSS) in DVDs.
Trusted client software is considered fundamentally insecure: once the security is broken by one user, the break is trivially copyable and available to others. As computer security specialist Bruce Schneier states, ""Against the average user, anything works; there's no need for complex security software. Against the skilled attacker, on the other hand, nothing works."" Trusted client hardware is somewhat more secure, but not a complete solution.Trusted clients are attractive to business as a form of vendor lock-in: sell the trusted client at a loss and charge more than would be otherwise economically viable for the associated service. One early example was radio receivers that were subsidized by broadcasters, but restricted to receiving only their radio station. Modern examples include video recorders being forced by law to include Macrovision copy protection, the DVD region code system and region-coded video game consoles.
Technically knowledgeable consumers and other manufacturers frequently bypass the limiting features of trusted clients — from the simple replacement of the fixed tuning capacitor in the early locked radios to the successful DeCSS cryptographic attack on CSS in 1999. Manufacturers have resorted to legal threats via the Digital Millennium Copyright Act and similar laws to prevent their circumvention, with varying degrees of success.  However, the nature of the internet enables any crack that is discovered and published to be virtually impossible to remove.
Trusted computing aims to create computer hardware which assists in the implementation of such restrictions in software, and attempts to make circumvention of these restrictions more difficult.

",2
Technology,Trustworthy computing,"The term Trustworthy Computing (TwC) has been applied to computing systems that are inherently secure, available, and reliable. It is particularly associated with the Microsoft initiative of the same name, launched in 2002.

",2
Technology,Tunneling protocol,"In computer networks, a tunneling protocol is a communication protocol which allows for the movement of data from one network to another, by exploiting encapsulation. It involves allowing private network communications to be sent across a public network (such as the Internet) through a process called encapsulation.
Because tunneling involves repackaging the traffic data into a different form, perhaps with encryption as standard, it can hide the nature of the traffic that is run through a tunnel.
The tunneling protocol works by using the data portion of a packet (the payload) to carry the packets that actually provide the service. Tunneling uses a layered protocol model such as those of the OSI or TCP/IP protocol suite, but usually violates the layering when using the payload to carry a service not normally provided by the network. Typically, the delivery protocol operates at an equal or higher level in the layered model than the payload protocol.

",2
Technology,Typed assembly language,"In computer science, a typed assembly language (TAL) is an assembly language that is extended to include a method of annotating the datatype of each value that is manipulated by the code. These annotations can then be used by a program (type checker) that processes the assembly language code in order to analyse how it will behave when it is executed.  Specifically, such a type checker can be used to prove the type safety of code that meets the criteria of some appropriate type system.  
Typed assembly languages usually include a high-level memory management system based on garbage collection.
A typed assembly language with a suitably expressive type system can be used to enable the safe execution of untrusted code without using an intermediate representation like bytecode, allowing features similar to those currently provided by virtual machine environments like Java and .NET.",2
Technology,US Cyber Challenge,"US Cyber Challenge is a private program which recruits, trains, and places candidates in cybersecurity jobs in the United States. US Cyber Challenge was formerly a DHS S&T-funded not-for-profit organization, and is currently a program of the Center for Internet Security.",2
Technology,Usage of job applications by hackers,"An application for employment is a standard business document that is prepared with questions deemed relevant by employers. It is used to determine the best candidate to fill a specific role within the company. Most companies provide such forms to anyone upon request, at which point it becomes the responsibility of the applicant to complete the form and return it to the employer for consideration. The completed and returned document notifies the company of the applicant's availability and desire to be employed as well as their qualifications and background so that a determination can be made as to the candidate's suitability to the position.",2
Technology,Vastaamo data breach,"Vastaamo was a Finnish private psychotherapy service provider founded in 2008.  On 21 October 2020, Vastaamo  announced that its patient database had been hacked. Some of the information has been used to extort both the service

provider and its clients and ended up on the dark net. The extorters demanded 40 bitcoins, roughly 450,000 euros, or threatened to publish the records. To add pressure for their demands, the extorters published hundreds of patient records a day on a Tor message board. After the extortion of the company failed, the extorters sent emails to the victims demanding them to pay ransoms in order to avoid publishing their sensitive personal data. The ransom demands were sent to roughly 30,000 victims. The company's security practices were found to be inadequate: the sensitive data was not encrypted and anonymized and the system root did not have a defined password. The patient records were first accessed by intruders in November 2018, while the security flaws continued to exist until March 2019.In December 2021, The Finish Data Protection Authority (DPA) fined Vastaamo 608,000 euros for violating the provisions of the General Data Protection Regulation (GDPR). This cyber-attack became the biggest criminal case in Finland history. It also turned into an international scandal and a cyber-attack unprecedented in its scope due to the tactic called double extortion applied by the cyber criminals.",2
Technology,Vulnerabilities Equities Process,"The Vulnerabilities Equities Process (VEP) is a process used by the U.S. federal government to determine  on a case-by-case basis how it should treat zero-day computer security vulnerabilities; whether to disclose them to the public to help improve general computer security, or to keep them secret for offensive use against the government's adversaries.The VEP was first developed during the period 2008–2009, but only became public in 2016, when the government released a redacted version of the VEP in response to a FOIA request by the Electronic Frontier Foundation.Following public pressure for greater transparency in the wake of the Shadow Brokers affair, the U.S. government made a more public disclosure of the VEP process in November 2017.",2
Technology,Vulnerability assessment (computing),"Vulnerability assessment is a process of defining, identifying and classifying the security holes in information technology systems. An attacker can exploit a vulnerability to violate the security of a system. Some known vulnerabilities are Authentication Vulnerability, Authorization Vulnerability and Input Validation Vulnerability.",2
Technology,Vulnerability Discovery Model,"A Vulnerability Discovery Model (VDM) uses discovery event data with software reliability models for predicting the same.    A thorough presentation of VDM techniques is available in.  Numerous model implementations are available in the MCMCBayes open source repository.  Several VDM examples include:

Alhazmi-Malaiya: Time based model (Alhazmi-Malaiya Logistic (AML) model)
Alhazmi-Malaiya: Effort based model
Rescorla: Quadratic Model and Exponential Model 
Anderson: Thermodynamic Model
Kim: Weibull Model
Linear Model
Hump-Shaped Model
Independent and Dependent Model
Vulnerability Discovery Modeling using Bayesian model averaging
Multivariate Vulnerability Discovery Models",2
Technology,Ware report,"Security Controls for Computer Systems, commonly called the Ware report, is a 1970 text by Willis Ware that was foundational in the field of computer security.",2
Technology,Wargame (hacking),"In hacking, a wargame (or war game) is a cyber-security challenge and mind sport in which the competitors must exploit or defend a vulnerability in a system or application, or gain or prevent access to a computer system.A wargame usually involves a capture the flag logic, based on pentesting, semantic URL attacks, knowledge-based authentication, password cracking, reverse engineering of software (often JavaScript, C and assembly language), code injection, SQL injections, cross-site scripting, exploits, IP address spoofing, forensics, and other hacking techniques.",2
Technology,WS-SecurityPolicy,"WS-SecurityPolicy is a web services specification, created by IBM and 12 co-authors, that has become an OASIS standard as of version 1.2. It extends the fundamental security protocols specified by the WS-Security, WS-Trust and WS-SecureConversation by offering mechanisms to represent the capabilities and requirements of web services as policies. Security policy assertions are based on the WS-Policy framework. 
Policy assertions can be used to require more generic security attributes like transport layer security <TransportBinding>, message level security <AsymmetricBinding> or timestamps, and specific attributes like token types. 
Most policy assertion can be found in following categories:

Protection assertions identify the elements of a message that are required to be signed, encrypted or existent.
Token assertions specify allowed token formats (SAML, X509, Username etc.).
Security binding assertions control basic security safeguards like transport and message level security, cryptographic algorithm suite and required timestamps.
Supporting token assertions add functions like user sign-on using a username token.Policies can be used to drive development tools to generate code with certain capabilities, or may be used at runtime to negotiate the security aspects of web service communication. Policies may be attached to WSDL elements such as service, port, operation and message, as defined in WS Policy Attachment.",2
Technology,Zardoz (computer security),"In computer security, the Zardoz list, more formally known as the Security-Digest list, was a famous semi-private full disclosure mailing list run by Neil Gorsuch from 1989 through 1991. It identified weaknesses in systems and gave directions on where to find them. Zardoz is most notable for its status as a perennial target for computer hackers, who sought archives of the list for information on undisclosed software vulnerabilities.",2
Technology,Zero-day (computing),"A zero-day (also known as a 0-day) is a computer-software vulnerability previously unknown to those who should be interested in its mitigation, like the vendor of the target software. Until the vulnerability is mitigated, hackers can exploit it to adversely affect programs, data, additional computers or a network. An exploit directed at a zero-day is called a zero-day exploit, or zero-day attack.
The term ""zero-day"" originally referred to the number of days since a new piece of software was released to the public, so ""zero-day software"" was obtained by hacking into a developer's computer before release. Eventually the term was applied to the vulnerabilities that allowed this hacking, and to the number of days that the vendor has had to fix them. Once the vendors learn of the vulnerability, they will usually create patches or advise workarounds to mitigate it.
The more recently that the vendor has become aware of the vulnerability, the more likely it is that no fix or mitigation has been developed. Once a fix is developed, the chance of the exploit succeeding decreases as more users apply the fix over time. For zero-day exploits, unless the vulnerability is inadvertently fixed, such as by an unrelated update that happens to fix the vulnerability, the probability that a user has applied a vendor-supplied patch that fixes the problem is zero, so the exploit would remain available. Zero-day attacks are a severe threat.",2
Technology,Zero-knowledge service,"In cloud computing, the term zero-knowledge (or occasionally no-knowledge or zero access) refers to software services that store, transfer or manipulate data such that it is only accessible to its owner, not to the service provider. This is accomplished by encrypting the raw data at the client side or end-to-end (in case of one or more clients, respectively), without disclosing the password to the service provider. This means neither the service provider, nor any third party that might intercept the data, can ever decrypt and access the data on their own, allowing the client a higher degree of privacy than would otherwise be possible. In addition, zero-knowledge services usually aspire to hold as little metadata as possible, so as not to jeopardize clients' privacy by holding data beyond what is functionally needed by the service.
The term ""zero-knowledge"" was popularized by backup service SpiderOak, which later switched to using the term ""no knowledge"" to avoid confusion with the computer science concept of zero-knowledge proof.
Providers of zero-knowledge services include:

SpiderOak
Tresorit
Sync.com
NordPass
Cubbit
ProtonMail
Signal
LucidLink

",2
Life Science,Portal:Biology,"Geology (from Ancient Greek  γῆ (gê) 'earth', and  -λoγία (-logía) 'study of, discourse') is a branch of natural science concerned with Earth and other astronomical objects, the features or rocks of which it is composed, and the processes by which they change over time. Modern geology significantly overlaps all other Earth sciences, including hydrology and the atmospheric sciences, and so is treated as one major aspect of integrated Earth system science and planetary science.
Geology describes the structure of the Earth on and beneath its surface, and the processes that have shaped that structure. It also provides tools to determine the relative and absolute ages of rocks found in a given location, and also to describe the histories of those rocks. By combining these tools, geologists are able to chronicle the geological history of the Earth as a whole, and also to demonstrate the age of the Earth. Geology provides the primary evidence for plate tectonics, the evolutionary history of life, and the Earth's past climates.
Geologists use a wide variety of methods to understand the Earth's structure and evolution, including field work, rock description, geophysical techniques, chemical analysis, physical experiments, and numerical modelling. In practical terms, geology is important for mineral and hydrocarbon exploration and exploitation, evaluating water resources, understanding of natural hazards, the remediation of environmental problems, and providing insights into past climate change. Geology is a major academic discipline, and it is central to geological engineering and plays an important role in geotechnical engineering.

",1
Life Science,Biology,"Geology (from Ancient Greek  γῆ (gê) 'earth', and  -λoγία (-logía) 'study of, discourse') is a branch of natural science concerned with Earth and other astronomical objects, the features or rocks of which it is composed, and the processes by which they change over time. Modern geology significantly overlaps all other Earth sciences, including hydrology and the atmospheric sciences, and so is treated as one major aspect of integrated Earth system science and planetary science.
Geology describes the structure of the Earth on and beneath its surface, and the processes that have shaped that structure. It also provides tools to determine the relative and absolute ages of rocks found in a given location, and also to describe the histories of those rocks. By combining these tools, geologists are able to chronicle the geological history of the Earth as a whole, and also to demonstrate the age of the Earth. Geology provides the primary evidence for plate tectonics, the evolutionary history of life, and the Earth's past climates.
Geologists use a wide variety of methods to understand the Earth's structure and evolution, including field work, rock description, geophysical techniques, chemical analysis, physical experiments, and numerical modelling. In practical terms, geology is important for mineral and hydrocarbon exploration and exploitation, evaluating water resources, understanding of natural hazards, the remediation of environmental problems, and providing insights into past climate change. Geology is a major academic discipline, and it is central to geological engineering and plays an important role in geotechnical engineering.

",1
Life Science,Biologist,"A biologist is a scientist who conducts research in biology. Biologists are interested in studying life on Earth, whether it is an individual cell, a multicellular organism, or a community of interacting populations. They usually specialize in a particular branch (e.g., molecular biology, zoology, and evolutionary biology) of biology and have a specific research focus (e.g., studying malaria or cancer).Biologists who are involved in basic research have the aim of advancing knowledge about the natural world. They conduct their research using the scientific method, which is an empirical method for testing hypotheses. Their discoveries may have applications for some specific purpose such as in biotechnology, which has the goal of developing medically useful products for humans.In modern times, most biologists have one or more academic degrees such as a bachelor's degree plus an advanced degree like a master's degree or a doctorate. Like other scientists, biologists can be found working in different sectors of the economy such as in academia, nonprofits, private industry, or government.",1
Life Science,Acquired neuroprotection,"Acquired neuroprotection is a synaptic-activity-dependent form of adaptation in the nervous system that renders neurons more resistant to harmful conditions. The term was coined by Hilmar Bading. This use-dependent enhancement of cellular survival activity requires changes in gene expression triggered by neuronal activity and nuclear calcium signaling. In rodents, components of the neuroprotective gene program can reduce brain damage caused by seizure-like activity or by a stroke. In acute and chronic neurodegenerative diseases, gene regulatory events important for acquired neuroprotection are antagonized by extrasynaptic NMDA receptor signaling leading to increased vulnerability, loss of structural integrity, and bioenergetics dysfunction.",1
Life Science,AGCS family,Members of the Alanine or Glycine:Cation Symporter (AGCS) Family (TC# 2.A.25) transport alanine and/or glycine in symport with Na+ and or H+.,1
Life Science,Allometric engineering,"Allometry is the study of the relationship of body size to shape, anatomy, physiology and finally behaviour, first outlined by Otto Snell in 1892, by D'Arcy Thompson in 1917 in On Growth and Form and by Julian Huxley in 1932.",1
Life Science,AnIML,"Animals (also called Metazoa) are multicellular, eukaryotic organisms in the biological kingdom Animalia. With few exceptions, animals consume organic material, breathe oxygen, are able to move, can reproduce sexually, and go through an ontogenetic stage in which their body consists of a hollow sphere of cells, the blastula, during embryonic development. Over 1.5 million living animal species have been described—of which around 1 million are insects—but it has been estimated there are over 7 million animal species in total. Animals range in length from 8.5 micrometres (0.00033 in) to 33.6 metres (110 ft). They have complex interactions with each other and their environments, forming intricate food webs. The scientific study of animals is known as zoology.
Most living animal species are in Bilateria, a clade whose members have a bilaterally symmetric body plan. The Bilateria include the protostomes, containing invertebrates such as nematodes, arthropods, and molluscs, and the deuterostomes, containing the echinoderms and the chordates, the latter including the vertebrates. Life forms interpreted as early animals were present in the Ediacaran biota of the late Precambrian. Many modern animal phyla became clearly established in the fossil record as marine species during the Cambrian explosion, which began around 539 million years ago. 6,331 groups of genes common to all living animals have been identified; these may have arisen from a single common ancestor that lived 650 million years ago.
Historically, Aristotle divided animals into those with blood and those without. Carl Linnaeus created the first hierarchical biological classification for animals in 1758 with his Systema Naturae, which Jean-Baptiste Lamarck expanded into 14 phyla by 1809. In 1874, Ernst Haeckel divided the animal kingdom into the multicellular Metazoa (now synonymous for Animalia) and the Protozoa, single-celled organisms no longer considered animals. In modern times, the biological classification of animals relies on advanced techniques, such as molecular phylogenetics, which are effective at demonstrating the evolutionary relationships between taxa.
Humans make use of many animal species, such as for food (including meat, milk, and eggs), for materials (such as leather and wool), as pets, and as working animals including for transport. Dogs have been used in hunting, as have birds of prey, while many terrestrial and aquatic animals were hunted for sports. Nonhuman animals have appeared in art from the earliest times and are featured in mythology and religion.",1
Life Science,APC Family,"The Amino Acid-Polyamine-Organocation (APC) Family (TC# 2.A.3) of transport proteins includes members that function as solute:cation symporters and solute:solute antiporters. They occur in bacteria, archaea, fungi, unicellular eukaryotic protists, slime molds, plants and animals. They vary in length, being as small as 350 residues and as large as 850 residues. The smaller proteins are generally of prokaryotic origin while the larger ones are of eukaryotic origin. Most of them possess twelve transmembrane α-helical spanners but have a re-entrant loop involving TMSs 2 and 3. The APC Superfamily was established to encompass a wider range of homologues.",1
Life Science,Archinephros,"The archinephros, or holonephros, is a primitive kidney that has been retained by the larvae of hagfish and some caecilians.  A recent author has referred to this structure as ""the hypothetical primitive kidney of ancestral vertebrates"".  In the earliest vertebrates, this structure potentially extended the entire length of the body and consisted of paired segmental structures which drained via a pair of archinephrenic ducts into the cloaca.  The entire structure arises from the nephric ridge, which in higher animal embryos gives rise to nephrotomes and the pronephroi at around 4 weeks gestation in humans.  The pronephroi are supplanted by mesonephroi and finally by definitive kidneys, the metanephroi, by around 5 weeks gestation.  The archinephros is nonfunctional in humans and other mammals.The three types of mature vertebrate kidneys develop from the archinephros: the pronephros from the front section, the mesonephros from the mid-section and the metanephros from the rear section.",1
Life Science,Artificial lateral line,"An Artificial Lateral Line (ALL) is a biomimetic lateral line system. A lateral line is a system of sensory organs in aquatic animals such as fish, that serves to  detect movement, vibration, and pressure gradients in their environment. An artificial lateral line is an artificial biomimetic array of distinct mechanosensory transducers that, similarly,  permits the formation of a spatial-temporal image of the sources in immediate vicinity based on hydrodynamic signatures; the opurpose is to assist in obstacle avoidance and object tracking.... The biomimetic lateral line system has the potential to improve navigation in Under-water Vehicles when vision is partially or fully compromised. Under-water navigation is challenging due to the rapid attenuation of radio frequency and Global Positioning System signals. In addition ALL system can overcome some of the drawbacks in traditional localization techniques like SONAR and optical imaging.
The basic component of either a natural or artificial lateral line is a neuromast, a mechanoreceptive organ that allows the sensing of mechanical changes in water. Hair cell serves as the basic unit in flow and acoustic sensing. Some species like arthropods use a single hair cell for this function and other creatures like fish use a bundle of hair cells to achieve pointwise sensing. The fish lateral line consists of thousands of hair cells. In fishe, a  neuromast is a fine hair-like structure that uses transduction of rate coding to transmit the directionality of the signal. Each neuromast has a direction of maximum sensitivity providing directionality.",1
Life Science,Autogamy depression,"Autogamy depression can be defined as the ""lowered viability of autogamous progeny relative to geitonogamous progeny”. Viability has also been evaluated in terms of percent fruit set or  seed set rather than reproductive fitness of the progeny. The experimental design for observing the occurrence of autogamy depression is called an ""autogamy depression test"" which has been described by researchers as analogous to a test for inbreeding depression. The ability for fitness of autogamous progeny to differ from geitonogamous progeny comes from the understanding that plants can accumulate heritable mutational variation through both mitotic division and meiotic division. Because plants have indeterminate growth, the apical meristems that contribute to the development of the reproductive structures of a plant have the potential to undergo continual mitosis resulting in the accumulation of somatic mutations (acquired mutations). It has been demonstrated through research that long lived plants can have higher per generation mutation rate (based on occurrences of more mitotic cell divisions compared to short lived plants). Any deleterious mutations that appear during mitotic growth are filtered out through cell lineage selection, in which deleterious mutations that are subject to developmental selection during mitotic growth are replaced by vigorous cell lineages, however, somatic mutations that are not expressed will not be subject to selection during growth of the plant and will accumulate in the apical meristem.",1
Life Science,Bacteriophage,"A bacteriophage (), also known informally as a phage (), is a virus that infects and replicates within bacteria and archaea. The term was derived from ""bacteria"" and the Greek φαγεῖν (phagein), meaning ""to devour"". Bacteriophages are composed of proteins that encapsulate a DNA or RNA genome, and may have structures that are either simple or elaborate. Their genomes may encode as few as four genes (e.g. MS2) and as many as hundreds of genes. Phages replicate within the bacterium following the injection of their genome into its cytoplasm.
Bacteriophages are among the most common and diverse entities in the biosphere. Bacteriophages are ubiquitous viruses, found wherever bacteria exist. It is estimated there are more than 1031 bacteriophages on the planet, more than every other organism on Earth, including bacteria, combined. Viruses are the most abundant biological entity in the water column of the world's oceans, and the second largest component of biomass after prokaryotes, where up to 9x108 virions per millilitre have been found in microbial mats at the surface, and up to 70% of marine bacteria may be infected by phages.Phages have been used since the late 20th century as an alternative to antibiotics in the former Soviet Union and Central Europe, as well as in France. They are seen as a possible therapy against multi-drug-resistant strains of many bacteria (see phage therapy).
Phages are known to interact with the immune system both indirectly via bacterial expression of phage-encoded proteins and directly by influencing innate immunity and bacterial clearance.",1
Life Science,Balanced lethal systems,"In evolutionary biology, a balanced lethal system is a situation where recessive lethal alleles are present on two homologous chromosomes. Each of the chromosomes in such a pair carries a different lethal allele, which is compensated for by the functioning allele on the other chromosome. Since both these lethal alleles end up in the gametes in the same frequency as the functioning alleles, half of the offspring, the homozygotes, receive two copies of a lethal allele and therefore die during development. In such systems, only the heterozygotes survive.Balanced lethal systems appear to pose a challenge to evolutionary theory, since a system so wasteful should be rapidly eliminated through natural selection and recombination. Instead, it has become fixed in various species all over the tree of life.",1
Life Science,Basic amino acid antiporter family,"The Basic Amino Acid Antiporter (ArcD) family (TC# 2.A.118) is a constituent of the IT superfamily. This family consists of proteins from Gram-negative and Gram-positive bacteria (e.g., Streptococcus, Escherichia, Salmonella, Fusobacterium and Borrelia species). The proteins are of about 480 amino acyl residues (aas) in length and have 10-12 putative transmembrane segments (TMSs). Functionally characterized homologues are in the DcuC (TC #2.A.61) and ArsB (TC #2.A.4) families. Some members of the family probably catalyze arginine/ornithine or citrulline/ornithine antiport.",1
Life Science,Betaine transporter,"Proteins of the Betaine/Carnitine/Choline Transporter (BCCT) family are found in Gram-negative and Gram-positive bacteria and archaea. The BCCT family is a member a large group of secondary transporters, the APC superfamily. Their common functional feature is that they all transport molecules with a quaternary ammonium group [R-N (CH3)3]. The BCCT family proteins vary in length between 481 and 706 amino acyl residues and possess 12 putative transmembrane α-helical spanners (TMSs).  The x-ray structures reveal two 5 TMS repeats with the total number of TMSs being 10. These porters catalyze bidirectional uniport or are energized by pmf-driven or smf-driven proton or sodium ion symport, respectively, or else by substrate:substrate antiport. Some of these permeases exhibit osmosensory and osmoregulatory properties inherent to their polypeptide chains.

",1
Life Science,Bibliography of encyclopedias: biology,"This is a list of encyclopedias as well as encyclopedic and biographical dictionaries published on the subject of biology in any language.

Entries are in the English language unless specifically stated as otherwise.",1
Life Science,Bioactive agents,"Bioactive agents are substances that can influence an organism, tissue or cell. Examples include enzymes, drugs, vitamins, phytochemicals, and bioactive compounds.
Bioactive agents can be incorporated into polymers, which has applications in drug delivery and commercial production of household goods and biomedical devices. In drug delivery systems, bioactive agents are loaded into enzyme-responsive polymers which can then be cleaved by target enzymes. Activation of the bioactive agents leads to the release of therapeutic cargos.",1
Life Science,Bioactive terrarium,"A bioactive terrarium (or vivarium) is a terrarium for housing one or more terrestrial animal species that includes live plants and populations of small invertebrates and microorganisms to consume and break down the waste products of the primary species. In a functional bioactive terrarium, the waste products will be broken down by these detritivores, reducing or eliminating the need for cage cleaning. Bioactive vivariums are used by zoos and hobbyists to house reptiles and amphibians in an aesthetically pleasing and enriched environment.",1
Life Science,Biological constraints,"Biological constraints are factors which make populations resistant to evolutionary change. One proposed definition of constraint is ""A property of a trait that, although possibly adaptive in the environment in which it originally evolved, acts to place limits on the production of new phenotypic variants.""  Constraint has played an important role in the development of such ideas as homology and body plans.",1
Life Science,Biological dark matter,"Biological dark matter is an informal term for unclassified or poorly understood genetic material. This genetic material may refer to genetic material produced by unclassified microorganisms. By extension, biological dark matter may also refer to the un-isolated microorganism whose existence can only be inferred from the genetic material that they produce. Some of the genetic material may not fall under the three existing domains of life: Bacteria, Archaea and Eukaryota; thus, it has been suggested that a possible fourth domain of life may yet be discovered, although other explanations are also probable. Alternatively, the genetic material may refer to non-coding DNA (so-called ""junk DNA"") and non-coding RNA produced by known organisms.

",1
Life Science,Biological data,"Biological data refers to a compound or information derived from living organisms and their products. A medicinal compound made from living organisms, such as a serum or a vaccine, could be characterized as biological data. Biological data is highly complex when compared with other forms of data. There are many forms of biological data, including text, sequence data, protein structure, genomic data and amino acids, and links among others.",1
Life Science,Biological exponential growth,"Biological exponential growth is the unrestricted growth of a population of organisms, occurring when resources in its habitat are unlimited. Most commonly apparent in species that reproduce quickly and asexually, like bacteria, exponential growth is intuitive from the fact that each organism can divide and produce two copies of itself. Each descendent bacterium can itself divide, again doubling the population size. The bacterium Escherichia coli, under optimal conditions, may divide as often as twice per hour. Left unrestricted, a colony would cover the Earth's surface in less than a day.Resource availability is essential for the unimpeded growth of a population. Ideally, when resources in the habitat are unlimited, each species has the ability to realise fully its innate potential to grow in number, as Charles Darwin observed while developing his theory of natural selection. In almost all situations (with exceptions, such as a laboratory) this is unrealistic; there is simply a finite quantity of everything necessary for life. As the population approaches its carrying capacity, the rate of growth decreases, and the population trend will become logistic.If, in a hypothetical population of size N, the birth rates (per capita) are represented as b and death rates (per capita) as d, then the increase or decrease in N during a time period t will be

  
    
      
        
          
            
              d
              N
            
            
              d
              t
            
          
        
        =
        (
        b
        −
        d
        )
        N
      
    
    {\displaystyle {\frac {dN}{dt}}=(b-d)N}
  
(b-d) is called the 'intrinsic rate of natural increase' and is a very important parameter chosen for assessing the impacts of any biotic or abiotic factor on population growth.Any species growing exponentially under unlimited resource conditions can reach enormous population densities in a short time. Darwin showed how even a slow growing animal like the elephant could reach an enormous population if there were unlimited resources for its growth in its habitat.",1
Life Science,Biological life cycle,"In biology, a biological life cycle (or just life cycle or lifecycle when the biological context is clear) is a series of changes in form that an organism undergoes, returning to the starting state. ""The concept is closely related to those of the life history, development and ontogeny, but differs from them in stressing renewal."" Transitions of form may involve growth, asexual reproduction, or sexual reproduction.
In some organisms, different ""generations"" of the species succeed each other during the life cycle. For plants and many algae, there are two multicellular stages, and the life cycle is referred to as alternation of generations. The term life history is often used, particularly for organisms such as the red algae which have three multicellular stages (or more), rather than two.Life cycles that include sexual reproduction involve alternating haploid (n) and diploid (2n) stages, i.e., a change of ploidy is involved. To return from a diploid stage to a haploid stage, meiosis must occur. In regard to changes of ploidy, there are 3 types of cycles:

haplontic life cycle — the haploid stage is multicellular and the diploid stage is a single cell, meiosis is ""zygotic"".
diplontic life cycle — the diploid stage is multicellular and haploid gametes are formed, meiosis is ""gametic"".
haplodiplontic life cycle (also referred to as diplohaplontic, diplobiontic, or dibiontic life cycle) — multicellular diploid and haploid stages occur, meiosis is ""sporic"".The cycles differ in when mitosis (growth) occurs. Zygotic meiosis and gametic meiosis have one mitotic stage: mitosis occurs during the n phase in zygotic meiosis and during the 2n phase in gametic meiosis. Therefore, zygotic and gametic meiosis are collectively termed ""haplobiontic"" (single mitotic phase, not to be confused with haplontic). Sporic meiosis, on the other hand, has mitosis in two stages, both the diploid and haploid stages, termed ""diplobiontic"" (not to be confused with diplontic).",1
Life Science,Biological organisation,"Biological organisation is the hierarchy of complex biological structures and systems that define life using a reductionistic approach. The traditional hierarchy, as detailed below, extends from atoms to biospheres. The higher levels of this scheme are often referred to as an ecological organisation concept, or as the field,  hierarchical ecology.
Each level in the hierarchy represents an increase in organisational complexity, with each ""object"" being primarily composed of the previous level's basic unit. The basic principle behind the organisation is the concept of emergence—the properties and functions found at a hierarchical level are not present and irrelevant at the lower levels.
The biological organisation of life is a fundamental premise for numerous areas of scientific research, particularly in the medical sciences. Without this necessary degree of organisation, it would be much more difficult—and likely impossible—to apply the study of the effects of various physical and chemical phenomena to diseases and physiology (body function). For example, fields such as cognitive and behavioral neuroscience could not exist if the brain was not composed of specific types of cells, and the basic concepts of pharmacology could not exist if it was not known that a change at the cellular level can affect an entire organism. These applications extend into the ecological levels as well. For example, DDT's direct insecticidal effect occurs at the subcellular level, but affects higher levels up to and including multiple ecosystems. Theoretically, a change in one atom could change the entire biosphere.",1
Life Science,Biological rules,"A biological rule or biological law is a generalized law, principle, or rule of thumb formulated to describe patterns observed in living organisms. Biological rules and laws are often developed as succinct, broadly applicable ways to explain complex phenomena or salient observations about the ecology and biogeographical distributions of plant and animal species around the world, though they have been proposed for or extended to all types of organisms. Many of these regularities of ecology and biogeography are named after the biologists who first described them.From the birth of their science, biologists have sought to explain apparent regularities in observational data. In his biology, Aristotle inferred rules governing differences between live-bearing tetrapods (in modern terms, terrestrial placental mammals). Among his rules were that brood size decreases with adult body mass, while lifespan increases with gestation period and with body mass, and fecundity decreases with lifespan. Thus, for example, elephants have smaller and fewer broods than mice, but longer lifespan and gestation. Rules like these concisely organized the sum of knowledge obtained by early scientific measurements of the natural world, and could be used as models to predict future observations. Among the earliest biological rules in modern times are those of Karl Ernst von Baer (from 1828 onwards) on embryonic development, and of Constantin Wilhelm Lambert Gloger on animal pigmentation, in 1833.
There is some scepticism among biogeographers about the usefulness of general rules. For example, J.C. Briggs, in his 1987 book Biogeography and Plate Tectonics, comments that while Willi Hennig's rules on cladistics ""have generally been helpful"", his progression rule is ""suspect"".",1
Life Science,Biomaterial surface modifications,"Biomaterials exhibit various degrees of compatibility with the harsh environment within a living organism. They need to be nonreactive chemically and physically with the body, as well as integrate when deposited into tissue. The extent of compatibility varies based on the application and material required. Often modifications to the surface of a biomaterial system are required to maximize performance. The surface can be modified in many ways, including plasma modification and applying coatings to the substrate. Surface modifications can be used to affect surface energy, adhesion, biocompatibility, chemical inertness, lubricity, sterility, asepsis, thrombogenicity, susceptibility to corrosion, degradation, and hydrophilicity.",1
Life Science,Bioresilience,"Bioresilience refers to the ability of a whole species or an individual of a species to adapt to change. Initially the term applied to changes in the natural environment, but increasingly it is also used for adaptation to anthropogenically induced change.",1
Life Science,Biosemiotics,"Biosemiotics (from the Greek βίος bios, ""life"" and σημειωτικός sēmeiōtikos, ""observant of signs"") is a field of semiotics and biology that studies the prelinguistic meaning-making, or production and interpretation of signs and codes and their communication in the biological realm.Biosemiotics integrates the findings of biology and semiotics and proposes a paradigmatic shift in the scientific view of life, in which semiosis (sign process, including meaning and interpretation) is one of its immanent and intrinsic features. The term biosemiotic was first used by Friedrich S. Rothschild in 1962, but Thomas Sebeok and Thure von Uexküll have implemented the term and field. The field, which challenges normative views of biology, is generally divided between theoretical and applied biosemiotics.
Insights from biosemiotics have also been adopted in the humanities and social sciences, including human-animal studies and human-plant studies.",1
Life Science,Biostasis,"Biostasis or Cryptobiosis is the ability of an organism to tolerate environmental changes without having to actively adapt to them. Biostasis is found in organisms that live in habitats that likely encounter unfavorable living conditions, such as drought, freezing temperatures, change in pH levels, pressure, or temperature. Insects undergo a type of dormancy to survive these conditions, called diapause. Diapause may be obligatory for these insects to survive. The insect may also be able to undergo change prior to the arrival of the initiating event.",1
Life Science,Biotron (Wisconsin),"The Biotron is a research facility located at the University of Wisconsin-Madison that ""provides controlled environments and climate-controlled greenhouses to support plant, animal, and materials research for university, non-profit, and commercial clients.""",1
Life Science,Bridged nucleic acid,"A bridged nucleic acid (BNA) is a modified RNA nucleotide. They are sometimes also referred to as constrained or inaccessible RNA molecules. BNA monomers can contain a five-membered, six-membered or even a seven-membered bridged structure with a ""fixed"" C3'-endo sugar puckering. The bridge is synthetically incorporated at the 2', 4'-position of the ribose to afford a 2', 4'-BNA monomer. The monomers can be incorporated into oligonucleotide polymeric structures using standard phosphoamidite chemistry. BNAs are structurally rigid oligo-nucleotides with increased binding affinities and stability.",1
Life Science,C1orf122,"C1orf122 (Chromosome 1 open reading frame 122) is a gene in the human genome that encodes the cytosolic protein ALAESM.. ALAESM is present in all tissue cells and highly up-regulated in the brain, spinal cord, adrenal gland and kidney. This gene can be expressed up to 2.5 times the average gene in its highly expressed tissues. Although the function of C1orf122 is unknown, it is predicted to be used for mitochondria localization.",1
Life Science,Canopy soils,"Canopy soils, also known as arboreal soils, exist in areas of the canopy in a forests where branches, crevices, or some other physical feature on a tree can accumulate organic matter, such as leaves or fine branches. Eventually, this organic matter weathers into some semblance of a soil, and can reach depths of 30 cm in some temperate rainforests.Epiphytes can take root in this thin soil, which accelerates the development of the soil by adding organic material and physically breaking up material with their root system. Common epiphytes in the canopy soils in temperate rainforests include mosses, ferns, and lichens. Epiphytes on trees in the temperate zone are often ubiquitous and can cover entire trees. Some host trees house up to 6.5 tons dry weight of epiphytic biomass, which can equate to more than 4x of its own foliar mass. This massive presence means their dynamics need to be better understood in order to fully understand forest dynamics.  The nutrients that become stored within canopy soils can then be utilized by the epiphytes that grow in them, and even the tree that the canopy soil is accumulating in through the growth of canopy roots. This storage allows nutrients to be more closely cycled through an ecosystem, and prevents nutrients from being washed out of the system.",1
Life Science,Cation diffusion facilitator,"Cation diffusion facilitators (CDFs) are transmembrane proteins that provide tolerance of cells to divalent metal ions, such as cadmium, zinc, and cobalt. These proteins are considered to be efflux pumps that remove these divalent metal ions from cells. However, some members of the CDF superfamily are implicated in ion uptake. All members of the CDF family possess six putative transmembrane spanners with strongest conservation in the four N-terminal spanners. The Cation Diffusion Facilitator (CDF) Superfamily includes the following families:
1.A.52 - The Ca2+ Release-activated Ca2+ (CRAC) Channel (CRAC-C) Family
2.A.4 - The Cation Diffusion Facilitator (CDF) Family
2.A.19 - The Ca2+:Cation Antiporter (CaCA) Family
2.A.103 - The Bacterial Murein Precursor Exporter (MPE) Family

",1
Life Science,Comparator system,"A comparator system, or simply comparator, in the fields of biophysics, biology, and neurology is a particular organisation of neurons. Comparators, as their name suggests, compare several inputs of internal or external information, and are important to the field of neural learning. In biological systems, comparators help an organism adapt to changes in its surroundings.",1
Life Science,Compartmentalized ciliogenesis,Compartmentalized ciliogenesis is the most common type of ciliogenesis where the cilium axoneme is formed separated from the cytoplasm by the ciliary membrane and a ciliary gate known as the transition zone.,1
Life Science,Compensatory growth (organ),"Compensatory growth is a type of regenerative growth that can take place in a number of human organs after the organs are either damaged, removed, or cease to function. Additionally, increased functional demand can also stimulate this growth in tissues and organs. The growth can be a result of increased cell size (compensatory hypertrophy) or an increase in cell division (compensatory hyperplasia) or both. For instance, if one kidney is surgically removed, the cells of other kidney divide at an increased rate. Eventually, the remaining kidney can grow until its mass approaches the combined mass of two kidneys. Along with the kidneys, compensatory growth has also been characterized in a number of other tissues and organs including:

the adrenal glands
the heart
muscles
the liver
the lungs
the pancreas (beta cells and acinar cells)
the mammary gland
the spleen (where bone marrow and lymphatic tissue undergo compensatory hypertrophy and assumes the spleen function during spleen injury)
the testicles
the thyroid gland
The turbinatesA large number of growth factors and hormones are involved with compensatory growth, but the exact mechanism is not fully understood and probably varies between different organs. Nevertheless, angiogenic growth factors which control the growth of blood vessels are particularly important because blood flow significantly determines the maximum growth of an organ.Compensatory growth may also refer to the accelerated growth following a period of slowed growth, particularly as a result of nutrient deprivation.",1
Life Science,Compensatory growth (organism),"Compensatory growth, known as catch-up growth and compensatory gain, is an accelerated growth of an organism following a period of slowed development, particularly as a result of nutrient deprivation. The growth may be with respect to weight or length (or height in humans). For example, oftentimes the body weights of animals who experience nutritional restriction will over time become similar to those of animals who did not experience such stress. It is possible for high compensatory growth rates to result in overcompensation, where the organism exceeds normal weight and often has excessive fat deposition.An organism can recover to normal weight without additional time. Sometimes when the nutrient restriction is severe, the growth period is extended to reach the normal weight. If the nutrient restriction is severe enough, the organism may have permanent stunted growth where it does not ever reach normal weight. Usually in animals, complete recovery from carbohydrate and protein restriction occurs.Compensatory growth has been observed in a number of organisms including humans, other species of mammals, birds, reptiles, fish, plants (especially grasses and young tree seedlings and saplings), fungi, microbes, and damselflies.

",1
Life Science,CRISPR-Display,"CRISPR-Display (CRISP-Disp) is a modification of the CRISPR/Cas9 (Clustered regularly interspaced short palindromic repeats) system for genome editing. The CRISPR/Cas9 system uses a short guide RNA (sgRNA) sequence to direct a Streptococcus pyogenes Cas9 nuclease, acting as a programmable DNA binding protein, to cleave DNA at a site of interest.CRISPR-Display, in contrast, uses a nuclease deficient Cas9 (dCas9) and an engineered sgRNA with aptameric accessory RNA domains, ranging from 100bp to 5kb, outside of the normal complementary targeting sequence. The accessory RNA domains can be functional domains, such as long non-coding RNAs (lncRNAs), protein-binding motifs, or epitope tags for immunochemistry. This allows for investigation of the functionality of certain lncRNAs, and targeting of ribonucleoprotein (RNP) complexes to genomic loci.
CRISPR-Display was first published in Nature Methods in July 2015, and developed by David M. Shechner, Ezgi Hacisuleyman, Scott T. Younger and John Rinn at Harvard University and Massachusetts Institute of Technology (MIT), USA.",1
Life Science,Cultural hitchhiking,"Cultural hitchhiking is a hypothesized gene-culture coevolutionary process through which cultural selection, sexual selection based on cultural preference, limits the diversity at genetically neutral loci being transmitted in parallel to selective cultural traits. The process is thought to account for exceptionally low diversity in neutral loci such as control regions of the mitochondrial genome unaccounted for by any other selective forces. Simply put, selection for certain learned social and cultural behaviors can manifest in specific shaping of a population’s genetic makeup. While the notion that culture plays a significant role in shaping community genetics is widely accepted in the context of human populations it had not been considered or documented in non-human organisms until the late 1990s. The term was coined by the cetologist Hal Whitehead who studies the cultures and population genetics of matrilineal whale communities.
Cultural hitchhiking and has been proposed as a cause for reduced genetic diversity at certain loci in prehistoric  Homo sapiens, dolphins, killer whales, and sperm whales.Cultural hitchhiking is a significant hypothesis because it investigates the relationship between population genetics and culture. By understanding how social behavior can shape the genetic makeup of communities scientists are better able to explain why certain communities have genetic traits distinct from the larger population.",1
Life Science,Deformity,"A deformity, dysmorphism, or dysmorphic feature is a major abnormality of an organism that makes a part of the body appear or function differently than how it is supposed to.",1
Life Science,Diploidization,Diploidization is the process of converting a polyploid genome back into a diploid one. Polyploidy is a product of whole genome duplication (WGD) and is followed by diploidization as a result of genome shock. The plant kingdom has undergone multiple events of polyploidization followed by diploidization in both ancient and recent lineages. It has also been hypothesized that vertebrate genomes have gone through two rounds of paleopolyploidy. The mechanisms of diploidization are poorly understood but patterns of chromosomal loss and evolution of novel genes are observed in the process.,1
Life Science,Disassortative mating,"Disassortative mating (also known as negative assortative mating or heterogamy) is a mating pattern in which individuals with dissimilar phenotypes mate with one another more frequently than would be expected under random mating. Disassortative mating reduces the mean genetic similarities within the population and produces a greater number of heterozygotes. The pattern is character specific, but does not affect allele frequencies. This nonrandom mating pattern will result in deviation from the Hardy-Weinberg principle (which states that genotype frequencies in a population will remain constant from generation to generation in the absence of other evolutionary influences, such as ""mate choice"" in this case).
Disassortative mating is different from outbreeding, which refers to mating patterns in relation to genotypes rather than phenotypes.
Due to homotypic preference (bias toward the same type), assortative mating occurs more frequently then disassortative mating. This is due to the fact that homotypic preferences increase relatedness between mates and between parents and offspring that would promote cooperation and increases inclusive fitness. With disassortative mating, heterotypic preference (bias towards different types) in many cases has been shown to increase overall fitness. When this preference is favored, it allows a population to generate and/or maintain polymorphism (genetic variation within a population).
The fitness advantage aspect of disassortative mating seems straightforward, but the evolution of selective forces involved in disassortative mating are still largely unknown in natural populations. 

",1
Life Science,Dorsal nexus,"The dorsal nexus is an area within the dorsal medial prefrontal cortex that serves as an intersection point for multiple brain networks. Research suggests it plays a role in the maintenance and manipulation of information, as well as supporting the control of cognitive functions such as behavior, memory, and conflict resolution. Abnormally increased connectivity between these networks through the Dorsal Nexus has been associated with certain types of depression. The activity generated by this abnormally high level of connectivity during a depressive state can be identified through Magnetic resonance imaging (MRI) and Positron emission tomography (PET).",1
Life Science,Dying,Dying is the final stage of life that leads to death.,1
Life Science,Encapsulin,"The encapsulins are a family of bacterial proteins that serve as the main structural components of encapsulin nanocompartments. There are several different encapsulin proteins, including EncA, which forms the shell, and EncB, EncC, and EncD, which form the core.",1
Life Science,Encapsulin nanocompartment,"Encapsulin nanocompartments, or encapsulin protein cages, are spherical bacterial organelle-like compartments roughly 25-30 nm in diameter that are involved in various aspects of metabolism, in particular protecting bacteria from oxidative stress. Encapsulin nanocompartments are structurally similar to the HK97 bacteriophage and their function depends on the proteins loaded into the nanocompartment. The sphere is formed from 60 (for a 25 nm sphere) or 180 (for a 30 nm sphere) copies of a single protomer, termed encapsulin. Their structure has been studied in great detail using X-ray crystallography and cryo-electron microscopy.A number of different types of proteins have been identified as being loaded into encapsulin nanocompartments. Peroxidases or proteins similar to ferritins are the two most common types of cargo proteins. While most encapsulin nanocompartments contain only one type of cargo protein, in some species two or three types of cargo proteins are loaded.Encapsulins purified from Rhodococcus jostii can be assembled and disassembled with changes in pH. In the assembled state, the compartment enhances the activity of its cargo, a peroxidase enzyme.",1
Life Science,Endogeny (biology),"Endogenous substances and processes are those that originate from within a living system such as an organism, tissue, or cell.Exogenous substances and processes, which originate from outside of the organism, such as drugs, contrast with endogenous ones.
Many animals are endogenous but mostly ants, many ant species go into hibernation no matter the temperature, moisture, weather, light, etc.
Examples:
Aphaenogaster sinensis,
Camponotus aethiops,
Camponotus herculeanus,
Camponotus japonicus,
Camponotus ligniperdus,
Camponotus pennsylvanicus,
Camponotus vagus,
Cataglyphis cursor,
Formica aquilonia,
Formica polyctena,
Formica ulkei,
Harpagoxenus sublaevis,
Lasius niger,
Lepisiota semenovi,
Leptothorax acervorum,
Leptothorax nylanderi,
Manica rubida,
Messor capitatus,
Myrmica lobicornis,
Myrmica rubra,
Myrmica ruginodis,
Plagiolepis compressus,
Plagiolepis pygmaea,
Ponera coarctata,
Tapinoma erraticum,
Tapinoma karavaievi,

",1
Life Science,Endothelial lipase,"Endothelial lipase (LIPG) is a form of lipase secreted by vascular endothelial cells in tissues with high metabolic rates and vascularization, such as the liver, lung, kidney, and thyroid gland. The LIPG enzyme is a vital component to many biological process. These processes include lipoprotein metabolism, cytokine expression, and lipid composition in cells. Unlike the lipases that hydrolyze Triglycerides, endothelial lipase primarily hydrolyzes phospholipids. Due to the hydrolysis specificity, endothelial lipase contributes to multiple vital systems within the body. On the contrary to the beneficial roles that LIPG plays within the body, endothelial lipase is thought of to play a potential role in cancer and inflammation. Knowledge obtained in vitro and in vivo suggest the relations to these conditions, but human interaction knowledge lacks due to the recent discovery of endothelial lipase. Endothelial lipase was first characterized in 1999. The two independent research groups which are notable for this discovery cloned the endothelial lipase gene and identified the novel lipase secreted from endothelial cells. The anti-Atherosclerosis opportunity through alleviating plaque blockage and prospective ability to raise High-density lipoprotein (HDL) have gained endothelial lipase recognition.",1
Life Science,Extremophiles in biotechnology,"Extremophiles in biotechnology is the application of organisms that thrive in extreme environments to biotechnology.
Extremophiles are organisms that thrive in the most volatile environments on the planet and it is due to their talents that they haven begun playing a large role in biotechnology. These organisms live everywhere from environments of high acidity or salinity to areas with limited or no oxygen are places they call home. Scientists show keen interest in organisms with rare or strange talents and in the past 20-30 years extremophiles have been at the forefront with thousands of researchers delving into their abilities. The area in which there has been the most talk, research, and development in relation to these organisms is biotechnology. Scientists around the globe are either extracting DNA to modify genomes or directly using extremophiles to complete tasks. Thanks to the discovery and interest in these organisms the enzymes used in PCR were found, making the rapid replication of DNA in the lab possible. Since they gained the spotlight researchers have been amassing databases of genome data for the hopes that new traits and abilities can be used to further biotechnical advancements Everything from the biodegradation of waste to the production of new fuels is on the horizon with the developments made in the field of biotechnology. There are many different kinds of extremophiles with each kind favoring a different environment. These organisms have become more and more important to biotechnology as their genomes have been uncovered, revealing a plethora of genetic potential. Currently the main uses of extremophiles lies in processes such as PCR, biofuel generation and biomining, but there are many other smaller scale operations at play. There are also labs that have identified what they wish to do with extremophiles, but haven't been able to fully achieve their goals. While these large scale goals have not yet been met the scientific community is working towards their completion in hope of creating new technologies and processes.

",1
Life Science,FAM63B,FAM63B is a protein which in humans is encoded by the gene FAM63B. This gene is highly expressed in humans. The FAM63B gene is also highly conserved throughout evolutionary history. The discovered function of FAM63B is an interaction with the kinesin-1 light chain and the transportation of vaccinia virus from the nucleus to the cell periphery.,1
Life Science,Fast-scan cyclic voltammetry,"Fast-scan cyclic voltammetry (FSCV) is cyclic voltammetry with a very high scan rate (up to 1×106 V·s−1). Application of high scan rate allows rapid acquisition of a voltammogram within several milliseconds and ensures high temporal resolution of this electroanalytical technique. An acquisition rate of 10 Hz is routinely employed.
FSCV in combination with carbon-fiber microelectrodes became a very popular method for detection of neurotransmitters, hormones and metabolites in biological systems. Initially, FSCV was successfully used for detection of electrochemically active biogenic amines release in chromaffin cells (adrenaline and noradrenaline), brain slices (5-HT, dopamine, norepinephrine) and in vivo in anesthetized or awake and behaving animals (dopamine). Further refinements of the method have enabled detection of 5-HT, HA, norepinephrine, adenosine, oxygen, pH changes in vivo in rats and mice as well as measurement of dopamine and serotonin concentration in fruit flies.

",1
Life Science,Feminist biology,"Feminist biology is an approach to biology that is concerned with the influence of gender values, the removal of gender bias, and the understanding of the overall role of social values in biological research and practices. Feminist Biology, was founded by, among others, Dr. Ruth Bleier of the University of Wisconsin-Madison (who authored the 1984 work Science and Gender: A Critique of Biology and Its Theories on Women and inspired the university's endowed fellowship for feminist biology), it aims to enhance biology by incorporating feminist critique in matters varying from the mechanisms of cell biology and sex selection to the assessment of the meaning of words such as ""gender"" and ""sex"".
Overall, the field is broadly defined and pertains itself to philosophies behind both biological and feminist practice. These considerations make feminist biology debatable and conflictive with itself, particularly when concerning matters of biological determinism, whereby descriptive sex terms of male and female are intrinsically confining, or extreme postmodernism, whereby the body is viewed more as a social construct. Despite opinions ranging from determinist to postmodernist, however, biologists, feminists, and feminist biologists of varying labels alike have made claims to the utility of applying feminist ideology to biological practice and procedure.",1
Life Science,Fenestra,"The skull is a bone structure that forms the head in vertebrates. It supports the structures of the face and provides a protective cavity for the brain. The skull is composed of two parts: the cranium and the mandible. In humans, these two parts are the neurocranium and the viscerocranium (facial skeleton) that includes the mandible as its largest bone. The skull forms the anterior-most portion of the skeleton and is a product of cephalisation—housing the brain, and several sensory structures such as the eyes, ears, nose, and mouth. In humans these sensory structures are part of the facial skeleton.
Functions of the skull include protection of the brain, fixing the distance between the eyes to allow stereoscopic vision, and fixing the position of the ears to enable sound localisation of the direction and distance of sounds. In some animals, such as horned ungulates (mammals with hooves), the skull also has a defensive function by providing the mount (on the frontal bone) for the horns.
The English word skull is probably derived from Old Norse skulle, while the Latin word cranium comes from the Greek root κρανίον (kranion). The human skull fully develops two years after birth.The junctions of the skull bones are joined together by structures called sutures.
The skull is made up of a number of fused flat bones, and contains many foramina, fossae, processes, and several cavities or sinuses. In zoology there are openings in the skull called fenestrae.

",1
Life Science,FMRFamide in Biomphalaria glabrata,"FMRFamide, a neuropeptide involved in cardiac activity regulation, is found in Biomphalaria glabrata, a species of a freshwater snail best known for its role as the intermediate host for the human-infecting trematode parasite Schistosoma mansoni.
This freshwater snail species is used as a model organism, in other words, a non-human species which is extensively studied to understand a biological phenomenon, with the expectation that discoveries made in the model will provide insight into the workings of other organisms. Model organisms are in vivo models and are widely used to research human disease when human experimentation would be unfeasible or unethical.",1
Life Science,Formate-nitrite transporter,"The Formate-Nitrite Transporter (FNT) Family belongs to the Major Intrinsic Protein (MIP) Superfamily. FNT family members have been sequenced from Gram-negative and Gram-positive bacteria, archaea, yeast, plants and lower eukaryotes. The prokaryotic proteins of the FNT family probably function in the transport of the structurally related compounds, formate and nitrite.",1
Life Science,Gender typing,Gender typing is the process by which a child becomes aware of their gender and thus behaves accordingly by adopting values and attributes of members of the sex that they identify as their own. This process is important for a child's social and personality development because it largely impacts the child's understanding of expected social behavior and influences social judgments.,1
Life Science,Genome mining,"Genome mining describes the exploitation of genomic information for the discovery of biosynthetic pathways of natural products and their possible interactions. It depends on computational technology and bioinformatics tools. The mining process relies on a huge amount of data (represented by DNA sequences and annotations) accessible in genomic databases. By applying data mining algorithms,  the data can be used to generate new knowledge in several areas of medicinal chemistry, such as discovering novel natural products.

",1
Life Science,Genotype,"In genetics, the phenotype (from Ancient Greek  φαίνω (phaínō) 'to appear, show, shine', and  τύπος (túpos) 'mark, type') is the set of observable characteristics or traits of an organism. The term covers the organism's  morphology or physical form and structure, its  developmental processes, its biochemical and physiological properties, its behavior, and the products of behavior. An organism's phenotype results from two basic factors: the  expression of an organism's genetic code, or its genotype, and the influence of environmental factors. Both factors may interact, further affecting phenotype. When two or more clearly different phenotypes exist in the same population of a species, the species is called  polymorphic. A well-documented example of polymorphism is  Labrador Retriever coloring; while the coat color depends on many genes, it is clearly seen in the environment as yellow, black, and brown. Richard Dawkins in 1978 and then again in his 1982 book The Extended Phenotype suggested that one can regard bird nests and other built structures such as  caddis-fly larva cases and beaver dams as ""extended phenotypes"".
Wilhelm Johannsen proposed the genotype–phenotype distinction in 1911 to make clear the difference between an organism's hereditary material and what that hereditary material produces. The distinction resembles that proposed by August Weismann (1834–1914), who distinguished between germ plasm (heredity) and somatic cells (the body).  More recently, in the Selfish Gene (1976), Richard Dawkins distinguished these concepts as replicators and vehicles.
The genotype–phenotype distinction should not be confused with Francis Crick's central dogma of molecular biology, a statement about the directionality of molecular sequential information flowing from DNA to protein, and not the reverse.

",1
Life Science,"H+, Na+-translocating pyrophosphatase family","Members of the H+, Na+-translocating Pyrophosphatase (M+-PPase) Family (TC# 3.A.10) are found in the vacuolar (tonoplast) membranes of higher plants, algae, and protozoa, and in both bacteria and archaea. They are therefore ancient enzymes.
Two types of inorganic diphosphatase, very different in terms of both amino acid sequence and structure, have been characterised to date: soluble and transmembrane proton-pumping pyrophosphatases (sPPases and H(+)-PPases, respectively). sPPases are ubiquitous proteins that hydrolyse pyrophosphate to release heat, whereas H+-PPases, so far unidentified in animal and fungal cells, couple the energy of PPi hydrolysis to proton movement across biological membranes. The latter type is represented by this group of proteins. H+-PPases vacuolar-type inorganic pyrophosphatases (V-PPase) or pyrophosphate-energised vacuolar membrane proton pumps. In plants, vacuoles contain two enzymes for acidifying the interior of the vacuole, the V-ATPase and the V-PPase (V is for vacuolar).Two distinct biochemical subclasses of H+-PPases have been characterised to date: K+-stimulated and K+-insensitive.",1
Life Science,Heterolysis (biology),"In biology, heterolysis refers to cellular necrosis by hydrolytic enzymes from surrounding (usually inflammatory) cells. On the other hand, Autolysis is cell necrosis of a cell by its own enzymes, usually due to various causes such as infective agents or physical agents.",1
Life Science,High throughput biology,"High throughput cell biology is the use of automation equipment with classical cell biology techniques to address biological questions that are  otherwise unattainable using conventional methods. It may incorporate techniques from optics, chemistry, biology or image analysis to permit rapid, highly parallel research into how cells function, interact with each other and how pathogens exploit them in disease.High throughput cell biology has many definitions, but is most commonly defined by the search for active compounds in natural materials like in medicinal plants. This is also known as high throughput screening (HTS) and is how most drug discoveries are made today, many cancer drugs, antibiotics, or viral antagonists have been discovered using HTS. The process of HTS also tests substances for potentially harmful chemicals that could be potential human health risks. HTS generally involves hundreds of samples of cells with the model disease and hundreds of different compounds being tested from a specific source. Most often a computer is used to determine when a compound of interest has a desired or interesting effect on the cell samples.
Using this method has contributed to the discovery of the drug Sorafenib (Nexavar). Sorafenib is used as medication to treat multiple types of cancers, including renal cell carcinoma (RCC, cancer in the kidneys), hepatocellular carcinoma (liver cancer), and thyroid cancer. It helps stop cancer cells from reproducing by blocking the abnormal proteins present. In 1994, high throughput screening for this particular drug was completed. It was initially discovered by Bayer Pharmaceuticals in 2001. By using a RAF kinase biochemical assay, 200,000 compounds were screened from medicinal chemistry directed synthesis or combinatorial libraries to identify active molecules against activeRAF kinase. Following three trials of testing, it was found to have anti-angiogenic effects on the cancers, which stops the process of creating new blood vessels in the body.Another discovery made using HTS is Maraviroc. It is an HIV entry inhibitor, and slows the process and prevents HIV from being able to enter human cells. It is used to treat a variety of cancers as well, reducing or blocking the metastasis of cancer cells, which is when cancer cells spread to a completely different part of the body from where it started. High throughput screening for Maraviroc was completed in 1997, and finalized in 2005 by Pfizer global research and development team.
High-throughput biology serves as one facet of what has also been called ""omics research"" - the interface between large scale biology (genome, proteome, transcriptome), technology and researchers. High throughput cell biology has a definite focus on the cell, and methods accessing the cell such as imaging, gene expression microarrays, or genome wide screening. The basic idea is to take methods normally performed on their own and do a very large number of them without impacting their quality High throughput research can be defined as the automation of experiments such that large scale repetition becomes feasible. This is important because many of the questions faced by life science researchers now involve large numbers. For example, the Human Genome contains at least 21,000 genes, all of which can potentially contribute to cell function, or disease. To be able to capture an idea of how these genes interact with one another, which genes are involved in and where they are, methods that encompass from the cell to the genome are of interest.",1
Life Science,HUMARA assay,"HUMARA Assay is one of the most widely used methods to determine the clonal origin of a tumor. The method is based on X chromosome inactivation and it takes the advantage of having different methylation status of a gene called HUMARA (short for human androgen receptor) that is located on X chromosome. Considering the fact that once one X chromosome is inactivated in a cell, all other cells derived from it will have the same X chromosome inactivated, this approach becomes a great tool to differentiate a monoclonal population from a polyclonal  one in a female tissue. HUMARA gene, in particular, has three important features that make it highly convenient for the purpose.
1-) The gene is located on X chromosome and it goes through inactivation by methylation in normal embryogenesis of a female infant. The fact that most-but not all-genes on X chromosome undergo inactivation, this feature becomes an important one.
2-) Human Androgen Receptor gene alleles have varying numbers of CAG repeats. Thus, when DNA from a healthy female tissue is amplified by PCR for a specific region of the gene, two separated bands can be seen on the gel.
3-) The region that is amplified by PCR also has certain base orders that make it susceptible to be digested by HpaII (or HhaI) enzyme when it is not methylated. This detail gives the opportunity to researchers to differentiate a methylated allele from the unmethylated one.
Thanks to these qualities of HUMARA gene, clonal origin of any tissue from a female mammalian organism can be determined.
The basic process is performed as the following :
1-) DNA from the tissue is isolated.
2-) The isolated DNA is treated with the suitable enzyme (such as HpaII) in optimal conditions for a suggested amount of time (i.e. overnight).
3-) DNA is cleaned and the certain region of HUMARA gene is amplified by PCR using ""suitable"" primers (as an example, please see:Ref.2)
4-) After running PCR products through a gel, the gel is visualized and the results are analyzed accordingly. If two bands are apparent, the tissue studied is most likely of polyclonal origin. If a single band is observed, the tissue is monoclonal unless two alleles have exactly the same numbers of CAG repeats or different cells with the same inactivated initiated the tumor; so, seemingly monoclonal although it is actually polyclonal.
In order to make a conclusion about the clonality of a tumor, it is best to use the DNA from a normal tissue of the same person, and a sample without enzyme treatment must also be amplified as a control. If, even in normal tissues without enzyme treatment, a single band is observed, it may be explained as follows; this person has the genetic pattern as XO (this possibility can be excluded to see a band after enzyme treatment because, if indeed XO is the genetic pattern of the sample, then, there will be NO methylation, therefore no band should be visible after digesting with the enzyme. In the case of seeing a band after enzyme treatment, the observation is most likely to mean that the person has two X chromosomes with exact CAG repeats.) When you see two bands for normal tissue (both enzyme treated and untreated), and you see two bands for enzyme treated tumor sample but two bands for untreated tumor DNA, then you are surely looking at a polyclonal tumor. However, if you see the same number of bands only with a single band after enzyme treatment, there is a rather high chance for the tumor of your interest to be monoclonal. In this last case, monoclonality is not for sure because, as stated earlier, there is the possibility of having exact same -CAG- repeats on both alleles.",1
Life Science,I-cell,"A pulmonary alveolus (plural: alveoli, from Latin alveolus, ""little cavity""), also known as an air sac or air space, is one of millions of hollow, distensible cup-shaped cavities in the lungs where oxygen is exchanged for carbon dioxide. Alveoli make up the functional tissue of the lungs known as the lung parenchyma, which takes up 90 percent of the total lung volume.Alveoli are first located in the respiratory bronchioles that mark the beginning of the  respiratory zone. They are located sparsely in these bronchioles, line the walls of the alveolar ducts, and are more numerous in the blind-ended alveolar sacs. The acini are the basic units of respiration, with gas exchange taking place in all the alveoli present. The alveolar membrane is the gas exchange surface, surrounded by a network of capillaries. Across the membrane oxygen is diffused into the capillaries and carbon dioxide released from the capillaries into the alveoli to be breathed out.Alveoli are particular to mammalian lungs. Different structures are involved in gas exchange in other vertebrates.

",1
Life Science,Indeterminate growth,"In biology and botany, indeterminate growth is growth that is not terminated in contrast to determinate growth that stops once a genetically pre-determined structure has completely formed. Thus, a plant that grows and produces flowers and fruit until killed by frost or some other external factor is called indeterminate. For example, the term is applied to tomato varieties that grow in a rather gangly fashion, producing fruit throughout the growing season, and in contrast to a determinate tomato plant, which grows in a more bushy shape and is most productive for a single, larger harvest, then either tapers off with minimal new growth or fruit, or dies.",1
Life Science,Infectious tolerance,"Infectious tolerance is a term referring to a phenomenon where a tolerance-inducing state is transferred from one cell population to another. It can be induced in many ways; although it is often artificially induced, it is a natural in vivo process. A number of research deal with the development of a strategy utilizing this phenomenon in transplantation immunology. The goal is to achieve long-term tolerance of the transplant through short-term therapy.",1
Life Science,Informative site,"In phylogenetics, informative site is a term used when maximum parsimony is the optimality criterion for construction of a phylogenetic tree. It refers to a characteristic for which the number of character-state evolutionary changes of at this site depends on the topology of the tree. The charactetistics can take on multiple types of data, including morphological (such as the presence of wings, tentacles, etc.) or molecular information such as sequences of DNA or proteins.
The informative site is a position in the relevant set of aligned sequences at which there are at least two different character states and each of those states occurs in at least two of the sequences. In other words, it cannot be a fully conserved (invariable) site nor can it be a (singleton) site with a difference in only one sequence. In both cases, the number of character-state changes is the same regardless of the topology of the tree, equal to 0 and 1 respectively.",1
Life Science,Infradian rhythm,"In chronobiology, an infradian rhythm is a rhythm with a period longer than the period of a circadian rhythm, i.e., with a frequency of less than one cycle in 24 hours. Some examples of infradian rhythms in mammals include menstruation, breeding, migration, hibernation, molting and fur or hair growth, and tidal or seasonal rhythms. In contrast, ultradian rhythms have periods shorter than the period of a circadian rhythm. Several infradian rhythms are known to be caused by hormone stimulation or exogenous factors. For example, seasonal depression, an example of an infradian rhythm occurring once a year, can be caused by the systematic lowering of light levels during the winter.",1
Life Science,Intersex (biology),"Intersex is a general term for an organism that has sex characteristics that intermediate between male and female. The term intersex typically applies to abnormal members of gonochoric species that are usually sterile. It is not to be confused with the term hermaphrodite.Intersexuality has been reported in mammals, fishes, nematodes, and  crustaceans.",1
Life Science,Inverted repeat,"An inverted repeat (or IR) is a single stranded sequence of nucleotides followed downstream by its reverse complement. The intervening sequence of nucleotides between the initial sequence and the reverse complement can be any length including zero. For example, 5'---TTACGnnnnnnCGTAA---3' is an inverted repeat sequence. When the intervening length is zero, the composite sequence is a palindromic sequence.Both inverted repeats and direct repeats constitute types of nucleotide sequences that occur repetitively.   These repeated DNA sequences often range from a pair of nucleotides to a whole gene, while the proximity of the repeat sequences varies between widely dispersed and simple tandem arrays. The short tandem repeat sequences may exist as just a few copies in a small region to thousands of copies dispersed all over the genome of most eukaryotes. Repeat sequences with about 10–100 base pairs are known as minisatellites, while shorter repeat sequences having mostly 2–4 base pairs are known as microsatellites.  The most common repeats include the dinucleotide repeats, which have the bases AC on one DNA strand, and GT on the complementary strand. Some elements of the genome with unique sequences function as exons, introns and regulatory DNA. Though the most familiar loci of the repetitive sequences are the centromere and the telomere, a large portion of the repeated sequences in the genome are found among the noncoding DNA.Inverted repeats have a number of important biological functions.  They define the boundaries in transposons and indicate regions capable of self-complementary base pairing (regions within a single sequence which can base pair with each other). These properties play an important role in genome instability and contribute not only to cellular evolution and genetic diversity but also to mutation and disease.  In order to study these effects in detail, a number of programs and databases have been developed to assist in discovery and annotation of inverted repeats in various genomes.",1
Life Science,Kin recognition,"Kin recognition, also called kin detection, is an organism's ability to distinguish between close genetic kin and non-kin. In evolutionary biology and psychology, such an ability is presumed to have evolved for inbreeding avoidance.
An additional adaptive function sometimes posited for kin recognition is a role in kin selection. There is debate over this, since in strict theoretical terms kin recognition is not necessary for kin selection or the cooperation associated with it. Rather, social behaviour can emerge by kin selection in the demographic conditions of 'viscous populations' with organisms interacting in their natal context, without active kin discrimination, since social participants by default typically share recent common origin. Since kin selection theory emerged, much research has been produced investigating the possible role of kin recognition mechanisms in mediating altruism. Taken as a whole, this research suggests that active powers of recognition play a negligible role in mediating social cooperation relative to less elaborate cue-based and context-based mechanisms, such as familiarity, imprinting and phenotype matching.
Because cue-based 'recognition' predominates in social mammals, outcomes are non-deterministic in relation to actual genetic kinship, instead outcomes simply reliably correlate with genetic kinship in an organism's typical conditions. A well-known human example of an inbreeding avoidance mechanism is the Westermarck effect, in which unrelated individuals who happen to spend their childhood in the same household find each other sexually unattractive. Similarly, due to the cue-based mechanisms that mediate social bonding and cooperation, unrelated individuals who grow up together in this way are also likely to demonstrate strong social and emotional ties, and enduring altruism.

",1
Life Science,Kleptoprotein,"A kleptoprotein is a protein which is not encoded in the genome of the organism which uses it, but instead is obtained through diet from a prey organism. Importantly, a kleptoprotein must maintain its function and be mostly or entirely undigested, drawing a distinction from proteins that are digested for nutrition, which become destroyed and non-functional in the process.
This phenomenon was first reported in the bioluminescent fish Parapriacanthus, which has specialized light organs adapted towards counter-illumination, but obtains the luciferase enzyme within these organs from bioluminescent ostracods, including Cypridina noctiluca or Vargula hilgendorfii.",1
Life Science,Machine learning in bioinformatics,"Machine learning in bioinformatics is the application of machine learning algorithms to bioinformatics, including genomics, proteomics, microarrays, systems biology, evolution, and text mining.Prior to the emergence of machine learning, bioinformatics algorithms had to be programmed by hand; for problems such as protein structure prediction, this proved difficult. Machine learning techniques, such as deep learning can learn features of data sets, instead of requiring the programmer to define them individually. The algorithm can further learn how to combine low-level features into more abstract features, and so on. This multi-layered approach allows such systems to make sophisticated predictions when appropriately trained. These methods contrast with other computational biology approaches which, while exploiting existing datasets, do not allow the data to be interpreted and analyzed in unanticipated ways. In recent years, the size and number of available biological datasets have skyrocketed.",1
Life Science,Macrobiology,Macrobiology is the branch of biology that studies macroscopic living organisms (termed macro-organisms) that can be seen by the naked eye.  It is the complement of microbiology.,1
Life Science,Marsh organ,"The marsh organ is a collection of plastic pipes attached to a wooden framework that is placed in marshes to measure water level changes for research purposes. 
The marsh organ was developed by the University of South Carolina and NOAA's National Centers for Coastal Ocean Science. Their objective was to quantify short-term and long-term effects of sea level rise on coastal processes such as plant productivity, decomposition of organic matter in soil, sedimentation that contribute to the structuring of wetland stability.",1
Life Science,Maternal recognition of pregnancy,"Maternal recognition of pregnancy is a crucial aspect of carrying a pregnancy to full term. Without maternal recognition to maintain pregnancy, the initial messengers which stop luteolysis and promote foetal implantation, growth and uterine development finish with nothing to replace them and the pregnancy is lost.  
Pregnancy maintenance relies on the continued production of progesterone which is initially produced by the corpus luteum (CL).   A hormone secreting structure that develops on the ovary after ovulation. Maternal recognition of pregnancy differs between species, however they all include a signal to prevent luteolysis, which then prevents the resumption of menstrual or oestrous cycles.
Luteolysis is the regression of the corpus luteum. The process is identified by the decline of progesterone and it signifies the absence of pregnancy following ovulation. In the non pregnant uterus, the decline of progesterone allows the return of oestrogen, resulting in the upregulation of oxytocin receptors and consequently pulsatile release of  PGF2α. In turn, luteolysis is induced. This regression allows the continuation of the menstrual cycle.
However, if pregnancy is established, luteolysis is evaded via maternal recognition of pregnancy because high levels of progesterone are maintained by the CL and the placental hormone hCG further maintains the CL.",1
Life Science,Merodiploid,"A merodiploid is a partially diploid bacterium, which has its own chromosome complement and a chromosome fragment introduced by conjugation, transformation or transduction. It can also be defined as an essentially haploid organism that carries a second copy of a part of its genome. The term is derived from the Greek, meros = part, and was originally used to describe both unstable partial diploidy, such as that which occurs briefly in recipients after mating with an Hfr strain (1), and the stable state, exemplified by F-prime strains (see Hfr'S And F-Primes). Over time the usage has tended to confine the term to descriptions of stable genetic states. Merodiploidy refers to the partial duplication of chromosomes in a haploid organism.",1
Life Science,Methyl green,"Methyl green  (CI 42585) is a cationic or positive charged stain, related to Ethyl Green, that has been used for staining DNA since the 19th century. It has been used for staining cell nuclei either as a part of the classical Unna-Pappenheim stain, or as a nuclear counterstain ever since.
In recent years, its fluorescent properties when bound to DNA have positioned it useful for far-red imaging of live cell nuclei. 
Fluorescent DNA staining is routinely used in cancer prognosis.
Methyl green also emerges as an alternative stain for DNA in agarose gels, fluorometric assays and flow cytometry. It has also been shown that it can be used as an exclusion viability stain for cells.
Its interaction with DNA has been shown to be non-intercalating, in other words not inserting itself into the DNA, but instead electrostatic with the DNA major groove. It is used in combination with pyronin in the methyl green–pyronin stain which stains and differentiates DNA and RNA.
When excited at 244 or 388 nm in neutral aqueous solution, methyl green produces a fluorescent emission at 488 or 633 nm respectively. The presence or absence of DNA does not effect these fluorescence behaviors. When binding DNA under neutral aqueous conditions, methyl green also becomes fluorescent in the far red with an excitation maximum of 633 nm and an emission maximum of 677 nm.Commercial Methyl green preparations are often contaminated with Crystal violet. Crystal violet can be removed by chloroform extraction.

",1
Life Science,Microbial rhodopsin,"Microbial rhodopsins, also known as bacterial rhodopsins are retinal-binding proteins that provide light-dependent ion transport and sensory functions in halophilic and other bacteria. They are integral membrane proteins with seven transmembrane helices, the last of which contains the attachment point (a conserved lysine) for retinal.
This protein family includes light-driven proton pumps, ion pumps and ion channels, as well as light sensors. For example, the proteins from halobacteria include bacteriorhodopsin and archaerhodopsin, which are light-driven proton pumps; halorhodopsin, a light-driven chloride pump; and sensory rhodopsin, which mediates both photoattractant (in the red) and photophobic (in the ultra-violet) responses. Proteins from other bacteria include proteorhodopsin.
Contrary to their name, microbial rhodopsins are found not only in Archaea and Bacteria, but also in Eukaryota (such as algae) and viruses; although they are rare in complex multicellular organisms.",1
Life Science,Minimally manipulated cells,"Minimally manipulated cells are cells that undergo minimal manipulation are non-cultured (non-expanded) cells isolated from the biological material by its grinding, homogenization or selective collection of cells. Minimally manipulated cells are usually using for the treatment of skin ulceration, alopecia, and arthritis.",1
Life Science,Mitochondrial fusion,"Mitochondria are dynamic organelles with the ability to fuse and divide (fission), forming constantly changing tubular networks in most eukaryotic cells. These mitochondrial dynamics, first observed over a hundred years ago are important for the health of the cell, and defects in dynamics lead to genetic disorders. Through fusion, mitochondria can overcome the dangerous consequences of genetic malfunction. The process of mitochondrial fusion involves a variety of proteins that assist the cell throughout the series of events that form this process.

",1
Life Science,Modes of toxic action,"A mode of toxic action is a common set of physiological and behavioral signs that characterize a type of adverse biological response. A mode of action should not be confused with mechanism of action, which refer to the biochemical processes underlying a given mode of action. Modes of toxic action are important, widely used tools in ecotoxicology and aquatic toxicology because they classify toxicants or pollutants according to their type of toxic action. There are two major types of modes of toxic action: non-specific acting toxicants and specific acting toxicants. Non-specific acting toxicants are those that produce narcosis, while specific acting toxicants are those that are non-narcotic and that produce a specific action at a specific target site.",1
Life Science,Natural resistance-associated macrophage protein,"Natural resistance-associated macrophage protein 2 (NRAMP 2), also known as divalent metal transporter 1 (DMT1) and divalent cation transporter 1 (DCT1), is a protein that in humans is encoded by the SLC11A2 (solute carrier family 11, member 2) gene. DMT1 represents a large family of orthologous metal ion transporter proteins that are highly conserved from bacteria to humans.As its name suggests, DMT1 binds a variety of divalent metals including cadmium (Cd2+), copper (Cu2+), and zinc (Zn2+,); however, it is best known for its role in transporting ferrous iron (Fe2+). DMT1 expression is regulated by body iron stores to maintain iron homeostasis. DMT1 is also important in the absorption and transport of manganese (Mn2+). In the digestive tract, it is located on the apical membrane of enterocytes, where it carries out H+-coupled transport of divalent metal cations from the intestinal lumen into the cell.",1
Life Science,Neontology,"Neonatology is a subspecialty of pediatrics that consists of the medical care of newborn infants, especially the ill or premature newborn. It is a hospital-based specialty, and is usually practised in neonatal intensive care units (NICUs). The principal patients of neonatologists are newborn infants who are ill or require special medical care due to prematurity, low birth weight, intrauterine growth restriction, congenital malformations (birth defects), sepsis, pulmonary hypoplasia or birth asphyxia.

",1
Life Science,Neural computation,"Neural computation is the information processing performed by networks of neurons. Neural computation is affiliated with the philosophical tradition known as Computational theory of mind, also referred to as computationalism, which advances the thesis that neural computation explains cognition. The first persons to propose an account of neural activity as being computational was Warren McCullock and Walter Pitts in their seminal 1943 paper, A Logical Calculus of the Ideas Immanent in Nervous Activity. There are three general branches of computationalism, including classicism, connectionism, and computational neuroscience. All three branches agree that cognition is computation, however they disagree on what sorts of computations constitute cognition. The classicism tradition believes that computation in the brain is digital, analogous with digital computing. Both connectionism and computational neuroscience do not require that the computations that realize cognition are necessarily digital computations. However, the two branches greatly disagree upon which sorts of experimental data should be used to construct explanatory models of cognitive phenomena. Connectionists rely upon behavioral evidence to construct models to explain cognitive phenomenon, whereas computational neuroscience leverages neuroanatomical and neurophysiological information to construct mathematical models that explain cognition.When comparing the three main traditions of the computational theory of mind, as well as the different possible forms of computation in the brain, it is helpful to define what we mean by computation in a general sense. Computation is the processing of vehicles, otherwise known as variables or entities, according to a set of rules. A rule in this sense is simply an instruction for executing a manipulation on the current state of the variable, in order to produce an specified output. In other words, a rule dictates which output to produce given a certain input to the computing system. A computing system is a mechanism whose components must be functionally organized to process the vehicles in accordance with the established set of rules. The types of vehicles processed by a computing system determine which type of computations it performs. Traditionally, in cognitive science there have been two proposed types of computation related to neural activity - digital and analog, with the vast majority of theoretical work incorporating a digital understanding of cognition. Computing systems that perform digital computation are functionally organized to execute operations on strings of digits with respect to the type and location of the digit on the string. It has been argued that neural spike train signaling implements some form of digital computation, since neural spikes may be considered as discrete units or digits, like 0 or 1 - the neuron either fires an action potential or it does not. Accordingly, neural spike trains could be seen as strings of digits. Alternatively, analog computing systems perform manipulations on non-discrete, irreducibly continuous variables, that is, entities that vary continuously as a function of time. These sorts of operations are characterized by systems of differential equations.Neural computation can be studied for example by building models of neural computation.
There is a scientific journal dedicated to this subject, Neural Computation.
Artificial neural networks (ANN) is a subfield of the research area machine learning. Work on ANNs has been somewhat inspired by knowledge of neural computation.",1
Life Science,Neuroscientist,"A neuroscientist (or neurobiologist) is a scientist who has specialised knowledge in neuroscience, a branch of biology that deals with the physiology, biochemistry, psychology, anatomy and molecular biology of neurons, neural circuits, and glial cells and especially their behavioral, biological, and psychological aspect in health and disease. 

Neuroscientists generally work as researchers within a college, university, government agency, or private industry setting. In research-oriented careers, neuroscientists typically spend their time designing and carrying out scientific experiments that contribute to the understanding of the nervous system and its function. They can engage in basic or applied research. Basic research seeks to add information to our current understanding of the nervous system, whereas applied research seeks to address a specific problem, such as developing a treatment for a neurological disorder. Biomedically-oriented neuroscientists typically engage in applied research. Neuroscientists also have a number of career opportunities outside the realm of research, including careers in industry, science writing,  government program management, science advocacy, and education. These individuals most commonly hold doctorate degrees in the sciences, but may also hold a master's degree.
The Neuroscientists day is celebrated on August 13.",1
Life Science,NicO transporters,The Nickel/Cobalt Transporter (NicO) Family is a member of the Lysine Exporter (LysE) Superfamily.,1
Life Science,Nicotinamide ribonucleoside uptake transporters,"The Nicotinamide Ribonucleoside (NR) Uptake Permease (PnuC) Family (TC# 4.B.1) is a family of transmembrane transporters that is part of the TOG superfamily. Close PnuC homologues are found in a wide range of Gram-negative and Gram-positive bacteria, archaea and eukaryotes.",1
Life Science,OrthoFinder,"OrthoFinder is a command-line software tool for comparative genomics. OrthoFinder determines the correspondence between genes in different organisms (also known as orthology analysis). This correspondence provides a framework for understanding the evolution of life on Earth, and enables the extrapolation and transfer of biological knowledge between organisms. 
OrthoFinder takes FASTA files of protein sequences as input (one per species) and as output provides:

Orthogroups
Rooted Phylogenetic trees of all orthogroups
A rooted species tree for the set of species included in the input dataset
Hierarchical orthogroups for each node in the species tree
Orthologs between all species
Gene duplication events mapped to branches in the species tree
Comparative genomic statisticsAs of August 2021, the tool has been referenced by more than 1500 published studies.

",1
Life Science,Overabundant species,"In biology, overabundant species refers to an excessive number of individuals and occurs when the normal population density has been exceeded. Increase in animal populations is influenced by a variety of factors, some of which include habitat destruction or augmentation by human activity, the introduction of invasive species and the reintroduction of threatened species to protected reserves.
Population overabundance can have a negative impact on the environment, and in some cases on the public as well. There are various methods through which populations can be controlled such as hunting, contraception, chemical controls, disease and genetic modification. Overabundant species is an important area of research as it can potentially impact the biodiversity of ecosystems.
Most research studies have examined negative impacts of overabundant species, whereas very few have documented or performed an in-depth examination on positive impacts. As a result, this article focuses on the negative impact of overabundant species.

",1
Life Science,P-Aminobenzoyl-glutamate transporter,"The p-aminobenzoyl-glutamate transporter (AbgT) family (TC# 2.A.68) is a family of transporter proteins belonging to the ion transporter (IT) superfamily. The AbgT family consists of the AbgT (YdaH; TC# 2.A.68.1.1) protein of E. coli and the MtrF drug exporter (TC# 2.A.68.1.2) of Neisseria gonorrhoeae. The former protein is apparently cryptic in wild-type cells, but when expressed on a high copy number plasmid, or when expressed at higher levels due to mutation, it appeared to allow uptake (Km = 123 nM; see Michaelis–Menten kinetics) and subsequent utilization of p-aminobenzoyl-glutamate as a source of p-aminobenzoate for p-aminobenzoate auxotrophs. p-Aminobenzoate is a constituent of and a precursor for the biosynthesis of folic acid. MtrF was annotated as a putative drug efflux pump.",1
Life Science,Parabiosis,"Parabiosis, meaning ""living beside"", is a laboratory technique to study physiology. It combines two living organisms which are joined together surgically to develop a single, shared physiological system.  Parabiosis is used in the study of areas such as obesity, biological aging, stem cell research, tissue regeneration, diabetes, organ transplantation, tumor biology, and endocrinology.It can also describe a communal ecology of separate species of ants in colonies.",1
Life Science,Peptidoglycolipid addressing protein,"The Peptidoglycolipid Addressing Protein (GAP) Family is a member of the Lysine Exporter (LysE) Superfamily. It is listed as item 2.A.116 in the Transporter Classification Database. The mechanism of its action is not known, but this family has been shown to be a member of the LsyE superfamily. Therefore, these proteins are most likely secondary carriers.
The proposed generalized reaction catalyzed by members of the GAP family is:
PGL (in) → PGL (outer membrane).",1
Life Science,Photoautotrophism,"Photoautotrophs are organisms that use light energy and inorganic carbon to produce organic materials. Eukaryotic photoautotrophs absorb energy through the chlorophyll molecules in their chloroplasts while prokaryotic photoautotrophs use chlorophylls and bacteriochlorophylls present in their cytoplasm. All known photoautotrophs perform photosynthesis. Examples include plants, algae, and cyanobacteria.",1
Life Science,Photothermal ratio,"The photothermal ratio (PTR), also named photothermal quotient,  is a variable that characterizes the amount of light available to plants relative to the temperature level. It is used in plant biology to characterize the growth environment of plants.

",1
Life Science,Phylogenetic inertia,"Phylogenetic inertia or phylogenetic constraint refers to the limitations on the future evolutionary pathways that have been imposed by previous adaptations.Charles Darwin first recognized this phenomenon, though the term was later coined by Huber in 1939. Darwin explained the idea of phylogenetic inertia based on his observations; he spoke about it when explaining the ""Law of Conditions of Existence"". Darwin also suggested that, after speciation, the organisms do not start over from scratch, but have characteristics that are built upon already existing ones that were inherited from their ancestors; and these characteristics likely limit the amount of evolution seen in that new taxa. This is the main concept of phylogenetic inertia.
Richard Dawkins also explained these constraints by likening natural selection to a river in his 1982 book The Extended Phenotype.

",1
Life Science,Phytogeomorphology,"Phytogeomorphology is the study of how terrain features affect plant growth. It was the subject of a treatise by Howard and Mitchell in 1985, who were considering the growth and varietal temporal and spatial variability found in forests, but recognized that their work also had application to farming, and the relatively new science (at that time) of precision agriculture.  The premise of Howard and Mitchell is that landforms, or features of the land's 3D topography significantly affect how and where plants (or trees in their case) grow.  Since that time, the ability to map and classify landform shapes and features has increased greatly.  The advent of GPS has made it possible to map almost any variable one might wish to measure.  Thus, a very increased awareness of the spatial variability of the environment that plants grow in has arisen.  The development of technology like airborne LiDAR has enabled the detailed measurement of landform features to better than sub-meter, and when combined with RTK-GPS (accuracies to 1mm) enables the creation of very accurate maps of where these features are.  Comparison of these landform maps with mapping of variables related to crop or plant growth show a strong correlation (see below for examples and references for precision agriculture).",1
Life Science,Pinch-induced behavioral inhibition,"Pinch-induced behavioural inhibition (PIBI), also called dorsal immobility, transport immobility or clipnosis, is a partially inert state which results from a gentle squeeze of the skin behind the neck.  It is mostly observed among cats and allows a female cat to carry her kitten easily with her jaws. It can be used to restrain most cats effectively in a domestic or veterinary context. The phenomenon also occurs in other animals, such as squirrels and mice.",1
Life Science,PITPNM3,"Nir1 or Membrane-associated phosphatidylinositol transfer protein 3 (PITPNM3) is a mammalian protein that localizes to endoplasmic reticulum (ER) and plasma membrane (PM) membrane contact sites (MCS) and aids the transfer of phosphatidylinositol between these two membranes, potentially by recruiting additional proteins to the ER-PM MCS.",1
Life Science,Potassium spatial buffering,"Potassium spatial buffering is a mechanism for the regulation of extracellular potassium concentration by astrocytes. Other mechanisms for astrocytic potassium clearance are carrier-operated or channel-operated potassium chloride uptake.
The repolarization of neurons tends to raise potassium concentration in the extracellular fluid.  If a significant rise occurs, it will interfere with neuronal signaling by depolarizing neurons. Astrocytes have large numbers of potassium ion channels facilitating removal of potassium ions from the extracellular fluid. They are taken up at one region of the astrocyte and then distributed throughout the cytoplasm of the cell, and further to its neighbors via gap junctions. This keeps extracellular potassium at levels that prevent interference with normal propagation of an action potential.

",1
Life Science,Protein arginine phosphatase,"Protein Arginine Phosphatase (PAPs), also known as Phosphoarginine Phosphatase, is an enzyme that catalyzes the dephosphorylation of phosphoarginine residues in proteins. Protein phosphatases (PPs) are ""obligatory heteromers"" made up of two maximum catalytic subunits attached to a non-catalytic subunit. Arginine modification is a post-translational protein modification in gram-positive bacteria. McsB and YwIE were recently identified as phosphorylating enzymes in Bacillus Subtilis (B.Subtilis). YwIE was thought to be a protein-tyrosine-phosphatase, and McsB a tyrosine-kinase, however in 2012 Elsholz et al. showed that McsB is a protein-arginine-kinase (PAK) and YwlE is a phosphatase-arginine-phosphatase (PAP).
Many proteins rely on protein phosphatase activity for regulating their stability, localization, and interaction with other proteins. Arginine modification is a post-translational protein modification in gram-positive bacteria, and protein arginine phosphorylation regulates transcription factors, in addition to tagging rogue proteins for degradation in gram-positive bacteria. Like phosphorylation, dephosphorylation is a reversible post-translational event. It is reversible through the action of kinases (enzymes that adds a phosphate group to a protein via phosphorylation), and this antagonist activity of phosphorylation and dephosphorylation of proteins controls all aspect of prokaryotic and eukaryotic life. In general, protein phosphatases play a crucial role in cell signaling regulation in both eukaryotes and prokaryotes. They act by removing a phosphate group from proteins, and their activity counteracts that of protein kinases.",1
Life Science,Pseudohypoxia,"Pseudohypoxia refers to increased cytosolic ratio of free NAD to NADH in cells, caused by hyperglycemia.
Research has shown that declining levels of NAD+ during aging cause pseudohypoxia, and that raising nuclear NAD+ in old mice reverses pseudohypoxia and metabolic dysfunction, thus reversing the aging process. It is expected that human NAD trials will begin in 2014.Pseudohypoxia is a feature commonly noted in poorly-controlled diabetes.",1
Life Science,Pursuit predation,"Pursuit predation is a form of predation in which predators actively give chase to their prey, either solitarily or as a group.  It is an alternate predation strategy to ambush predation — pursuit predators rely on superior speed, endurance and/or teamwork to seize the prey, while ambush predators use stealth, luring, the use of surroundings and the element of surprise to capture the prey.  While the two patterns of predation are not mutually exclusive, morphological differences in an organism's body plan can create an evolutionary bias favoring either type of predation.
Pursuit predation is typically observed in carnivorous species within the kingdom Animalia, such as cheetahs, lions, wolves and early Homo species.  The chase can be initiated either by the predator, or by the prey if it is alerted to a predator's presence and attempt to flee before the predator gets close.  The chase ends either when the predator successfully catches up and tackles the prey, or when the predator abandons the attempt after the prey outruns it and escapes.
One particular form of pursuit predation is persistence hunting, where the predator stalks the prey slowly but persistently to wear it down physically with fatigue or overheating; some animals are examples of both types of pursuit.",1
Life Science,Radioactivity in the life sciences,"Radioactivity is generally used in life sciences for highly sensitive and direct measurements of biological phenomena, and for visualizing the location of biomolecules radiolabelled with a radioisotope.
All atoms exist as stable or unstable isotopes and the latter decay at a given half-life ranging from attoseconds to billions of years; radioisotopes useful to biological and experimental systems have half-lives ranging from minutes to months.  In the case of the hydrogen isotope tritium (half-life = 12.3 years) and carbon-14 (half-life = 5,730 years), these isotopes derive their importance from all organic life containing hydrogen and carbon and therefore can be used to study countless living processes, reactions, and phenomena.  Most short lived isotopes are produced in cyclotrons, linear particle accelerators, or nuclear reactors and their relatively short half-lives give them high maximum theoretical specific activities which is useful for detection in biological systems.

  
Radiolabeling is a technique used to track the passage of a molecule that incorporates a radioisotope through a reaction, metabolic pathway, cell, tissue, organism, or biological system. The reactant is 'labeled' by replacing specific atoms by their isotope.  Replacing an atom with its own radioisotope is an intrinsic label that does not alter the structure of the molecule.  Alternatively, molecules can be radiolabeled by chemical reactions that introduce an atom, moiety, or functional group that contains a radionuclide.  For example, radio-iodination of peptides and proteins with biologically useful iodine isotopes is easily done by an oxidation reaction that replaces the hydroxyl group with iodine on tyrosine and histadine residues.  Another example is to use chelators such DOTA that can be chemically coupled to a protein; the chelator in turn traps radiometals thus radiolabeling the protein.  This has been used for introducing Yttrium-90 onto a monoclonal antibody for therapeutic purposes and for introducing Gallium-68 onto the peptide Octreotide for diagnostic imaging by PET imaging.  (See DOTA uses.)
Radiolabeling is not necessary for some applications.  For some purposes,  soluble ionic salts can be used directly without further modification (e.g., gallium-67, gallium-68, and radioiodine isotopes).  These uses rely on the chemical and biological properties of the radioisotope itself, to localize it within the organism or biological system.
Molecular imaging is the biomedical field that employs radiotracers to visualize and quantify biological processes using positron emission tomography (PET) and single-photon emission computed tomography (SPECT) imaging.  Again, a key feature of using radioactivity in life science applications is that it is a quantitative technique, so PET/SPECT not only reveals where a radiolabelled molecule is but how much is there.
Radiobiology (also known as radiation biology) is a field of clinical and basic medical sciences that involves the study of the action of radioactivity on biological systems.  The controlled action of deleterious radioactivity on living systems is the basis of radiation therapy.",1
Life Science,RAN translation,"Repeat Associated Non-AUG translation, or RAN translation, is an irregular mode of mRNA translation that can occur in eukaryotic cells.

",1
Life Science,Sequence graph,"Sequence graph, also called an alignment graph, breakpoint graph, or adjacency graph, are bidirected graphs used in comparative genomics. The structure consists of multiple graphs or genomes with a series of edges and vertices represented as adjacencies between segments in a genome and DNA segments respectively. Traversing a connected component of segments and adjacency edges (called a thread) yields a sequence, which typically represents a genome or a section of a genome. The segments can be thought of as synteny blocks, with the edges dictating how to arrange these blocks in a particular genome, and the labelling of the adjacency edges representing bases that are not contained in synteny blocks.",1
Life Science,Sessility (motility),"Sessility is the biological property of an organism describing its lack of a means of self-locomotion. Sessile organisms for which natural motility is absent are normally immobile. This is distinct from the botanical concept of sessility, which refers to an organism or biological structure attached directly by its base without a stalk.
Sessile organisms can move via external forces (such as water currents), but are usually permanently attached to something. Organisms such as corals lay down their own substrate from which they grow. Other sessile organisms grow from a solid such as a rock, dead tree trunk, or a man-made object such as a buoy or ship's hull.

",1
Life Science,Sociobiology,"Sociobiology is a field of biology that aims to examine and explain social behavior in terms of evolution. It draws from disciplines including psychology, ethology, anthropology, evolution, zoology, archaeology, and population genetics. Within the study of human societies, sociobiology is closely allied to evolutionary anthropology, human behavioral ecology, evolutionary psychology, and sociology.Sociobiology investigates social behaviors such as mating patterns, territorial fights, pack hunting, and the hive society of social insects. It argues that just as selection pressure led to animals evolving useful ways of interacting with the natural environment, so also it led to the genetic evolution of advantageous social behavior.While the term ""sociobiology"" originated at least as early as the 1940s, the concept did not gain major recognition until the publication of E. O. Wilson's book Sociobiology: The New Synthesis in 1975. The new field quickly became the subject of controversy.  Critics, led by Richard Lewontin and Stephen Jay Gould, argued that genes played a role in human behavior, but that traits such as aggressiveness  could be explained by social environment rather than by biology. Sociobiologists responded by pointing to the complex relationship between nature and nurture.

",1
Life Science,Solenocyte,"In biology, solenocytes are flagellated cells associated with excretion, osmoregulation and ionoregulation in many animals and in some chordates under the sub-phylum Cephalochordata.
These are the cells which form subtypes of protonephridium along with the other type i.e. flame cells. Flame cells can be distinguished from solenocytes as the former is usually ciliated whereas the latter is flagellated.
An example of organisms in which excretion is performed by solenocytic protonephridia is the genus Branchiostoma.",1
Life Science,Stanislav Drobyshevsky,"Stanislav Vladimirovich Drobyshevsky (born 2 July 1978, Chita) is a Russian anthropologist and science popularizer. He is a Candidate of Sciences and works at the Anthropology department of the Faculty of Biology of Moscow State University. He is a scientific editor of the popular science portal Antropogenez.ru. His body of work includes monographs, university textbooks, and popular science books. He is a vlogger and YouTuber. He also frequently appears in other Russian popular science channels.

",1
Life Science,Stercomata,"Stercomata (or stercomes) are extracellular pellets of waste material produced by some groups of foraminiferans, including xenophyophoreans and komokiaceans, Gromia, and testate amoebae. The pellets are ovoid (egg-shaped), brownish in color, and on average measure from 10-20 µm in length. Stercomata are composed of small mineral grains and undigested waste products held together by strands of glycosaminoglycans.The term “sterkome” was first used Schaudinn in 1899 to describe the balls of undigested food remains produced by the testate amoeba Trichosphaerium sieboldi, the foraminiferan Saccammina sphaerica, and the gromiid Gromia dujardinii. Schaudinn conducted feeding experiments on live individuals of Trichosphaerium sieboldi kept in culture dishes to confirm that stercomata were accumulations of waste material produced as a byproduct of feeding.

",1
Life Science,Sulfide intrusion,"In ecology, sulfide intrusion refers to an excess of sulfide molecules (S2-) in the soil that interfere with plant growth, often seagrass.Seagrass bed sediment (soil) is typically anoxic, containing a reduced form of sulfur: hydrogen sulfide (H2S).  H2S is a phytotoxin that results from anaerobic digestion, the decomposition of organic matter in the absence of oxygen.  However, seagrass can persist in this environment because of physiological adaptations, as well as functional adaptations of other organisms in the ecosystem.  For example, bivalves (clams) in the family Lucinidae host symbiotic bacteria that oxidize sulfides.  Lucinid bivalves' gills house the bacteria, and the siphon supplies the bacteria and surrounding pore water with oxygenated water from above the sediment.  Bacterial oxidation of the sulfides results in sulfates, reducing toxicity.

",1
Life Science,Sulphide Indole Motility medium,"Sulphide Indole Motility (SIM) medium is a bacterial growth medium which tests for the ability to reduce sulfates, the ability to produce indoles, and motility. This combination of challenges in one mixture is convenient and commercially available in stab tubes. Inoculated needles are then punctured into the culture and incubated, if the culture becomes cloudy the bacteria were able to infiltrate the media and survive. This method is particularly useful for pathogenic bacteria which are dangerous to handle on wet mount slides",1
Life Science,Synthetic microbial consortia,"Synthetic microbial consortia (commonly called co-cultures) are multi-population systems that can contain a diverse range of microbial species, and are adjustable to serve a variety of industrial, ecological, and tautological interests.  For synthetic biology, consortia take the ability to engineer novel cell behaviors to a population level.
Consortia are more common than not in nature, and generally prove to be more robust than monocultures. Just over 7,000 species of bacteria have been cultured and identified to date. Many of the estimated 1.2 million bacteria species that remain have yet to be cultured and identified, in part due to inabilities to be cultured axenically. Evidence for symbiosis between microbes strongly suggests it to have been a necessary precursor of the evolution of land plants and for their transition from algal communities in the sea to land. When designing synthetic consortia, or editing naturally occurring consortia, synthetic biologists keep track of pH, temperature, initial metabolic profiles, incubation times, growth rate, and other pertinent variables.

",1
Life Science,Syntrophy,"In biology, syntrophy, synthrophy, or cross-feeding (from Greek syn meaning together, trophe meaning nourishment) is the phenomenon of one species living off the metabolic products of another species. In this type of biological interaction, the growth of one partner depends on the nutrients, growth factors, or substrates provided by the other partner. Jan Dolfing describes syntrophy as ""the critical interdependency between producer and consumer"". This term for nutritional interdependence is often used in microbiology to describe this symbiotic relationship between bacterial species. Morris et al. have described the process as ""obligately mutualistic metabolism"".",1
Life Science,Tachyaerobic,"Tachyaerobic is a term used in biology to describe the muscles of large animals and birds that are able to maintain high levels or physical activity because their hearts make up at least 0.5-0.6 percent of their body mass and maintain high blood pressures.  A reptile displaying equal size to a tachyaerobic mammal does not have the same capabilities.  Tachyaerobic animals' hearts beat more quickly, produce more oxygen, and distribute blood at a quicker rate than reptiles.
The use of tachyaerobic muscles is important to animals such as giraffes that need blood circulated through a large body size quickly.",1
Life Science,Tactoid,"Tactoids are liquid crystal microdomains nucleated in isotropic phases, which can be distinguished as spherical or spindle-shaped birefringent microdroplets under polarized light microscopy. Tactoids are a transition state between isotropic and macroscopic liquid crystalline phases. The first observation of tactoids was made by Zocher in 1925, when he studied the nematic phase formed in vanadium pentoxide sols. After that, tactoids have been found in the phase transition processes in many lyotropic liquid crystalline substances, such as tobacco mosaic virus, polypeptides, and cellulose nanocrystals.",1
Life Science,Tellurium ion resistance,The Tellurium Ion Resistance (TerC) Family (TC# 2.A.109) is part of the Lysine Exporter (LysE) Superfamily. A representative list of proteins belonging to the TerC family can be found in the Transporter Classification Database.,1
Life Science,Thermophyte,"Thermophyte (Greek thérmos = warmth, heat + phyton = plant)  is an organism which is tolerant or thriving  at high temperatures. These organisms are categorized according to ecological valences at high temperatures, including biological extremely. Such organisms included the hot-spring taxa also.A large amount of thermophytes are algae, more specifically blue-green algae, also referred to as cyanobacteria. This type of algae thrives in hot conditions ranging anywhere from 50 to 70 degrees Celsius, which other plants and organisms cannot survive in. Thermophytes are able to survive extreme temperatures as their cells contain an “unorganized nucleus”.
As the name suggests, thermophytes are found in high temperatures. They can be found in abundance in and around places like freshwater hot springs, such as YellowStone National park and in the Lassen Volcanic National park.",1
Life Science,Third-generation sequencing,"Third-generation sequencing (also known as long-read sequencing) is a class of DNA sequencing methods currently under active development.Third generation sequencing technologies have the capability to produce substantially longer reads than second generation sequencing. Such an advantage has critical implications for both genome science and the study of biology in general. However, third generation sequencing data have much higher error rates than previous technologies, which can complicate downstream genome assembly and analysis of the resulting data. These technologies are undergoing active development and it is expected that there will be improvements to the high error rates. For applications that are more tolerant to error rates, such as structural variant calling, third generation sequencing has been found to outperform existing methods, even at a low depth of sequencing coverage.",1
Life Science,Tokogeny,"Tokogeny or tocogeny is the biological relationship between parent and offspring, or more generally between ancestors and descendants.  In contradistinction to phylogeny it applies to individual organisms as opposed to species.
In the tokogentic system shared characteristics are called traits.",1
Life Science,Trabecular cartilage,"Trabecular cartilages (trabeculae cranii, sometimes simply trabeculae, prechordal cartilages) are paired, rod-shaped cartilages, which develop in the head of the vertebrate embryo. They are the primordia of the anterior part of the cranial base, and are derived from the cranial neural crest cells.",1
Life Science,Trace metal,"Trace metals are the metals subset of trace elements; that is, metals normally present in small but measurable amounts in animal and plant cells and tissues and that are a necessary part of nutrition and physiology. Many biometals are trace metals. Ingestion of, or exposure to, excessive quantities can be toxic. However, insufficient plasma or tissue levels of certain trace metals can cause pathology, as is the case with iron.
Trace metals within the human body include iron, lithium, zinc, copper, chromium, nickel, cobalt, vanadium, molybdenum, manganese and others.Trace metals are metals needed by living organisms to function properly and are depleted through the expenditure of energy by various metabolic processes of living organisms. They are replenished in animals through diet as well as environmental exposure, and in plants through the uptake of nutrients from the soil in which the plant grows. Human vitamin pills and plant fertilizers can be a source of trace metals.
Trace metals are sometimes referred to as trace elements, although the latter includes minerals and is a broader category. See also Dietary mineral. Trace elements are required by the body for specific functions. Things such as vitamins, sports drinks, fresh fruits and vegetables are sources. Taken in excessive amounts, trace elements can cause problems. For example fluorine is required for the formation of bones and enamel on teeth. However, when taken in an excessive amount can cause a disease called ""Fluorosis', in which bone deformations and yellowing of teeth are seen. Fluorine can occur naturally in some areas in ground water.
Trace metals include iron that can help to prevent anemia, and zinc that is a cofactor in over 100 enzyme reactions.",1
Life Science,Trans-Golgi network vesicle protein 23 A,"Trans-Golgi network vesicle protein 23 A (TVP23A) is a protein coded for the TVP23A gene, formerly known as FAM18A. TVP23A is located on chromosome 16.  It is known to have human paralogs, TVP23B and TVP23C, as well as orthologs in many different species, notably yeast, mice, and chickens. The general consensus on the TVP23A protein indicate that it has some function in the late Golgi apparatus and is involved in retrograde transport from endosomes back into the Golgi apparatus. The nature of this transport is still unknown.",1
Life Science,Transboundary breed,"A transboundary breed is a breed which is present in several countries. Transboundary species of the five significant livestock types (cattle, sheep, goats, pigs and chickens), have been developed for a hundred years or more in intensive manufacturing systems, which has led to global availability.  A relatively small number of worldwide transboundary breeds compose the ever-increasing share of total global animal products. However, only in North America and the Southwest Pacific do the number of transboundary breeds surpass that of local breeds.There can be both regional and international types of transboundary breeds. Regional breeds are breeds that are reported to only be found in one ""region"", which may include several countries, and an international transboundary breed is one that is reported to be found in multiple regions. For example, the Holstein Fresian cattle is an international transboundary breed, because it is found in several different continents and regions.",1
Life Science,Triploid block,"Triploid block is a phenomenon describing the formation of nonviable progeny after hybridization of flowering plants that differ in ploidy. The barrier is established in the endosperm, a nutritive tissue supporting embryo growth. This phenomenon usually happens when autopolyploidy occurs in diploid plants. Triploid blocks lead to reproductive isolation. The triploid block effects have been explained as possibly due to genomic imprinting in the endosperm.",1
Life Science,TSUP family,"The 4-Toluene Sulfonate Uptake Permease (TSUP) family (TC# 2.A.102) is also referred to as the TauE/SafE/YfcA/DUF81 Family. 
Although its members have not been rigorously characterized, evidence is available that at least some members function in the transport of Sulfur containing Organic compounds.  These include 4-toluene sulfonate which may be transported by the TsaS of Cupriavidus necator (TC# 2.A.102.1.1), sulfolactate which may be exported by the TauE protein of Cupriavidus necator (TC# 2.A.102.2.1)  and sulfoacetate which may be exported by the SafE1 protein of Neptuniibacter caesariensis (TC# 2.A.102.2.2).  Another member of the TSUP family, TsaS of Comamonas testosteroni, has been reported to function in the uptake of 4-toluene sulfonate.  None of these functional assignments can be considered to be certain.",1
Life Science,Unconventional protein secretion,"Unconventional protein secretion (known as ER/Golgi-independent protein secretion or nonclassical protein export ) represents a manner in which the proteins are delivered to the surface of plasma membrane or extracellular matrix independent of the endoplasmic reticulum or Golgi apparatus. This includes cytokines and mitogens with crucial function in complex processes such as inflammatory response or tumor-induced angiogenesis. Most of these proteins are involved in processes in higher eukaryotes, however an unconventional export mechanism was found in lower eukaryotes too. Even proteins folded in their correct conformation can pass plasma membrane this way, unlike proteins transported via ER/Golgi pathway.  Two types of unconventional protein secretion are these: signal-peptid-containing proteins and cytoplasmatic and  nuclear proteins that are missing  an ER-signal peptide (1).

",1
Life Science,Universality–diversity paradigm,"The universality–diversity paradigm is the analysis of biological materials based on the universality and diversity of its fundamental structural elements and functional mechanisms. The analysis of biological systems based on this classification has been a cornerstone of modern biology.
For example, proteins constitute the elementary building blocks of a vast variety of biological materials such as cells, spider silk or bone, where they create extremely robust, multi-functional materials by self-organization of structures over many length- and time scales, from nano to macro. Some of the structural features are commonly found in many different tissues, that is, they are conservation|highly conserved. Examples of such universal building blocks include alpha-helices, beta-sheets or tropocollagen molecules. In contrast, other features are highly specific to tissue types, such as particular filament assemblies, beta-sheet nanocrystals in spider silk or tendon fascicles. This coexistence of universality and diversity—referred to as the universality–diversity paradigm (UDP)—is an overarching feature in biological materials and a crucial component of materiomics. It might provide guidelines for bioinspired and biomimetic material development, where this concept is translated into the use of inorganic or hybrid organic-inorganic building blocks.",1
Life Science,Use of beta-adrenergic agonists livestock,"Beta2-adrenergic agonists, also known as adrenergic β2 receptor agonists, are a class of drugs that act on the β2 adrenergic receptor. Like other β adrenergic agonists, they cause smooth muscle relaxation. β2 adrenergic agonists' effects on smooth muscle cause dilation of bronchial passages, vasodilation in muscle and liver, relaxation of uterine muscle, and release of insulin. They are primarily used to treat asthma and other pulmonary disorders, such as Chronic obstructive pulmonary disease (COPD).",1
Life Science,Biotechnology,"Biotechnology is ""the integration of natural sciences and engineering sciences in order to achieve the application of organisms, cells, parts thereof and molecular analogues for products and services"". The term biotechnology was first used by Károly Ereky in 1919, meaning the production of products from raw materials with the aid of living organisms.",1
Life Science,History of biotechnology,"Biotechnology is the application of scientific and engineering principles to the processing of materials by biological agents to provide goods and services. From its inception, biotechnology has maintained a close relationship with society. Although now most often associated with the development of drugs, historically biotechnology has been principally associated with food, addressing such issues as malnutrition and famine. The history of biotechnology begins with zymotechnology, which commenced with a focus on brewing techniques for beer. By World War I, however, zymotechnology would expand to tackle larger industrial issues, and the potential of industrial fermentation gave rise to biotechnology. However, both the single-cell protein and gasohol projects failed to progress due to varying issues including public resistance, a changing economic scene, and shifts in political power.
Yet the formation of a new field, genetic engineering, would soon bring biotechnology to the forefront of science in society, and the intimate relationship between the scientific community, the public, and the government would ensue. These debates gained exposure in 1975 at the Asilomar Conference, where Joshua Lederberg was the most outspoken supporter for this emerging field in biotechnology. By as early as 1978, with the development of synthetic human insulin, Lederberg's claims would prove valid, and the biotechnology industry grew rapidly. Each new scientific advance became a media event designed to capture public support, and by the 1980s, biotechnology grew into a promising real industry. In 1988, only five proteins from genetically engineered cells had been approved as drugs by the United States Food and Drug Administration (FDA), but this number would skyrocket to over 125 by the end of the 1990s.
The field of genetic engineering remains a heated topic of discussion in today's society with the advent of gene therapy, stem cell research, cloning, and genetically modified food. While it seems only natural nowadays to link pharmaceutical drugs as solutions to health and societal problems, this relationship of biotechnology serving social needs began centuries ago.",1
Life Science,Index of biotechnology articles,"This page provides an alphabetical list of articles and other pages (including categories, lists, etc.) about biotechnology.",1
Life Science,Outline of biotechnology,"The following outline is provided as an overview of and topical guide to biotechnology:
Biotechnology – field of applied biology that involves the use of living organisms and bioprocesses in engineering, technology, medicine and other fields requiring bioproducts. Biotechnology also utilizes these products for manufacturing purposes.",1
Life Science,2 base encoding,"2 Base Encoding, also called SOLiD (sequencing by oligonucleotide ligation and detection), is a next-generation sequencing technology developed by Applied Biosystems and has been commercially available since 2008. These technologies generate hundreds of thousands of small sequence reads at one time. Well-known examples of such DNA sequencing methods include 454 pyrosequencing (introduced in 2005), the Solexa system (introduced in 2006) and the SOLiD system (introduced in 2007). These methods have reduced the cost from $0.01/base in 2004 to nearly $0.0001/base in 2006 and increased the sequencing capacity from 1,000,000 bases/machine/day in 2004 to more than 100,000,000 bases/machine/day in 2006.
2-base encoding is based on ligation sequencing rather than sequencing by synthesis. However, instead of using fluorescent labeled 9-mer probes that distinguish only 6 bases, 2-base encoding takes advantage of fluorescent labeled 8-mer probes that distinguish the two 3 prime most bases but can be cycled similar to the Macevicz method, thus greater than 6bp reads can be obtained (25-50bp published, 50bp in NCBI in Feb 2008). The 2 base encoding enables reading each base twice without performing twice the work.",1
Life Science,3DISCO,"3DISCO (stands for “3D imaging of solvent-cleared organs“) is histological method which make biological samples more transparent (so called “cleared”), by using series of organic solvents for matching refractive index (RI) of tissue and surrounding medium.  Structures in transparent tissues can be examined by fluorescence microscopy without need for time-consuming physical sectioning and following reconstruction in silico.
The method was developed by team around Ali Ertürk and Hans-Ulrich Dodt from Max-Planck-Institute in Munich  primarily for clearing and imaging unsectioned mouse brain and spinal cord. Later on the method or its modifications were successfully used in many fields of biological research to  image and investigate whole body of mouse,  structure and function of mouse brain, stem cells, tumor tissues, developmental processes or whole human embryos.",1
Life Science,100K Pathogen Genome Project,"The 100K Pathogen Genome Project was launched in July 2012 by Bart Weimer (UC Davis) as an academic, public, and private partnership. It aims to sequence the genomes of 100,000 infectious microorganisms to create a database of bacterial genome sequences for use in public health, outbreak detection, and bacterial pathogen detection. This will speed up the diagnosis of foodborne illnesses and shorten infectious disease outbreaks.The 100K Pathogen Genome Project is a public-private collaborative project to sequence the genomes of 100,000 infectious microorganisms. The 100K Genome Project will provide a roadmap for developing tests to identify pathogens and trace their origins more quickly.
Partners announced in the launch of the project were UC Davis, Agilent Technologies, and the US Food and Drug Administration, with the US Centers for Disease Control and Prevention and the US Department of Agriculture noted as collaborators. As the project has proceeded, the partnership has evolved to include or replace these founding partners. The 100K Pathogen Genome Project was selected by the IBM/Mars Food Safety Consortium for metagenomic sequences.The 100K Pathogen Genome Project is conducting high-throughput next-generation sequencing (NGS) to investigate the genomes of targeted microorganisms, with whole genome sequencing to be carried out on a small number of microorganisms for use as a reference genome. Most bacterial strains will be sequenced and assembled as draft genomes; however, the project has also produced closed genomes for a variety of enteric pathogens in the 100K bioproject.This strategy enables worldwide collaboration to identify sets of genetic biomarkers associated with important pathogen traits. This five-year microbial pathogen project will result in a free, public database containing the sequence information for each pathogen's genome. The completed gene sequences will be stored in the National Institutes of Health (NIH)'s National Center for Biotechnology Information (NCBI)'s public database. Using the database, scientists will be able to develop new methods of controlling disease-causing bacteria in the food chain.",1
Life Science,ABI Solid Sequencing,"SOLiD  (Sequencing by Oligonucleotide Ligation and Detection) is a next-generation DNA sequencing technology developed by Life Technologies and has been commercially available since 2006. This next generation technology generates 108 - 109 small sequence reads at one time. It uses 2 base encoding to decode the raw data generated by the sequencing platform into sequence data.
This method should not be confused with ""sequencing by synthesis,"" a principle used by Roche-454 pyrosequencing (introduced in 2005, generating millions of 200-400bp reads in 2009), and the Solexa system (now owned by Illumina) (introduced in 2006, generating hundreds of millions of 50-100bp reads in 2009)
These methods have reduced the cost from $0.01/base in 2004 to nearly $0.0001/base in 2006 and increased the sequencing capacity from 1,000,000 bases/machine/day in 2004 to more than 5,000,000,000 bases/machine/day in 2009. Over 30 publications exist describing its use first for nucleosome positioning from Valouev et al., transcriptional profiling or strand sensitive RNA-Seq  with Cloonan et al., single cell transcriptional profiling with Tang et al. and ultimately human resequencing with McKernan et al.The method used by this machine (sequencing-by-ligation) has been reported to have some issue sequencing palindromic sequences.

",1
Life Science,Adult stem cell,"Adult stem cells are undifferentiated cells, found throughout the body after development, that multiply by cell division to replenish dying cells and regenerate damaged tissues. Also known as somatic stem cells (from Greek σωματικóς, meaning of the body), they can be found in juvenile, adult animals, and humans, unlike embryonic stem cells. 
Scientific interest in adult stem cells is centered around two main characteristics. The first of which, being their ability to divide or self-renew indefinitely, and secondly, their ability to generate all the cell types of the organ from which they originate, potentially regenerating the entire organ from a few cells. Unlike embryonic stem cells, the use of human adult stem cells in research and therapy is not considered to be controversial, as they are derived from adult tissue samples rather than human embryos designated for scientific research. The main functions of adult stem cells are to replace cells that are at risk of possibly dying as a result of disease or injury and to maintain a state of homeostasis within the cell. There are three main methods to determine if the adult stem cell is capable of becoming a specialized cell. The adult stem cell can be labeled in vivo and tracked, it can be isolated and then transplanted back into the organism, and it can be isolated in vivo and manipulated with growth hormones.  They have mainly been studied in humans and model organisms such as mice and rats.",1
Life Science,Agricultural biotechnology,"Agricultural biotechnology, also known as agritech, is an area of agricultural science involving the use of scientific tools and techniques, including genetic engineering, molecular markers, molecular diagnostics, vaccines, and tissue culture, to modify living organisms: plants, animals, and microorganisms. Crop biotechnology is one aspect of agricultural biotechnology which has been greatly developed upon in recent times. Desired trait are exported from a particular species of Crop to an entirely different species. These transgene crops possess desirable characteristics in terms of flavor, color of flowers, growth rate, size of harvested products and resistance to diseases and pests.",1
Life Science,Agrobacterium,"Agrobacterium is a genus of Gram-negative bacteria established by H. J. Conn that uses horizontal gene transfer to cause tumors in plants. Agrobacterium tumefaciens is the most commonly studied species in this genus. Agrobacterium is well known for its ability to transfer DNA between itself and plants, and for this reason it has become an important tool for genetic engineering.

",1
Life Science,Agroinfiltration,"Agroinfiltration is a method used in plant biology and especially lately in plant biotechnology to induce transient expression of genes in a plant, or isolated leaves from a plant, or even in cultures of plant cells, in order to produce a desired protein. In the method, a suspension of Agrobacterium tumefaciens is introduced into a plant leaf by direct injection or by vacuum infiltration, or brought into association with plant cells immobilised on a porous support (plant cell packs), whereafter the bacteria transfer the desired gene into the plant cells via transfer of T-DNA. The main benefit of agroinfiltration when compared to the more traditional plant transformation is speed and convenience, although yields of the recombinant protein are generally also higher and more consistent.
The first step is to introduce a gene of interest to a strain of Agrobacterium tumefaciens. Subsequently, the strain is grown in a liquid culture and the resulting bacteria are washed and suspended into a suitable buffer solution. For injection, this solution is then placed in a syringe (without a needle). The tip of the syringe is pressed against the underside of a leaf while simultaneously applying gentle counterpressure to the other side of the leaf. The Agrobacterium suspension is then injected into the airspaces inside the leaf through stomata, or sometimes through a tiny incision made to the underside of the leaf.
Vacuum infiltration is another way to introduce Agrobacterium deep into plant tissue.  In this procedure, leaf disks, leaves, or whole plants are submerged in a beaker containing the solution, and the beaker is placed in a vacuum chamber.  The vacuum is then applied, forcing air out of the  intercellular spaces within the leaves via the stomata.  When the vacuum is released, the pressure difference forces the ""Agrobacterium"" suspension into the leaves through the stomata into the mesophyll tissue.  This can result in nearly all of the cells in any given leaf being in contact with the bacteria.
Once inside the leaf the Agrobacterium remains in the intercellular space and transfers the gene of interest as part of the Ti plasmid-derived T-DNA in high copy numbers into the plant cells. The gene transfer occurs when the plant signals are induced and physical contact is made between the plant cells and the bacteria. The bacteria create a mechanism that burrows a hole  and transfers the new T-DNA strand into the plant cell. The T-DNA moves into the nucleus of the plant and begins to integrate into the plants' chromosome.  The gene is then transiently expressed through RNA synthesis from appropriate promoter sequences in all transfected cells (no selection for stable integration is performed). The plant can be monitored for a possible effect in the phenotype, subjected to experimental conditions or harvested and used for purification of the protein of interest. Many plant species can be processed using this method, but the most common ones are Nicotiana benthamiana and less often, Nicotiana tabacum.
Transient expression in cultured plant cell packs is a new procedure, recently patented by the Fraunhofer Institute IVV, Germany.  For this technique, suspension cultured cells of tobacco (e.g.: NT1 or BY2 cell lines of Nicotiana tabacum) are immobilised by filtration onto a porous support to form a well-aerated cell pack, then incubated with recombinant Agrobacterium for a time to allow T-DNA transfer, before refiltration to remove excess bacteria and liquid.  Incubation of the cell pack in a humid environment for time periods up to several days allows transient expression of protein.  Secreted proteins can be washed out of the cell pack by application of buffer and further filtration.

",1
Life Science,Algae bioreactor,"An algae bioreactor is used for cultivating micro or macro algae. Algae may be cultivated for the purposes of biomass production (as in a seaweed cultivator), wastewater treatment, CO2 fixation, or aquarium/pond filtration in the form of an algae scrubber. Algae bioreactors vary widely in design, falling broadly into two categories: open reactors and enclosed reactors. Open reactors are exposed to the atmosphere while enclosed reactors, also commonly called photobioreactors, are isolated to varying extent from the atmosphere. Specifically, algae bioreactors can be used to produce fuels such as biodiesel and bioethanol, to generate animal feed, or to reduce pollutants such as NOx and CO2 in flue gases of power plants.  Fundamentally, this kind of bioreactor is based on the photosynthetic reaction which is performed by the chlorophyll-containing algae itself using dissolved carbon dioxide and sunlight energy. The carbon dioxide is dispersed into the reactor fluid to make it accessible for the algae. The bioreactor has to be made out of transparent material.",1
Life Science,AlgaePARC,"Wageningen UR (University & Research centre) has constructed AlgaePARC (Algae Production And Research Centre) at the Wageningen Campus. The goal of AlgaePARC is to fill the gap between fundamental research on algae and full-scale algae production facilities. This will be done by setting up flexible pilot scale facilities to perform applied research and obtain direct practical experience. It is a joined initiative of BioProcess Engineering and Food & Biobased Research of the Wageningen University.
AlgaePARC facility
AlgaePARC uses four different photobioreactors comprising 24 m2 ground surface: an open pond, two types of tubular reactors and a plastic film bioreactor, and a number of smaller systems for the testing of new technologies. This facility is unique, because it is the first facility in which the productivity of four different production systems can be compared during the year under identical conditions. At the same time, knowledge is gained for the development of new photobioreactors and the design of systems on a production scale.
For the construction of the facility 2.25 M€  has been made available by the Ministry of Agriculture, Nature and Food Quality (1.5 M€) and the Provincie Gelderland (0.75 M€).
Microalgae
Microalgae are currently seen by some persons as a promising source of biodiesel and chemical building blocks, which can be used in paint and plastics. Biomass from algae offers a sustainable alternative to products and fuels from the petrochemical industry. When fully developed this contributes to a biobased economy as algae help to reduce the emissions of carbon dioxide (CO2) and make the economy less dependent on fossil fuels.
AlgaePARC research
The costs of biomass produced from algae for biofuels are still ten times too high to be able to compete with today’s other fuels. Within the business community, the question being asked is how it could be produced more cheaply, making it economically viable. Companies within the energy, food, oil and chemical sectors, the Ministry of Agriculture, Nature & Food Quality, the Provincial Government of Gelderland, Oost NV and Wageningen UR are all working together in or contributing to the algae research centre AlgaePARC in order to answer that question.

",1
Life Science,ALLO-715,"ALLO-715 is a CAR-T therapy by Allogene Therapeutics that targets B-cell maturation antigen (BCMA). As of June 2021, it is undergoing clinical trials for the treatment of multiple myeloma. On 21 April 2021, Allogene Therapeutics announced that the Food and Drug Administration has granted Regenerative Medicine Advanced Therapy status to ALLO-715.",1
Life Science,Aminoallyl nucleotide,"Aminoallyl nucleotide is a nucleotide with a modified base containing an allylamine. They are used in post-labeling of nucleic acids by fluorescence detection in microarray. They are reactive with N-Hydroxysuccinimide ester group which helps attach a fluorescent dye to the primary amino group on the nucleotide. These nucleotides are known as 5-(3-aminoallyl)-nucleotides since the aminoallyl group is usually attached to carbon 5 of the pyrimidine ring of uracil or cytosine. The primary amine group in the aminoallyl moiety is aliphatic and thus more reactive compared to the amine groups that are directly attached to the rings (aromatic) of the bases. Common names of aminoallyl nucleosides are initially abbreviated with aa- or AA- to indicate aminoallyl. The 5-carbon sugar is indicated with or without the lowercase ""d"" indicating deoxyribose if included or ribose if not. Finally the nitrogenous base and number of phosphates are indicated (i.e. aa-UTP = aminoallyl uridine triphosphate).

",1
Life Science,Amplicon,"In molecular biology, an amplicon is a piece of DNA or RNA that is the source and/or product of amplification or replication events. It can be formed artificially, using various methods including polymerase chain reactions (PCR) or ligase chain reactions (LCR), or naturally through gene duplication. In this context, amplification refers to the production of one or more copies of a genetic fragment or target sequence, specifically the amplicon. As it refers to the product of an amplification reaction, amplicon is used interchangeably with common laboratory terms, such as ""PCR product.""
Artificial amplification is used in research, forensics, and medicine for purposes that include detection and quantification of infectious agents, identification of human remains, and extracting genotypes from human hair.Natural gene duplication plays a major role in evolution. It is also implicated in several forms of human cancer including primary mediastinal B cell lymphoma and Hodgkin's lymphoma. In this context the term amplicon can refer both to a section of chromosomal DNA that has been excised, amplified, and reinserted elsewhere in the genome, and to a fragment of extrachromasomal DNA known as a double minute, each of which can be composed of one or more genes. Amplification of the genes encoded by these amplicons generally increases transcription of those genes and ultimately the volume of associated proteins.",1
Life Science,Analog ear,"An analog ear or analog cochlea is a model of the ear or of the cochlea (in the inner ear) based on an electrical, electronic or mechanical analog.  An analog ear is commonly described as an interconnection of electrical elements such as resistors, capacitors, and inductors; sometimes transformers and active amplifiers are included.",1
Life Science,Animal efficacy rule,"The FDA animal efficacy rule (also known as animal rule) applies to development and testing of drugs and biologicals to reduce or prevent serious or life-threatening conditions caused by exposure to lethal or permanently disabling toxic agents (chemical, biological, radiological, or nuclear substances), where human efficacy trials are not feasible or ethical. The animal efficacy rule was finalized by the FDA and authorized by the United States Congress in 2002, following the September 11 attacks and concerns regarding bioterrorism.",1
Life Science,Antibody-drug conjugate,"Antibody-drug conjugates or ADCs are a class of biopharmaceutical drugs designed as a targeted therapy for treating cancer. Unlike chemotherapy, ADCs are intended to target and kill tumor cells while sparing healthy cells. As of 2019, some 56 pharmaceutical companies were developing ADCs.ADCs are complex molecules composed of an antibody linked to a biologically active cytotoxic (anticancer) payload or drug. Antibody-drug conjugates are examples of bioconjugates and immunoconjugates.
ADCs combine the targeting capabilities of monoclonal antibodies with the cancer-killing ability of cytotoxic drugs. They can be designed to discriminate between healthy and diseased tissue.",1
Life Science,Antibody-oligonucleotide conjugate,"Antibody-drug conjugates or ADCs are a class of biopharmaceutical drugs designed as a targeted therapy for treating cancer. Unlike chemotherapy, ADCs are intended to target and kill tumor cells while sparing healthy cells. As of 2019, some 56 pharmaceutical companies were developing ADCs.ADCs are complex molecules composed of an antibody linked to a biologically active cytotoxic (anticancer) payload or drug. Antibody-drug conjugates are examples of bioconjugates and immunoconjugates.
ADCs combine the targeting capabilities of monoclonal antibodies with the cancer-killing ability of cytotoxic drugs. They can be designed to discriminate between healthy and diseased tissue.",1
Life Science,Antisense therapy,"Antisense therapy is a form of treatment that uses antisense oligonucleotides (ASOs) to target messenger RNA (mRNA). ASOs are capable of altering mRNA expression through a variety of mechanisms, including ribonuclease H mediated decay of the pre-mRNA, direct steric blockage, and exon content modulation through splicing site binding on pre-mRNA. Several ASOs have been approved in the United States, the European Union, and elsewhere.",1
Life Science,Artificially Expanded Genetic Information System,"Artificially Expanded Genetic Information System (AEGIS) is a synthetic DNA analog experiment that uses some unnatural base pairs from the laboratories of the Foundation for Applied Molecular Evolution in Gainesville, Florida. AEGIS is a NASA-funded project to try to understand how extraterrestrial life may have developed.The system uses twelve different nucleobases in its genetic code. These include the four canonical nucleobases found in DNA (adenine, cytosine, guanine and thymine) plus eight synthetic nucleobases). AEGIS includes S:B, Z:P, V:J and K:X base pairs.",1
Life Science,Arven Pharmaceuticals,"Arven Pharmaceuticals is a Turkish pharmaceutical corporation headquartered in Istanbul established as a subsidiary of Toksöz Group in 2013. Arven’s primary focus is development and production of high-technology inhaler and biotechnology products. The company is specialized on difficult to make products and strives to develop quality products. Arven is the first Turkish company developing biosimilars for global markets, including USA and EU.Arven has obtained a marketing authorization in 2016 for the biosimilar of Filgrastim, marketed under Fraven, which is the first biosimilar drug, developed and manufactured from cell to final product in Turkey.Additionally, Arven is the first Turkish company to design and develop a patented Dry Powder Inhaler (DPI) device under Arvohaler trademark and globally introduced Cyplos (salmeterol/fluticasone) product inhaled with Arvohaler device.Some of the ministry of health  foundations have visitations to the company for the vaccine manufacturing potential during Covid-19 pandemic period.",1
Life Science,3-Arylpropiolonitriles,"3-Arylpropiolonitriles (APN) belong to a class of electron-deficient alkyne derivatives substituted by two electron-withdrawing groups  – a nitrile and an aryl moieties. Such activation results in improved selectivity towards highly reactive thiol-containing molecules, namely cysteine residues in proteins. APN-based modification of proteins was reported to surpass several important drawbacks of existing strategies in bioconjugation, notably the presence of side reactions with other nucleophilic amino acid residues and the relative instability of the resulting bioconjugates in the blood stream. The latter drawback is especially important for the preparation of targeted therapies, such as antibody-drug conjugates.",1
Life Science,Aspergillus niger,"Aspergillus niger is a fungus and one of the most common species of the genus Aspergillus.
It causes a disease called ""black mold"" on certain fruits and vegetables such as grapes, apricots, onions, and peanuts, and is a common contaminant of food. It is ubiquitous in soil and is commonly reported from indoor environments, where its black colonies can be confused with those of Stachybotrys (species of which have also been called ""black mold"").Some strains of A. niger have been reported to produce potent mycotoxins called ochratoxins; other sources disagree, claiming this report is based upon misidentification of the fungal species. Recent evidence suggests some true A. niger strains do produce ochratoxin A. It also produces the isoflavone orobol.

",1
Life Science,Assisted reproductive technology,"Assisted reproductive technology (ART) includes medical procedures used primarily to address infertility. This subject involves procedures such as in vitro fertilization (IVF), intracytoplasmic sperm injection (ICSI), cryopreservation of gametes or embryos, and/or the use of fertility medication. When used to address infertility, ART may also be referred to as fertility treatment. ART mainly belongs to the field of reproductive endocrinology and infertility. Some forms of ART may be used with regard to fertile couples for genetic purpose (see preimplantation genetic diagnosis). ART may also be used in surrogacy arrangements, although not all surrogacy arrangements involve ART.
The existence of sterility will not always require ART to be the first option to consider, as there are occasions when its cause is a mild disorder that can be solved with more conventional treatments or with behaviors based on promoting health and reproductive habits.",1
Life Science,Axenic,"In biology, axenic (, ) describes the state of a culture in which only a single species, variety, or strain of organism is present and entirely free of all other contaminating organisms. The earliest axenic cultures were  of  bacteria or unicellular eukaryotes, but axenic cultures of many multicellular organisms are also possible. Axenic culture is an important tool for the study of symbiotic and parasitic organisms in a controlled environment.",1
Life Science,Bacterial conjugation,"Bacterial conjugation is the transfer of genetic material between bacterial cells by direct cell-to-cell contact or by a bridge-like connection between two cells. This takes place through a pilus. It is a parasexual mode of reproduction in bacteria.
It is a mechanism of horizontal gene transfer as are transformation and transduction although these two other mechanisms do not involve cell-to-cell contact.Classical E. coli bacterial conjugation is often regarded as the bacterial equivalent of sexual reproduction or mating since it involves the exchange of genetic material. However, it is not sexual reproduction, since no exchange of gamete occurs, and indeed no generation of a new organism: instead an existing organism is transformed. During classical E. coli conjugation the donor cell provides a conjugative or mobilizable genetic element that is most often a plasmid or transposon. Most conjugative plasmids have systems ensuring that the recipient cell does not already contain a similar element.
The genetic information transferred is often beneficial to the recipient. Benefits may include antibiotic resistance, xenobiotic tolerance or the ability to use new metabolites. Other elements can be detrimental and may be viewed as bacterial parasites.
Conjugation in Escherichia coli by spontaneous zygogenesis and in Mycobacterium smegmatis by distributive conjugal transfer differ from the better studied classical E. coli conjugation in that these cases involve substantial blending of the parental genomes.

",1
Life Science,Bacterial therapy,"Bacterial therapy is the therapeutic use of bacteria to treat diseases. Bacterial therapeutics are living medicines, and may be wild type bacteria (often in the form of probiotics) or bacteria that have been genetically engineered to possess therapeutic properties that is injected into a patient.
Other examples of living medicines include cellular therapeutics (including immunotherapeutics) and phage therapeutics.",1
Life Science,BamHI,"BamHI (pronounced ""Bam H one"") (from Bacillus amyloliquefaciens) is a type II restriction endonuclease, having the capacity for recognizing short sequences (6 bp) of DNA and specifically cleaving them at a target site. This exhibit focuses on the structure-function relations of BamHI as described by Newman, et al. (1995). BamHI binds at the recognition sequence 5'-GGATCC-3', and cleaves these sequences just after the 5'-guanine on each strand. This cleavage results in sticky ends which are 4 bp long. In its unbound form, BamHI displays a central b sheet, which resides in between α-helices. 
BamHI undergoes a series of unconventional conformational changes upon DNA recognition. This allows the DNA to maintain its normal B-DNA conformation without distorting to facilitate enzyme binding. BamHI is a symmetric dimer. DNA is bound in a large cleft that is formed between dimers; the enzyme binds in a ""crossover"" manner. Each BamHI subunit makes the majority of its backbone contacts with the phosphates of a DNA half site but base pair contacts are made between each BamHI subunit and nitrogenous bases in the major groove of the opposite DNA half site. The protein binds the bases through either direct hydrogen bonds or water-mediated H-bonds between the protein and every H-bond donor/acceptor group in the major groove. Major groove contacts are formed by atoms residing on the amino-terminus of a parallel 4 helix bundle. This bundle marks the BamHI dimer interface, and it is thought that the dipole moments of the NH2-terminal atoms on this bundle may contribute to electrostatic stabilization.",1
Life Science,Betibeglogene autotemcel,"Betibeglogene autotemcel, sold under the brand name Zynteglo, is a medication for the treatment for beta thalassemia. It was developed by Bluebird Bio and was given breakthrough therapy designation by the U.S. Food and Drug Administration in February 2015. It was approved for medical use in the European Union in May 2019.The most serious side effect observed is thrombocytopenia (low blood levels of platelets).

",1
Life Science,Bio Base Europe,"Bio Base Europe is an innovation and training center for the biobased economy. It is a platform that supports the development of sustainable, biobased products such as bio-chemicals, bio-plastics, bio-materials, bio-detergents, and bio-energy from renewable biomass resources. Its mission is to stimulate sustainable development and economic growth by facilitating R&D and training for biobased process development. It consists of a Pilot Plant for the biobased economy located in the port of Ghent (Belgium), and a Training Center for the biobased economy in Terneuzen (Netherlands).

",1
Life Science,Biobased economy,"Biobased economy, bioeconomy or biotechonomy is economic activity involving the use of biotechnology and biomass in the production of goods, services, or energy. The terms are widely used by regional development agencies, national and international organizations, and biotechnology companies. They are closely linked to the evolution of the biotechnology industry and the capacity to study, understand, and manipulate genetic material that has been possible due to scientific research and technological development. This includes the application of scientific and technological developments to agriculture, health, chemical, and energy industries. The terms bioeconomy (BE) and bio-based economy (BBE) are sometimes used interchangeably. However, it is worth to distinguish them: the biobased economy takes into consideration the production of non-food goods, whilst bioeconomy covers both bio-based economy and the production and use of food and feed.",1
Life Science,Biochemistry,"Biochemistry or biological chemistry, is the study of chemical processes within and relating to living organisms.  A sub-discipline of both chemistry and biology, biochemistry may be divided into three fields: structural biology, enzymology and metabolism.  Over the last decades of the 20th century, biochemistry has become successful at explaining living processes through these three disciplines.  Almost all areas of the life sciences are being uncovered and developed through biochemical methodology and research.  Biochemistry focuses on understanding the chemical basis which allows biological molecules to give rise to the processes that occur within living cells and between cells, in turn relating greatly to the understanding of tissues and organs, as well as organism structure and function. Biochemistry is closely related to molecular biology, which is the study of the molecular mechanisms of biological phenomena.Much of biochemistry deals with the structures, bonding, functions, and interactions of biological macromolecules, such as proteins, nucleic acids, carbohydrates, and lipids.  They provide the structure of cells and perform many of the functions associated with life.  The chemistry of the cell also depends upon the reactions of small molecules and ions.  These can be inorganic (for example, water and metal ions) or organic (for example, the amino acids, which are used to synthesize proteins).  The mechanisms used by cells to harness energy from their environment via chemical reactions are known as metabolism.  The findings of biochemistry are applied primarily in medicine, nutrition and agriculture.  In medicine, biochemists investigate the causes and cures of diseases.  Nutrition studies how to maintain health and wellness and also the effects of nutritional deficiencies.  In agriculture, biochemists investigate soil and fertilizers. Improving crop cultivation, crop storage, and pest control are also goals.",1
Life Science,BioCyc database collection,"The BioCyc database collection is an assortment of organism specific Pathway/Genome Databases (PGDBs) that provide reference to genome and metabolic pathway information for thousands of organisms. As of June 2021, there were over 17,800 databases within BioCyc. SRI International, based in Menlo Park, California, maintains the BioCyc database family.",1
Life Science,Bioelectronics,"Bioelectronics is a field of research in the convergence of biology and electronics.

",1
Life Science,Biofact (philosophy),"In philosophy, sociology, and the arts, the word ""biofact"" is a neologism coined from the combination of the words bios and artifact and denotes a being that is both an artifact and living being or both natural and artificial. This being has been created by purposive human action but exists by processes of growth. There are sources who cite some creations of genetic engineering as examples of biofacts.",1
Life Science,Biofunctionalisation,"In the field of bioengineering, biofunctionalisation (or biofunctionalization) is the modification of a material to have  biological function and/or stimulus, whether permanent or temporary, while at the same time being biologically compatible.Various types of medical implants are designed to biofunctionalize so that they can replace or repair a defective biological function  and are accepted by the host organism.",1
Life Science,Biohappiness,"Biohappiness, or bio-happiness, is the elevation of wellbeing in humans through biological methods, including germline engineering through screening embryos with genes associated with a high level of happiness, or the use of drugs intended to raise baseline levels of happiness. The object is to facilitate the achievement of a state of ""better than well.""Proponents of biohappiness include the transhumanist philosopher David Pearce, whose goal is to end the suffering of all sentient beings and the Canadian ethicist Mark Alan Walker. Walker has sought to defend biohappiness on the grounds that happiness ought to be of interest to a wide range of moral theorists; and that hyperthymia, a state of high baseline happiness, is associated with better outcomes in health and human achievement.The concept of biohappiness also has its high-profile critics, including Leon Kass, who served on the President's Council on Bioethics during the presidency of George W. Bush.",1
Life Science,Biohydrometallurgy,Biohydrometallurgy is a technique in the world of metallurgy that utilizes biological agents (bacteria) to recover and treat metals such as copper. Modern biohydrometallurgy advances started with the bioleaching of copper more efficiently in the 1950's,1
Life Science,Bioinstructive material,"Bioinstructive materials provide instruction to biological cells or tissue, for example immune instruction when monocytes are cultured on certain polymers they polarise to pro- or anti-inflammatory macrophages with potential applications in implanted devices, or materials for the repair of musculoskeletal tissues. Due to the paucity of information on the mechanism of materials control of cells, beyond the general recognition of the important role of adsorbed biomolecules, high throughput screening of large libraries of materials, topographies, and shapes are often used to identify cell instructive material systems. Applications of bioinstructive materials as substrates for stem cell production, cell delivery and reduction of foreign body reaction and coatings to reduce infections on medical devices. This non-leaching approach is distinct from strategies of infection control relying on antibiotic release, cytokine delivery or guidance of cells by surface located epitopes inspired by nature.",1
Life Science,Bioleaching,"Bioleaching is the extraction of metals from their ores through the use of living organisms. This is much cleaner than the traditional heap leaching using cyanide. Bioleaching is one of several applications within biohydrometallurgy and several methods are used to recover copper, zinc, lead,  arsenic, antimony, nickel, molybdenum, gold, silver, and cobalt.",1
Life Science,BioLegend,"BioLegend is a global developer and manufacturer of antibodies and reagents used in biomedical research located in San Diego, California. It was incorporated in June 2002 and has since expanded to include BioLegend Japan KK, where it is partnered with Tomy Digital Biology Co., Ltd. in Tokyo, BioLegend Europe in the United Kingdom, BioLegend GmbH in Germany, and BioLegend UK Ltd in the United Kingdom. BioLegend manufactures products in the areas of neuroscience, cell immunophenotyping, cytokines and chemokines, adhesion, cancer research, T regulatory cells, stem cells, innate immunity, cell-cycle analysis, apoptosis, and modification-specific antibodies. Reagents are created for use in flow cytometry, proteogenomics, ELISA, immunoprecipitation, Western blotting, immunofluorescence microscopy, immunohistochemistry, and in vitro or in vivo functional assays.

",1
Life Science,Biological computing,"Biological computers use biologically derived molecules — such as DNA and proteins — to perform digital or real computations.
The development of biocomputers has been made possible by the expanding new science of nanobiotechnology. The term nanobiotechnology can be defined in multiple ways; in a more general sense, nanobiotechnology can be defined as any type of technology that uses both nano-scale materials (i.e. materials having characteristic dimensions of 1-100 nanometers) and biologically based materials. A more restrictive definition views nanobiotechnology more specifically as the design and engineering of proteins that can then be assembled into larger, functional structures
The implementation of nanobiotechnology, as defined in this narrower sense, provides scientists with the ability to engineer biomolecular systems specifically so that they interact in a fashion that can ultimately result in the computational functionality of a computer.

",1
Life Science,Biological hydrogen production (algae),"Biohydrogen is H2 that is produced biologically.  Interest is high in this technology because H2 is a clean fuel and can be readily produced from certain kinds of biomass.Many challenges characterize this technology, including those intrinsic to H2, such as storage and transportation of a noncondensible gas.  Hydrogen producing organisms are poisoned by O2.  Yields of H2 are often low.",1
Life Science,Biomagnetics,"Biomagnetics is a field of biotechnology. It has actively been researched since at least 2004. Although the majority of structures found in living organisms are diamagnetic, the magnetic field itself, as well as magnetic nanoparticles, microstructures and paramagnetic molecules can influence specific physiological functions of organisms under certain conditions. The effect of magnetic fields on biosystems is a topic of research that falls under the biomagnetic umbrella, as well as the construction of magnetic structures or systems that are either biocompatible, biodegradable or biomimetic. Magnetic nanoparticles and magnetic microparticles are known to interact with certain prokaryotes and certain eukaryotes.Magnetic nanoparticles under the influence of magnetic and electromagnetic fields were shown to modulate redox reactions for the inhibition or the promotion of animal tumor growth. The mechanism underlying nanomagnetic modulation involves the convergence of magnetochemical and magneto-mechanical reactions.",1
Life Science,Biomanufacturing,"Biomanufacturing is a type of manufacturing or biotechnology that utilizes biological systems to produce commercially important biomaterials and biomolecules for use in medicines, food and beverage processing, and industrial applications.  Biomanufacturing products are recovered from natural sources, such as blood, or from cultures of microbes, animal cells, or plant cells grown in specialized equipment.  The cells used during the production may have been naturally occurring or derived using genetic engineering techniques.

",1
Life Science,Biomarker (medicine),"In medicine, a biomarker is  a measurable indicator of the severity or presence of some disease state. More generally a biomarker is anything that can be used as an indicator of a particular disease state or some other physiological state of an organism. According to the WHO, the indicator may be chemical, physical, or biological in nature - and the measurement may be functional, physiological, biochemical, cellular, or molecular.A biomarker can be a substance that is introduced into an organism as a means to examine organ function or other aspects of health. For example, rubidium chloride is used in isotopic labeling to evaluate perfusion of heart muscle. It can also be a substance whose detection indicates a particular disease state, for example, the presence of an antibody may indicate an infection. More specifically, a biomarker indicates a change in expression or state of a protein that correlates with the risk or progression of a disease, or with the susceptibility of the disease to a given treatment. Biomarkers can be characteristic biological properties or molecules that can be detected and measured in parts of the body like the blood or tissue. They may indicate either normal or diseased processes in the body. Biomarkers can be specific cells, molecules, or genes, gene products, enzymes, or hormones. Complex organ functions or general characteristic changes in biological structures can also serve as biomarkers. Although the term biomarker is relatively new, biomarkers have been used in pre-clinical research and clinical diagnosis for a considerable time. For example, body temperature is a well-known biomarker for fever. Blood pressure is used to determine the risk of stroke. It is also widely known that cholesterol values are a biomarker and risk indicator for coronary and vascular disease, and that C-reactive protein (CRP) is a marker for inflammation.
Biomarkers are useful in a number of ways, including measuring the progress of disease, evaluating the most effective therapeutic regimes for a particular cancer type, and establishing long-term susceptibility to cancer or its recurrence. The parameter can be chemical, physical or biological. In molecular terms biomarker is ""the subset of markers that might be discovered using genomics, proteomics technologies or imaging technologies. Biomarkers play major roles in medicinal biology. Biomarkers help in early diagnosis, disease prevention, drug target identification, drug response etc. Several biomarkers have been identified for many diseases such as serum LDL for cholesterol, blood pressure, and P53 gene and MMPs  as tumor markers for cancer.

",1
Life Science,Biomimetics,"Biomimetics or biomimicry is the emulation of the models, systems, and elements of nature for the purpose of solving complex human problems. The terms ""biomimetics"" and ""biomimicry"" are derived from Ancient Greek: βίος (bios), life, and μίμησις (mīmēsis), imitation, from μιμεῖσθαι (mīmeisthai), to imitate, from μῖμος (mimos), actor. A closely related field is bionics.Living organisms have evolved well-adapted structures and materials over geological time through natural selection. Biomimetics has given rise to new technologies inspired by biological solutions at macro and nanoscales. Humans have looked at nature for answers to problems throughout their existence. Nature has solved engineering problems such as self-healing abilities, environmental exposure tolerance and resistance, hydrophobicity, self-assembly, and harnessing solar energy.",1
Life Science,Biomin,"Biomin is an animal health and nutrition company headquartered in Inzersdorf-Getzersdorf, Austria that develops and produces feed additives and premixes for livestock animals including swine, poultry, dairy and beef cattle as well as aquaculture.The firm supplies customers in more than 100 countries throughout the world.
The Biomin Research Center (BRC) at Campus Tulln in Austria, employs 80 researchers engaged in applied basic research to lead the firm’s in-house R&D efforts, supported by a research network of 150 academic and research institutions worldwide.

",1
Life Science,Biomolecular engineering,"Biomolecular engineering is the application of engineering principles and practices to the purposeful manipulation of molecules of biological origin. Biomolecular engineers integrate knowledge of biological processes with the core knowledge of chemical engineering in order to focus on molecular level solutions to issues and problems in the life sciences related to the environment, agriculture, energy, industry, food production, biotechnology and medicine.
Biomolecular engineers purposefully manipulate carbohydrates, proteins, nucleic acids and lipids within the framework of the relation between their structure (see: nucleic acid structure, carbohydrate chemistry, protein structure,), function (see: protein function) and properties and in relation to applicability to such areas as environmental remediation, crop and livestock production, biofuel cells and biomolecular diagnostics. The thermodynamics and kinetics of molecular recognition in enzymes, antibodies, DNA hybridization, bio-conjugation/bio-immobilization and bioseparations are studied. Attention is also given to the rudiments of engineered biomolecules in cell signaling, cell growth kinetics, biochemical pathway engineering and bioreactor engineering.",1
Life Science,BioMotiv,"BioMotiv is an accelerator company associated with The Harrington Project, a $340 million initiative centered at University Hospitals of Cleveland. Therapeutic opportunities are identified through relationships with The Harrington Discovery Institute, university and research institutions, disease foundations, and industry sources. Once opportunities are identified, BioMotiv oversees the development, funding, active management, and partnering of the therapeutic products.

",1
Life Science,Bionics,"Bionics or biologically inspired engineering is the application of biological methods and systems found in nature to the study and design of engineering systems and modern technology.The word bionic,  coined by Jack E. Steele in August 1958, is a portmanteau from biology and electronics that was popularized by the 1970s U.S. television series The Six Million Dollar Man and The Bionic Woman, both based upon the novel Cyborg by Martin Caidin. All three stories feature humans given various superhuman powers by their electromechanical implants.
According to proponents of bionic technology, the transfer of technology between lifeforms and manufactured objects is desirable because evolutionary pressure typically forces living organisms--fauna and flora--to become optimized and efficient. For example, dirt- and water-repellent paint (coating) developed from the observation that practically nothing sticks to the surface of the lotus flower plant (the lotus effect)..
The term ""biomimetic"" is preferred for references to chemical reactions,  such as reactions that, in nature, involve biological macromolecules (e.g., enzymes or nucleic acids) whose chemistry can be replicated in vitro using much smaller molecules.
Examples of bionics in engineering include the hulls of boats imitating the thick skin of dolphins; sonar, radar, and medical ultrasound imaging imitating animal echolocation.
In the field of computer science, the study of bionics has produced artificial neurons, artificial neural networks, and swarm intelligence. Bionics also motivated advancement in Evolutionary computation but took the idea further by simulating evolution in silico and producing well-optimized solutions that had never appeared in nature.
It is estimated by Julian Vincent, professor of biomimetics at the University of Bath's Department of Mechanical Engineering, that ""at present there is only a 12% overlap between biology and technology in terms of the mechanisms used"".",1
Life Science,Biopesticide,"Biopesticides, a contraction of 'biological pesticides', include several types of pest management intervention: through predatory, parasitic, or chemical relationships. The term has been associated historically with biological pest control – and by implication, the manipulation of living organisms. Regulatory positions can be influenced by public perceptions, thus:

in the EU, biopesticides have been defined as ""a form of pesticide based on micro-organisms or natural products"".
the US EPA states that they ""include naturally occurring substances that control pests (biochemical pesticides), microorganisms that control pests (microbial pesticides), and pesticidal substances produced by plants containing added genetic material (plant-incorporated protectants) or PIPs"".They are obtained from organisms including plants, bacteria and other microbes, fungi, nematodes, etc. They are often important components of integrated pest management (IPM) programmes, and have received much practical attention as substitutes to synthetic chemical plant protection products (PPPs).",1
Life Science,Biopharmaceutical,"A biopharmaceutical, also known as a biologic(al) medical product, or biologic, is any pharmaceutical drug product manufactured in, extracted from, or semisynthesized from biological sources. Different from totally synthesized pharmaceuticals, they include vaccines, whole blood, blood components, allergenics, somatic cells, gene therapies, tissues, recombinant therapeutic protein, and living medicines used in cell therapy. Biologics can be composed of sugars, proteins, nucleic acids, or complex combinations of these substances, or may be living cells or tissues. They (or their precursors or components) are isolated from living sources—human, animal, plant, fungal, or microbial. They can be used in both human and animal medicine.Terminology surrounding biopharmaceuticals varies between groups and entities, with different terms referring to different subsets of therapeutics within the general biopharmaceutical category. Some regulatory agencies use the terms biological medicinal products or therapeutic biological product to refer specifically to engineered macromolecular products like protein- and nucleic acid-based drugs, distinguishing them from products like blood, blood components, or vaccines, which are usually extracted directly from a biological source. Biopharmaceutics is pharmaceutics that works with biopharmaceuticals. Biopharmacology is the branch of pharmacology that studies biopharmaceuticals. Specialty drugs, a recent classification of pharmaceuticals, are high-cost drugs that are often biologics. The European Medicines Agency uses the term advanced therapy medicinal products (ATMPs) for medicines for human use that are ""based on genes, cells, or tissue engineering"", including gene therapy medicines, somatic-cell therapy medicines, tissue-engineered medicines, and combinations thereof. Within EMA contexts, the term advanced therapies refers specifically to ATMPs, although that term is rather nonspecific outside those contexts.
Gene-based and cellular biologics, for example, often are at the forefront of biomedicine and biomedical research, and may be used to treat a variety of medical conditions for which no other treatments are available.In some jurisdictions, biologics are regulated via different pathways from other small molecule drugs and medical devices.

",1
Life Science,Biopiracy,"Biopiracy (also known as scientific colonialism) is defined as the unauthorized appropriation of knowledge and genetic resources of farming and indigenous communities by individuals or institutions seeking exclusive monopoly control through patents or intellectual property. While bioprospecting is the act of exploring natural resources for undiscovered chemical compounds with medicinal or anti-microbial properties, commercial success from bioprospecting leads to the company's attempt at protecting their intellectual property rights on indigenous medicinal plants, seeds, genetic resources, and traditional medicines.Moreover, if biological resources and traditional knowledge are taken from indigenous or marginalized groups, the commercialization of their natural resource can harm  communities. Despite the medicinal and innovative benefits of bioprospecting and biochemical research, the expropriation of indigenous land for their genetic resources without fair compensation inevitably leads to exploitation. Biopiracy can harm indigenous populations in multiple ways. Without proper compensation or reward for traditional knowledge of natural resources, the sudden increase in commercial value of the species producing the active compound can make it now unaffordable for the native people. In some cases, a patent filed by the western company could prohibit the use or sell of the resource by any individual or institution, including the indigenous group. With nearly one third of all small-molecule drugs approved by the U.S. Food and Drug Administration (FDA) between 1981 and 2014 being either natural products or compounds derived from natural products, bioprospecting or piracy is growing more significantly, especially in the pharmaceutical industry.With the advancement of extraction techniques of genetic material in biochemistry and molecular biology, scientists are now able to identify a specific gene, which directs to enzymes capable of converting one molecule to another. This scientific breakthrough brings up the question of whether the organism containing the gene that has been modified through a series of tests and experiments should be accredited to the country of origin.",1
Life Science,Bioprocess,"A bioprocess is a specific process that uses complete living cells or their components (e.g., bacteria, enzymes, chloroplasts) to obtain desired products.
Transport of energy and mass is fundamental to many biological and environmental processes. Areas, from food processing (including brewing beer) to thermal design of buildings to biomedical devices, manufacture of monoclonal antibodies to pollution control and global warming, require knowledge of how energy and mass can be transported through materials (momentum, heat transfer, etc.).

",1
Life Science,Bioprocess engineering,"Bioprocess engineering, also biochemical engineering, is a specialization of  chemical engineering or biological engineering. It deals with the design and development of equipment and processes for the manufacturing of products such as agriculture, food, feed, pharmaceuticals, nutraceuticals, chemicals, and polymers and paper from biological materials & treatment of waste water. 
Bioprocess engineering is a conglomerate of mathematics, biology and industrial design, and consists of various spectrums like designing of bioreactors, study of fermentors (mode of operations etc.). It also deals with studying various biotechnological processes used in industries for large scale production of biological product for optimization of yield in the end product and the quality of end product. Bioprocess engineering may include the work of mechanical, electrical, and industrial engineers to apply principles of their disciplines to processes based on using living cells or sub component of such cells.

",1
Life Science,Bioprocessor,"A bioprocessor is a miniaturized bioreactor capable of culturing mammalian, insect and microbial cells.  Bioprocessors are capable of mimicking performance of large-scale bioreactors, hence making them ideal for laboratory scale experimentation of cell culture processes. Bioprocessors are also used for concentrating bioparticles (such as cells) in bioanalytical systems. Microfluidic processes such as electrophoresis can be implemented by bioprocessors to aid in DNA isolation and purification.",1
Life Science,Bioproducts engineering,"Bioproducts or bio-based products are materials, chemicals and energy derived from renewable biological resources.",1
Life Science,Bioreactor,"A bioreactor refers to any manufactured device or system that supports a biologically active environment. In one case, a bioreactor is a vessel in which a chemical process is carried out which involves organisms or biochemically active substances derived from such organisms. This process can either be aerobic or anaerobic. These bioreactors are commonly cylindrical, ranging in size from litres to cubic metres, and are often made of stainless steel.
It may also refer to a device or system designed to grow cells or tissues in the context of cell culture. These devices are being developed for use in tissue engineering or biochemical/bioprocess engineering.
On the basis of mode of operation, a bioreactor may be classified as batch, fed batch or continuous (e.g. a continuous stirred-tank reactor model). An example of a continuous bioreactor is the chemostat.Bioreactors are highly nonlinear and many novel control strategies have been proposed for their control.Organisms growing in bioreactors may be submerged in liquid medium or may be attached to the surface of a solid medium. Submerged cultures may be suspended or immobilized. Suspension bioreactors can use a wider variety of organisms, since special attachment surfaces are not needed, and can operate at a much larger scale than immobilized cultures. However, in a continuously operated process the organisms will be removed from the reactor with the effluent. Immobilization is a general term describing a wide variety of methods for cell or particle attachment or entrapment. It can be applied to basically all types of
biocatalysis including enzymes, cellular organelles, animal and plant cells and organs. Immobilization is useful for continuously operated processes, since the organisms will not be removed with the reactor effluent, but is limited in scale because the microbes are only present on the surfaces of the vessel.
Large scale immobilized cell bioreactors are: 

moving media, also known as moving bed biofilm reactor (MBBR)
packed bed
fibrous bed
membrane",1
Life Science,Biorefining,"Biorefining is the process of ""building"" multiple products from biomass as a feedstock or raw material much like a petroleum refinery that is currently in use. A biorefinery is a facility like a petroleum refinery that comprises the various process steps or unit operations and related equipment to produce various bioproducts including fuels, power, materials and chemicals from biomass. Industrial biorefineries have been identified as the most promising route to the creation of a new domestic biobased industry producing entire spectrum of bioproducts or bio-based products.
Biomass has various components such as lignin, cellulose, hemicellulose, extractives, etc. Biorefinery can take advantage of the unique properties of each of biomass components enabling the production of various products. The various bioproducts can include fiber, fuels, chemicals, plastics etc.",1
Life Science,Bioremediation,"Bioremediation broadly refers to any process wherein a biological system (typically bacteria, microalgae, fungi, and plants), living or dead, is employed for removing environmental pollutants from air, water, soil, flue gasses, industrial effluents etc, in natural or artificial settings. The natural ability of organisms to adsorb, accumulate, and degrade common and emerging pollutants has attracted the use of biological resources in treatment of contaminated environment. In comparison to conventional physiochemical treatment methods which suffer serious drawbacks, bioremediation is sustainable, eco-friendly, cheap, and scalable.
Most bioremediation is inadvertent, involving native organisms. Research on bioremediation is heavily focused on stimulating the process by inoculation of a polluted site with organisms or supplying nutrients to promote the growth.  In principle, bioremediation could be used to reduce the impact of byproducts created from anthropogenic activities, such as industrialization and agricultural processes. Bioremediation could prove less expensive and more sustainable than other remediation alternatives.For organic pollutants, which are generally susceptible to biodegradation than heavy metals, bioremediation usually involves oxidations.  Oxidations enhance the water-solubility of organic compounds and their susceptibility to further degradation by oxidation and hydrolysis. Ultimately biodegradation convert hydrocarbons to carbon dioxide and water.  For heavy metals, bioremediation offers few solutions.  Metal containing can be removed or reduced with varying bioremediation techniques. The main challenge to bioremediations is rate: the processes are slow.Bioremediation techniques can be classified as (i) in situ techniques, which treats polluted sites directly, vs (ii) ex situ techniques which are applied to excavated materials. In both these approaches, additional nutrients, vitamins, minerals, and pH buffers are added to enhance the growth and metabolism of the microorganisms. In some cases, specialized microbial cultures are added (biostimulation). Some examples of bioremediation related technologies are phytoremediation, bioventing, bioattenuation, biosparging, composting (biopiles and windrows), and landfarming. Other remediation techniques include thermal desorption, vitrification, air stripping, bioleaching, rhizofiltration, and soil washing. Biological treatment, bioremediation, is a similar approach used to treat wastes including wastewater, industrial waste and solid waste. The end goal of bioremediation is to remove or reduce harmful compounds to improve soil and water quality.

",1
Life Science,Biorobotics,"Biorobotics is an interdisciplinary science that combines the fields of biomedical engineering, cybernetics, and robotics to develop new technologies that integrate biology with mechanical systems to develop more efficient communication, alter genetic information, and create machines that imitate biological systems.

",1
Life Science,Master of Bioscience Enterprise,"A Master of Bioscience Enterprise (abbreviated MBE or MBioEnt) is a specialised degree taught at The University of Auckland, New Zealand, Karolinska Institute, Sweden and The University of Cambridge, United Kingdom. The MBE is an interdisciplinary programme incorporating multiple faculties and includes significant industry involvement.
The degree is primarily focused on the commercialisation of biotechnology. Both universities have developed the MBE programme to provide specialist business and legal skills relevant to employment in the bio-economy. The context in which both programmes were developed are significantly different. These differences are reflected in internship placements, thesis topics and postgraduate employment opportunities.",1
Life Science,"Bioseparation of 1,3-propanediol","1,3-Propanediol is the organic compound with the formula CH2(CH2OH)2.  This 3-carbon diol is a colorless viscous liquid that is miscible with water.",1
Life Science,Bioship,"In molecular biology, biochips are engineered substrates (""miniaturized laboratories"") that can host large numbers of simultaneous biochemical reactions. One of the goals of biochip technology is to efficiently screen large numbers of biological analytes, with potential applications ranging from disease diagnosis to detection of bioterrorism agents. For example, digital microfluidic biochips are under investigation for applications in biomedical fields. In a digital microfluidic biochip, a group of (adjacent) cells in the microfluidic array can be configured to work as storage, functional operations, as well as for transporting fluid droplets dynamically.

",1
Life Science,Biosimilar,"A biosimilar (also known as follow-on biologic or subsequent entry biologic) is a biologic medical product that is almost an identical copy of an original product that is manufactured by a different company. Biosimilars are officially approved versions of original ""innovator"" products and can be manufactured when the original product's patent expires. Reference to the innovator product is an integral component of the approval.Unlike with generic drugs of the more common small-molecule type, biologics generally exhibit high molecular complexity and may be quite sensitive to changes in manufacturing processes. Despite that heterogeneity, all biopharmaceuticals, including biosimilars, must maintain consistent quality and clinical performance throughout their lifecycle.Drug-related authorities such as the European Medicines Agency (EMA) of the European Union, the United States Food and Drug Administration (FDA), and the Health Products and Food Branch of Health Canada hold their own guidance on requirements for demonstration of the similar nature of two biological products in terms of safety and efficacy. According to them, analytical studies demonstrate that the biological product is highly similar to the reference product, despite minor differences in clinically inactive components, animal studies (including the assessment of toxicity), and a clinical study or studies (including the assessment of immunogenicity and pharmacokinetics or pharmacodynamics). They are sufficient to demonstrate safety, purity, and potency in one or more appropriate conditions of use for which the reference product is licensed and is intended to be used and for which licensure is sought for the biological product.The World Health Organization (WHO) published its ""Guidelines for the evaluation of similar biotherapeutic products (SBPs)"" in 2009. The purpose of this guideline is to provide an international norm for evaluating biosimilars.The EMA has granted marketing authorizations for more than 50 biosimilars since 2006, (first approved biosimilar Somatropin (Growth hormone)). The first biosimilar of a monoclonal antibody to be approved worldwide was a biosimilar of infliximab in the EU in 2013. On March 6, 2015, the FDA approved the United States' first biosimilar product, the biosimilar of filgrastim called filgrastim-sndz (trade name Zarxio) by Sandoz.

",1
Life Science,Biostimulation,"Biostimulation involves the modification of the environment to stimulate existing bacteria capable of bioremediation.  This can be done by addition of various forms of rate limiting nutrients and electron acceptors, such as phosphorus, nitrogen, oxygen, or carbon (e.g. in the form of molasses).  Alternatively, remediation of halogenated contaminants in anaerobic environments may be stimulated by adding electron donors (organic substrates), thus allowing indigenous microorganisms to use the halogenated contaminants as electron acceptors. EPA Anaerobic Bioremediation Technologies Additives are usually added to the subsurface through injection wells, although injection well technology for biostimulation purposes is still emerging.  Removal of the contaminated material is also an option, albeit an expensive one.  Biostimulation can be enhanced by bioaugmentation.  This process, overall, is referred to as bioremediation and is an EPA-approved method for reversing the presence of oil or gas spills.  While biostimulation is usually associated with remediation of hydrocarbon or high production volume chemical spills, it is also potentially useful for treatment of less frequently encountered contaminant spills, such as pesticides, particularly herbicides.The primary advantage of biostimulation is that bioremediation will be undertaken by already present native microorganisms that are well-suited to the subsurface environment, and are well distributed spatially within the subsurface.  The primary disadvantage is that the delivery of additives in a manner that allows the additives to be readily available to subsurface microorganisms is based on the local geology of the subsurface.  Tight, impermeable subsurface lithology (tight clays or other fine-grained material) make it difficult to spread additives throughout the affected area.  Fractures in the subsurface create preferential pathways in the subsurface which additives preferentially follow, preventing even distribution of additives.
Recently a number of products have been introduced which allow popular use of bioremediation using biostimulative methods.  They may harness local bacteria using biostimulation by creating a hospitable environment for hydrocarbon-devouring microorganisms, or they may introduce foreign bacteria into the environment as a direct application to the hydrocarbon.  While the jury is out as to whether either is particularly more effective than the other, prima facie consideration suggests the introduction of foreign bacteria to any environment stands a chance of mutating organisms already present and affecting the biome.
Investigations to determine subsurface characteristics (such as natural groundwater velocity during ambient conditions, hydraulic conductivity of the subsurface, and lithology of the subsurface) are important in developing a successful biostimulation system.  In addition, a pilot-scale study of the potential biostimulation system should be undertaken prior to full-scale design and implementation.
However, some biostimulative agents may be used in chaotic surfaces such as open water and sand so long as they are [oleophilic], meaning that they bond exclusively to hydrocarbons, and basically sink in the water column, bonding to oil, where they then float to the water's surface, exposing the hydrocarbon to more abundant sunlight and oxygen where greater micro-organic aerobic activity can be encouraged.  Some consumer-targeted biostimulants bond possess this quality, others do not.",1
Life Science,Biotechnology risk,"Biotechnology risk is a form of existential risk that could come from biological sources, such as genetically engineered biological agents. The origin of such a high-consequence pathogen could be a deliberate release (in the form of bioterrorism or biological weapons), an accidental release, or a naturally occurring event. 
A chapter on biotechnology and biosecurity was published in Nick Bostrom's 2008 anthology Global Catastrophic Risks, which covered risks including as viral agents. Since then, new technologies like CRISPR and gene drives have been introduced.
While the ability to deliberately engineer pathogens has been constrained to high-end labs run by top researchers, the technology to achieve this (and other astonishing feats of bioengineering) is rapidly becoming cheaper and more widespread. Such examples include the diminishing cost of sequencing the human genome (from $10 million to $1,000), the accumulation of large datasets of genetic information, the discovery of gene drives, and the discovery of CRISPR. Biotechnology risk is therefore a credible explanation for the Fermi paradox.",1
Life Science,Biotransducer,"A biotransducer is the recognition-transduction component of a biosensor system. It consists of two intimately coupled parts; a bio-recognition layer and a physicochemical transducer, which acting together converts a biochemical signal to an electronic or optical signal. The bio-recognition layer typically contains an enzyme or another binding protein such as antibody. However, oligonucleotide sequences, sub-cellular fragments such as organelles (e.g. mitochondria) and receptor carrying fragments (e.g. cell wall), single whole cells, small numbers of cells on synthetic scaffolds, or thin slices of animal or plant tissues, may also comprise the bio-recognition layer. It gives the biosensor selectivity and specificity. The physicochemical transducer is typically in intimate and controlled contact with the recognition layer. As a result of the presence and biochemical action of the analyte (target of interest), a physico-chemical change is produced within the biorecognition layer that is measured by the physicochemical transducer producing a signal that is proportionate to the concentration of the analyte.  The physicochemical transducer may be electrochemical, optical, electronic, gravimetric, pyroelectric or piezoelectric. Based on the type of biotransducer, biosensors can be classified as shown to the right.",1
Life Science,Biotransformation,"Biotransformation is the biochemical modification of one chemical compound or a mixture of chemical compounds. Biotransformations can be conducted with whole cells, their lysates, or purified enzymes.  Increasingly, biotransformations are effected with purified enzymes.  Major industries and life-saving technologies depend on biotransformations.",1
Life Science,BioWeb,"The BioWeb is the connotation for a network of web-enabled biological devices (e.g. trees, plants, and flowers) which extends an internet of things to the Internet of Living Things of natural sensory devices. The BioWeb devices give insights to real-time ecological data and feedback to changes in the environment. The biodiversity of today is one giant ecological mesh network of information exchange, and a resource humanity should be able to access for a better understanding of the state of our global ecology.",1
Life Science,Body capacitance,"Body capacitance is the physical property of the human body that has it act as a capacitor. Like any other electrically-conductive object, a human body can store electric charge if insulated. The actual amount of capacitance varies with the surroundings; it would be low when standing on top of a pole with nothing nearby, but high when leaning against an insulated, but grounded large metal surface, such as a household refrigerator, or a metal wall in a factory.",1
Life Science,Brain–brain interface,"A brain–brain interface is a direct communication pathway between the brain of one animal and the brain of another animal.
Brain to brain interfaces have been used to help rats collaborate with each other. When a second rat was unable to choose the correct lever, the first rat noticed (not getting a second reward), and produced a round of task-related neuron firing that made the second rat more likely to choose the correct lever.In 2013, Rajesh Rao was able to use electrical brain recordings and a form of magnetic stimulation to send a brain signal to Andrea Stocco on the other side of the University of Washington campus. In 2015, researchers linked up multiple brains, of both monkeys and rats, to form an “organic computer.It is hypothesized that by using brain-to-brain interfaces (BTBIs) a biological computer, or brain-net, could be constructed using animal brains as its computational units. Initial exploratory work demonstrated collaboration between rats in distant cages linked by signals from cortical microelectrode arrays implanted in their brains. The rats were rewarded when actions were performed by the ""decoding rat"" which conformed to incoming signals and when signals were transmitted by the ""encoding rat"" which resulted in the desired action. In the initial experiment the rewarded action was pushing a lever in the remote location corresponding to the position of a lever near a lighted LED at the home location. About a month was required for the rats to acclimate themselves to incoming ""brainwaves.""
Lastly, it is important to stress that the topology of BTBI does not need to be restricted to one encoder and one decoder subjects. Instead, we have already proposed that, in theory, channel accuracy can be increased if instead of a dyad a whole grid of multiple reciprocally interconnected brains are employed. Such a computing structure could define the first example of an organic computer capable of solving heuristic problems that would be deemed non-computable by a general Turing-machine. Future works will elucidate in detail the characteristics of this multi-brain system, its computational capabilities, and how it compares to other non-Turing computational architectures
Miguel Nicolelis of Duke University, one of the investigators who did the experiment with rats, has done previous work using a brain–computer interface.

",1
Life Science,Broad Institute,"The Eli and Edythe L. Broad Institute of MIT and Harvard (IPA: , pronunciation respelling: BROHD), often referred to as the Broad Institute, is a biomedical and genomic research center located in Cambridge, Massachusetts, United States. The institute is independently governed and supported as a 501(c)(3) nonprofit research organization under the name Broad Institute Inc., and it partners with the Massachusetts Institute of Technology, Harvard University, and the five Harvard teaching hospitals.

",1
Life Science,Calix Limited,"Calix Limited is an Australian technology company whose core technology is a ""kiln"" built in Bacchus Marsh that produces ""mineral honeycomb"". Calix's technology includes work on CO2 capture to address global sustainability challenges across several industries including wastewater treatment, aquaculture, advanced energy storage and cement / lime production.

",1
Life Science,Callus (cell biology),"Plant callus (plural calluses or calli) is a growing mass of unorganized plant parenchyma cells. In living plants, callus cells are those cells that cover a plant wound. In biological research and biotechnology callus formation is induced from plant tissue samples (explants) after surface sterilization and plating onto tissue culture medium in vitro (in a closed culture vessel such as a Petri dish).    The culture medium is supplemented with plant growth regulators, such as auxin, cytokinin, and gibberellin, to initiate callus formation or somatic embryogenesis.  Callus initiation has been described for all major groups of land plants.",1
Life Science,3D cell culturing by magnetic levitation,"3D cell culture by the magnetic levitation method (MLM) is the application of growing 3D tissue by inducing cells treated with magnetic nanoparticle assemblies in spatially varying magnetic fields using neodymium magnetic drivers and promoting cell to cell interactions by levitating the cells up to the air/liquid interface of a standard petri dish. The magnetic nanoparticle assemblies consist of magnetic iron oxide nanoparticles, gold nanoparticles, and the polymer polylysine. 3D cell culturing is scalable, with the capability for culturing 500 cells to millions of cells or from single dish to high-throughput low volume systems. Once magnetized cultures are generated, they can also be used as the building block material, or the ""ink"", for the magnetic 3D bioprinting process.",1
Life Science,Cell culture,"Cell culture is the process by which cells are grown under controlled conditions, generally outside their natural environment. After the cells of interest have been isolated from living tissue, they can subsequently be maintained under carefully controlled conditions. These conditions vary for each cell type, but generally consist of a suitable vessel with a substrate or medium that supplies the essential nutrients (amino acids, carbohydrates, vitamins, minerals), growth factors, hormones, and gases (CO2, O2), and regulates the physio-chemical environment (pH buffer, osmotic pressure, temperature). Most cells require a surface or an artificial substrate to form an adherent culture as a monolayer (one single-cell thick), whereas others can be grown free floating in a medium as a suspension culture. The lifespan of most cells is genetically determined, but some cell culturing cells have been “transformed” into immortal cells which will reproduce indefinitely if the optimal conditions are provided.
In practice, the term ""cell culture"" now refers to the culturing of cells derived from multicellular eukaryotes, especially animal cells, in contrast with other types of culture that also grow cells, such as plant tissue culture, fungal culture, and microbiological culture (of microbes). The historical development and methods of cell culture are closely interrelated to those of tissue culture and organ culture. Viral culture is also related, with cells as hosts for the viruses.
The laboratory technique of maintaining live cell lines (a population of cells descended from a single cell and containing the same genetic makeup) separated from their original tissue source became more robust in the middle 20th century.",1
Life Science,Chemically defined medium,"A chemically defined medium is a growth medium suitable for the in vitro cell culture of human or animal cells in which all of the chemical components are known. Standard cell culture media commonly consist of a basal medium supplemented with animal serum (such as fetal bovine serum, FBS) as a source of nutrients and other ill-defined factors. The technical disadvantages to using serum include its undefined nature, batch-to-batch variability in composition, and the risk of contamination.
There is a clear distinction between serum-based media and chemically defined media. Serum-based media may contain undefined animal-derived products such as serum (purified from blood), hydrolysates, growth factors, hormones, carrier proteins, and attachment factors. These undefined animal-derived products will contain complex contaminants, such as the lipid content of albumin. In contrast, chemically defined media require that all of the components must be identified and have their exact concentrations known. Therefore, a chemically defined medium must be entirely free of animal-derived components and cannot contain either fetal bovine serum, bovine serum or human serum. To achieve this, chemically defined media is commonly supplemented with recombinant versions of albumin and growth factors, usually derived from rice or E. coli, or synthetic chemical such as the polymer polyvinyl alcohol which can reproduce some of the functions of BSA/HSA.
The constituents of a chemically defined media include: a basal media (such as DMEM, F12, or RPMI 1640, containing amino acids, vitamins, inorganic salts, buffers, antioxidants and energy sources), which is supplemented with recombinant albumin, chemically defined lipids, recombinant insulin and/or zinc, recombinant transferrin or iron, selenium and an antioxidant thiol such as 2-mercaptoethanol or 1-thioglycerol. Chemically defined media that are designed for the cultivation of cells in suspension additionally contain suitable surfactants such as poloxamers in order to reduce shear stress caused by shaking and stirring.",1
Life Science,Cell culture assay,"A cell culture assay is any method used to assess the cytotoxicity of a material.  This refers to the in vitro assessment of a material to determine whether it releases toxic chemicals in the cell. It also determines if the quantity is sufficient to kill cells, either directly or indirectly, through the inhibition of cell metabolic pathways.  Cell culture evaluations are the precursor to whole animal studies and are a way to determine if significant cytotoxicity exists for the given material.  Cell culture assays are standardized by ASTM, ISO, and BSI (British Standards Institution.)

",1
Life Science,Cell therapy,"Cell therapy (also called cellular therapy, cell transplantation, or cytotherapy) is a therapy in which viable cells are injected, grafted or implanted into a patient in order to effectuate a medicinal effect, for example, by transplanting T-cells capable of fighting cancer cells via cell-mediated immunity in the course of immunotherapy, or grafting stem cells to regenerate diseased tissues.
Cell therapy originated in the nineteenth century when scientists experimented by injecting animal material in an attempt to prevent and treat illness. Although such attempts produced no positive benefit, further research found in the mid twentieth century that human cells could be used to help prevent the human body rejecting transplanted organs, leading in time to successful bone marrow transplantation as has become common practice in treatment for patients that have compromised bone marrow after disease, infection, radiation or chemotherapy. In recent decades, however, stem cell and cell transplantation has gained significant interest by researchers as a potential new therapeutic strategy for a wide range of diseases, in particular for degenerative and immunogenic pathologies.",1
Life Science,Cellular Agriculture Society,"Cellular agriculture focuses on the production of agriculture products from cell cultures using a combination of biotechnology, tissue engineering, molecular biology, and synthetic biology to create and design new methods of producing proteins, fats, and tissues that would otherwise come from traditional agriculture. Most of the industry is focused on animal products such as meat, milk, and eggs, produced in cell culture rather than raising and slaughtering farmed livestock which is associated with substantial global problems of detrimental environmental impacts (e.g. of meat production), animal welfare, food security and human health. Cellular agriculture is field of the biobased economy. The most well known cellular agriculture concept is cultured meat.

",1
Life Science,Cellular microarray,"A cellular microarray (or cell microarray) is a laboratory tool that allows for the multiplex interrogation of living cells on the surface of a solid support.  The support, sometimes called a ""chip"", is spotted with varying materials, such as antibodies, proteins, or lipids, which can interact with the cells, leading to their capture on specific spots.  Combinations of different materials can be spotted in a given area, allowing not only cellular capture, when a specific interaction exists, but also the triggering of a cellular response, change in phenotype, or detection of a response from the cell, such as a specific secreted factor.
There are a large number of types of cellular microarrays:

Reverse transfection cell microarrays. David M. Sabatini's laboratory developed reverse-transfection cell microarrays at the Whitehead Institute, publishing their work in 2001.
PMHC Cellular Microarrays.  This type of microarray were developed by Daniel Chen, Yoav Soen, Dan Kraft, Patrick Brown and Mark Davis at Stanford University Medical Center.",1
Life Science,Institute of Himalayan Bioresource Technology,"C.S.I.R - Institute of Himalayan Bioresource Technology or CSIR-IHBT established in 1983 is a constituent laboratory of Council of Scientific and Industrial Research. This institute located in Palampur, Kangra, Himachal Pradesh, India is engaged in various advanced research aspects of Himalayan Bio-resources and modern biology. It has also been imparting Ph.D. in Biological and Chemical Sciences.
Situated among pristine environ in the lap of Dhauladhar ranges, CSIR-IHBT is the only laboratory of the Council of Scientific and Industrial Research in the State of Himachal Pradesh (H.P.), India. Institute has a focused research mandate on bioresources for catalysizing bioeconomy in a sustainable manner.
The institute has state-of the art laboratories; remote sensing and mapping facilities; internationally recognised herbarium; animal house facility; pilot plants in nutraceuticals, essential oil and herbals; farms and polyhouses. The young and dynamic team of scientists propel the research and work dedicatedly to discover and find solutions to new challenging problems faced by the society. International collaborations further strengthens scientific interactions at a global scale. Promoting industrial growth through technological interventions is a constant endeavour and several technologies developed by the institute are transferred to industries. For socio- economic upliftment,  regular training programmes and  advisory services are rendered to farmers,  floriculturists, tea planters and small entrepreneurs involved in food processing sector. Institute has been recognised as one of the Incubation Centres by MSME  GoI and in the area of Affordable Health Care by DSIR.  Institute encourages industries to share the technological problems faced them, such that efforts could be made in developing a viable solution. Confidentiality is strictly maintained. Work on plant adaptation studies and  high altitude medicinal plants are further strengthened by the field lab ”Centre for High Altitude Biology (CeHAB) situated at Ribling in Lahaul & Spiti district of H.P. Through this centre, institute disseminates technologies by way of trainings and demonstrations that could transform the economy  of the region and help in solving unique challenges faced by them. Institute fosters student-scientist interaction and school children are welcome to visit the institute. Post graduate students can do project and sharpen their research skills at CSIR-IHBT. Young researchers are welcome for to do Ph.D. in cutting edge areas under the able guidance of expert faculty. Institute passionately contribute its bit in the development of society, industry and environment.",1
Life Science,Certolizumab pegol,"Certolizumab pegol, sold under the brand name Cimzia, is a biologic medication for the treatment of Crohn's disease, rheumatoid arthritis, psoriatic arthritis and ankylosing spondylitis. It is a fragment of a monoclonal antibody specific to tumor necrosis factor alpha (TNF-α) and is manufactured by UCB.It is on the World Health Organization's List of Essential Medicines.",1
Life Science,Chargaff's rules,"Chargaff's rules state that DNA from any species of any organism should have a 1:1 stoichiometric ratio of purine and pyrimidine bases (i.e., A+G=T+C) and, more specifically, that the amount of guanine should be equal to cytosine and the amount of adenine should be equal to thymine. This pattern is found in both strands of the DNA. They were discovered by Austrian-born chemist Erwin Chargaff, in the late 1940s.",1
Life Science,Chem-seq,"Chem-seq is a technique that is used to map genome-wide interactions between small molecules and their protein targets in the chromatin of eukaryotic cell nuclei. The method employs chemical affinity capture coupled with massively parallel DNA sequencing to identify genomic sites where small molecules interact with their target proteins or DNA.  It was first described by Lars Anders et al. in the January, 2014 issue of ""Nature Biotechnology"".",1
Life Science,Chemical Engineering and Biotechnology Abstracts,"Chemical Engineering and Biotechnology Abstracts (CEABA-VTB) is an abstracting and indexing service that is published by DECHEMA, BASF, and Bayer Technology Services, all based in Germany. This is a bibliographic database that covers multiple disciplines.",1
Life Science,Chemically induced dimerization,"Chemically Induced Dimerization (CID) is a biological mechanism in which two proteins bind only in the presence of a certain small molecule, enzyme or other dimerizing agent. Genetically engineered CID systems are used in biological research to control protein localization, to manipulate signalling pathways and to induce protein activation.  

",1
Life Science,ChIP sequencing,"ChIP-sequencing, also known as ChIP-seq, is a method used to analyze protein interactions with DNA. ChIP-seq combines chromatin immunoprecipitation (ChIP) with massively parallel DNA sequencing to identify the binding sites of DNA-associated proteins. It can be used to map global binding sites precisely for any protein of interest. Previously, ChIP-on-chip was the most common technique utilized to study these protein–DNA relations.

",1
Life Science,COLD-PCR,"COLD-PCR (co-amplification at lower denaturation temperature PCR) is a modified polymerase chain reaction (PCR) protocol that enriches variant alleles from a mixture of wildtype and mutation-containing DNA. The ability to preferentially amplify and identify minority alleles and low-level somatic DNA mutations in the presence of excess wildtype alleles is useful for the detection of mutations. Detection of mutations is important in the case of early cancer detection from tissue biopsies and body fluids such as blood plasma or serum, assessment of residual disease after surgery or chemotherapy, disease staging and molecular profiling for prognosis or tailoring therapy to individual patients, and monitoring of therapy outcome and cancer remission or relapse. Common PCR will amplify both the major (wildtype) and minor (mutant) alleles with the same efficiency, occluding the ability to easily detect the presence of low-level mutations. The capacity to detect a mutation in a mixture of variant/wildtype DNA is valuable because this mixture of variant DNAs can occur when provided with a heterogeneous sample – as is often the case with cancer biopsies. Currently, traditional PCR is used in tandem with a number of different downstream assays for genotyping or the detection of somatic mutations. These can include the use of amplified DNA for RFLP analysis, MALDI-TOF (matrix-assisted laser-desorption–time-of-flight) genotyping, or direct sequencing for detection of mutations by Sanger sequencing or pyrosequencing. Replacing traditional PCR with COLD-PCR for these downstream assays will increase the reliability in detecting mutations from mixed samples, including tumors and body fluids.

",1
Life Science,Color Genomics,"Color is a population health technology company which provides genetic tests and analysis directly to patients as well as through employers. The product focuses on genes that indicate risk for heart disease, cancer, and that affect medication response.

",1
Life Science,Colostrum,"Colostrum (known colloquially as beestings, bisnings or first milk) is the first form of milk produced by the mammary glands of mammals (including humans) immediately following delivery of the newborn. Most species will begin to generate colostrum just prior to giving birth. Colostrum has an especially high amount of bioactive compounds compared to mature milk to give the newborn the best possible start to life. Specifically, colostrum contains antibodies to protect the newborn against disease and infection, and immune and growth factors and other bioactives that help to activate a newborn’s immune system, jumpstart gut function, and seed a healthy gut microbiome in the first few days of life. The bioactives found in colostrum are essential for a newborn’s health, growth and vitality.
At birth, the surroundings of the newborn mammal change from the relatively sterile environment in the mother’s uterus, with a constant nutrient supply via the placenta, to the microbe-rich environment outside, with irregular oral intake of complex milk nutrients through the gastrointestinal tract. This transition puts high demands on the gastrointestinal tract of the neonate, as the gut plays an important part in both the digestive system and the immune system. Colostrum has evolved to care for highly sensitive mammalian neonates and contributes significantly to initial immunological defense as well as to the growth, development, and maturation of the neonate’s gastrointestinal tract by providing key nutrients and bioactive factors.Colostrum also has a mild laxative effect, encouraging the passing of the baby's first stool, which is called meconium.  This clears excess bilirubin, a waste-product of dead red blood cells which is produced in large quantities at birth due to blood volume reduction from the infant's body and which also helps prevent jaundice.",1
Life Science,Combinatorial biology,"In biotechnology, combinatorial biology is the creation of a large number of compounds (usually proteins or peptides) through technologies such as phage display. Similar to combinatorial chemistry, compounds are produced by biosynthesis rather than organic chemistry. This process was developed independently by Richard A. Houghten and H. Mario Geysen in the 1980s.  Combinatorial biology allows the generation and selection of the large number of ligands for high-throughput screening.Combinatorial biology techniques generally begin with large numbers of peptides, which are generated and screened by physically linking a gene encoding a protein and a copy of said protein. This could involve the protein being fused to the M13 minor coat protein pIII, with the gene encoding this protein being held within the phage particle. Large libraries of phages with different proteins on their surfaces can then be screened through automated selection and amplification for a protein that binds tightly to a particular target.",1
Life Science,Comparison of DNA sequencing services,"This page lists the different DNA sequencing services. 
2 main types can be distinguished:

Whole genome sequencing (WGS) services
Single-nucleotide polymorphism (SNP) sequencing servicesWhole exome sequencing is the middle ground between these two types, where a large amount of genes are sequenced, but only those that produce meaningful differences important for practical purposes, which is only 1% of the whole genome.
Both allow people to detect the presence of hereditary diseases (and/or other imperfections) in their DNA, and (when WGS) is used, it even allows people to know the specifics of their hereditary diseases (and/or other imperfections). These specifics can be important, as in many cases, it's not a single gene that causes the disease, but rather a combination of genes. In some cases, the exact gene is not even known, but only the approximate location where the imperfect nucleotides are situated is known.",1
Life Science,Competitions and prizes in biotechnology,There exist a number of competitions and prizes to reward distinguished contributions and to encourage developments in biotechnology.,1
Life Science,Complex systems biology,"Complex systems biology (CSB) is a branch or subfield of mathematical and theoretical biology invented by Robert Rosen concerned with complexity of both structure and function in biological organisms, as well as the emergence and evolution of organisms and species, with emphasis being placed on the interconnectivity of, and within, biological network inference, and on  modelling the fundamental relations inherent to life.According to Baianu et al.  CSB is a field that has only a partial overlap with the more conventional concepts of complex systems theory and systems biology, because CSB is concerned with philosophy and human consciousness. Moreover, mathematics can model a wide range of complex systems, but this is claimed not to be relevant.",1
Life Science,Connect (biotechnology organization),"Connect is a non-profit serving the San Diego and Southern California region.  Connect elevates innovators and entrepreneurs throughout their growth journey by providing educational programming, mentorship, networking events, and access to capital.  The current CEO is Mike Krenn.

",1
Life Science,Contract research organization,"In the life sciences, a contract research organization (CRO) is a company that provides support to the  pharmaceutical, biotechnology, and medical device industries in the form of research services outsourced on a contract basis.  A CRO may provide such services as biopharmaceutical development, biologic assay development, commercialization, clinical development, clinical trials management, pharmacovigilance, outcomes research, and Real world evidence. CROs are designed to reduce costs for companies developing new medicines and drugs in niche markets. They aim to simplify entry into drug markets, and simplify development, as the need for large pharmaceutical companies to do everything ‘in house’ is now redundant. CROs also support foundations, research institutions, and universities, in addition to governmental organizations (such as the NIH, EMA, etc.).Many CROs specifically provide clinical-study and clinical-trial support for drugs and/or medical devices. CROs range from large, international full-service organizations to small, niche specialty groups. CROs that specialize in clinical-trials services can offer their clients the expertise of moving a new drug or device from its conception to FDA/EMA marketing approval, without the drug sponsor having to maintain a staff for these services.

",1
Life Science,CRISPR gene editing,"CRISPR gene editing  (pronounced  ""crisper"") is a genetic engineering technique in molecular biology by which the genomes of living organisms may be modified. It is based on a simplified version of the bacterial CRISPR-Cas9 antiviral defense system. By delivering the Cas9 nuclease complexed with a synthetic guide RNA (gRNA) into a cell, the cell's genome can be cut at a desired location, allowing existing genes to be removed and/or new ones added in vivo.The technique is considered highly significant in biotechnology and medicine as it allows for the genomes to be edited in vivo with extremely high precision, cheaply, and with ease. It can be used in the creation of new medicines, agricultural products, and genetically modified organisms, or as a means of controlling pathogens and pests. It also has possibilities in the treatment of inherited genetic diseases as well as diseases arising from somatic mutations such as cancer. However, its use in human germline genetic modification is highly controversial. The development of the technique earned Jennifer Doudna and Emmanuelle Charpentier the Nobel Prize in Chemistry in 2020. The third researcher group that shared the Kavli Prize for the same discovery, led by Virginijus Šikšnys, was not awarded the Nobel prize.Working like genetic scissors, the Cas9 nuclease opens both strands of the targeted sequence of DNA to introduce the modification by one of two methods. Knock-in mutations, facilitated via homology directed repair (HDR), is the traditional pathway of targeted genomic editing approaches. This allows for the introduction of targeted DNA damage and repair. HDR employs the use of similar DNA sequences to drive the repair of the break via the incorporation of exogenous DNA to function as the repair template. This method relies on the periodic and isolated occurrence of DNA damage at the target site in order for the repair to commence. Knock-out mutations caused by CRISPR-Cas9 result in the repair of the double-stranded break by means of non-homologous end joining (NHEJ). NHEJ can often result in random deletions or insertions at the repair site, which may disrupt or alter gene functionality. Therefore, genomic engineering by CRISPR-Cas9 gives researchers the ability to generate targeted random gene disruption. Because of this, the precision of genome editing is a great concern. Genomic editing leads to irreversible changes to the genome.
While genome editing in eukaryotic cells has been possible using various methods since the 1980s, the methods employed had proved to be inefficient and impractical to implement on a large scale. With the discovery of CRISPR and specifically the Cas9 nuclease molecule, efficient and highly selective editing is now a reality. Cas9 derived from the bacterial species Streptococcus pyogenes has facilitated targeted genomic modification in eukaryotic cells by allowing for a reliable method of creating a targeted break at a specific location as designated by the crRNA and tracrRNA guide strands. The ease with which researchers can insert Cas9 and template RNA in order to silence or cause point mutations at specific loci has proved invaluable to the quick and efficient mapping of genomic models and biological processes associated with various genes in a variety of eukaryotes. Newly engineered variants of the Cas9 nuclease have been developed that significantly reduce off-target activity.CRISPR-Cas9 genome editing techniques have many potential applications, including in medicine and agriculture. The use of the CRISPR-Cas9-gRNA complex for genome editing was the AAAS's choice for Breakthrough of the Year in 2015. Many bioethical concerns have been raised about the prospect of using CRISPR for germline editing, especially in human embryos.",1
Life Science,Cultured meat,"Cultured meat (also known by other names, see below) is a meat produced by in vitro cell cultures of animal cells. It is a form of cellular agriculture, with such agricultural methods being explored in the context of increased consumer demand for protein.Cultured meat is produced using tissue engineering techniques traditionally used in regenerative medicines. The concept of cultured meat was introduced to wider audiences by Jason Matheny in the early 2000s after he co-authored a paper on cultured meat production and created New Harvest, the world's first nonprofit organization dedicated to in-vitro meat research.Cultured meat may have the potential to address substantial global problems of the environmental impact of meat production, animal welfare, food security and human health. Specifically, it can be thought of in the context of the mitigation of climate change.

In 2013, professor Mark Post at Maastricht University pioneered a proof-of-concept for cultured meat by creating the first hamburger patty grown directly from cells. Since then, other cultured meat prototypes have gained media attention: SuperMeat opened a farm-to-fork restaurant called ""The Chicken"" in Tel Aviv to test consumer reaction to its ""Chicken"" burger, while the ""world's first commercial sale of cell-cultured meat"" occurred in December 2020 at the Singapore restaurant ""1880"", where cultured meat manufactured by the US firm Eat Just was sold.While most efforts in the space focus on common meats such as pork, beef, and chicken which comprise the bulk of consumption in developed countries, some new companies such as Orbillion Bio have focused on high end or unusual meats including Elk, Lamb, Bison, and the prized Wagyu strain of beef. Avant Meats has brought cultured grouper fish to market  as other companies have started to pursue cultivating additional fish species and other seafood.The production process is constantly evolving, driven by multiple companies and research institutions. The applications of cultured meat have led to ethical, health, environmental, cultural, and economic discussions. In terms of market strength, data published by the non-governmental organization Good Food Institute found that in 2021 cultivated meat companies attracted $140 million in Europe alone.
Currently cultured meat is served at special events and few high end restaurants, mass production of cultured meat has not started yet.",1
Life Science,Cunninghamella elegans,"Cunninghamella elegans is a species of fungus in the genus Cunninghamella found in soil.It can be grown in Sabouraud dextrose broth, a liquid medium used for cultivation of yeasts and molds from liquid which are normally sterile.
As opposed to C. bertholletiae, it is not a human pathogen, with the exception of two documented patients.

",1
Life Science,Cure Rare Disease,"Cure Rare Disease is a non-profit biotechnology company based in Boston, Massachusetts that is working to create individualized therapeutics using CRISPR technology to treat people impacted by rare diseases.",1
Life Science,Custom-made medical device,"A custom-made medical device, commonly referred to as a custom-made device (CMD) (Canada, European Union, United Kingdom) or a custom device (United States), is a medical device designed and manufactured for the sole use of a particular patient. Examples of custom-made medical devices include auricular splints, dentures, orthodontic appliances, orthotics and prostheses.",1
Life Science,Cyborg,"The Borg are an alien group that appear as recurring antagonists in the Star Trek fictional universe. The Borg are cybernetic organisms (cyborgs) linked in a hive mind called ""the Collective"". The Borg co-opt the technology and knowledge of other alien species to the Collective through the process of ""assimilation"": forcibly transforming individual beings into ""drones"" by injecting nanoprobes into their bodies and surgically augmenting them with cybernetic components. The Borg's ultimate goal is ""achieving perfection"".Aside from being recurring antagonists in the Next Generation television series, they are depicted as the main threat in the film Star Trek: First Contact. In addition, they played major roles in the Voyager series.
The Borg have become a symbol in popular culture for any juggernaut against which ""resistance is futile"", a common phrase uttered by the Borg.",1
Life Science,Dendrosome,"Dendrosomes are novel vesicular, spherical, supramolecular entities wherein the dendrimer–nucleic acid complex is encapsulated within a lipophilic shell. They possess negligible hemolytic toxicity and higher transfection efficiency, and they are better tolerated in vivo than are dendrimers. The word "" Dendrosome"" came from the Greek word  ""Dendron"" meaning tree and "" some"" means vesicles. Thus dendrosomes are vesicular structures composed of dendrimers.",1
Life Science,DEPT (medicine),"Directed enzyme prodrug therapy (DEPT) uses enzymes artificially introduced into the body to  convert prodrugs, which have no or poor biologically activity, to the active form in the desired location within the body. Many chemotherapy drugs for cancer lack tumour specificity and the doses required to reach therapeutic levels in the tumour are often toxic to other tissues. DEPT strategies are an experimental method of reducing the systemic toxicity of a drug, by achieving high levels of the active drug only at the desired site. This article describes the variations of DEPT technology.

",1
Life Science,Differential display,"Differential display (also referred to as DDRT-PCR or DD-PCR) is a laboratory technique that allows a researcher to compare and identify changes in gene expression at the mRNA level between two or more eukaryotic cell samples.  It was the most commonly used method to compare expression profiles of two eukaryotic cell samples in the 1990s. By 2000, differential display was superseded by DNA microarray approaches.In differential display, first all the RNA in each sample is reverse transcribed using a set of 3′ ""anchored primers"" (having a short sequence of deoxy-thymidine nucleotides at the end) to create a cDNA library for each sample, followed by PCR amplification using arbitrary 3′ primers for cDNA strand amplification together with anchored 3′ primers for RNA strand amplification, identical to those used to create the library; about forty arbitrary primers is the optimal number to transcribe almost all of the mRNA. The resulting transcripts are then separated by electrophoresis and visualized, so that they can be compared.   The method was prone to error due to different mRNAs migrated into single bands, differences in less abundant mRNAs getting drowned by more abundant mRNAs, sensitivity to small changes in cell culture conditions, and a tendency to amplify 3′ fragments rather than full mRNAs, and the necessity to use about 300 primers to catch all the mRNA.: 316–317   The method was first published in Science in 1992.",1
Life Science,Digital microfluidics,"Digital microfluidics (DMF) is a platform for lab-on-a-chip systems that is based upon the manipulation of microdroplets. Droplets are dispensed, moved, stored, mixed, reacted, or analyzed on a platform with a set of insulated electrodes. Digital microfluidics can be used together with analytical analysis procedures such as mass spectrometry, colorimetry, electrochemical, and electrochemiluminescense.",1
Life Science,Directed differentiation,"Directed differentiation is a bioengineering methodology at the interface of stem cell biology, developmental biology and tissue engineering. It is essentially harnessing the potential of stem cells by constraining their differentiation in vitro toward a specific cell type or tissue of interest. Stem cells are by definition pluripotent, able to differentiate into several cell types such as neurons, cardiomyocytes, hepatocytes, etc. Efficient directed differentiation requires a detailed understanding of the lineage and cell fate decision, often provided by developmental biology.",1
Life Science,Discovery District,"The Discovery District is one of the commercial districts in Downtown Toronto, Ontario, Canada. It has a high concentration of hospitals and research institutions, particularly those related to biotechnology. The district is roughly bounded by Bloor Street on the north, Bay Street on the east, Dundas Street on the south, and Spadina Avenue on the west.

",1
Life Science,Do-it-yourself biology,"Do-it-yourself biology (DIY biology, DIY bio) is a growing biotechnological social movement in which individuals, communities, and small organizations study biology and life science using the same methods as traditional research institutions. DIY biology is primarily undertaken by individuals with limited research training from academia or corporations, who then mentor and oversee other DIY biologists with little or no formal training. This may be done as a hobby, as a not-for-profit endeavour for community learning and open-science innovation, or for profit, to start a business.
Other terms are also associated with the do-it-yourself biology community. The terms biohacking and wetware hacking emphasize the connection to hacker culture and the hacker ethic. The term hacker is used in the original sense of finding new and clever ways to do things. The term biohacking is also used by the grinder body modification community, which is considered related but distinct from the do-it-yourself biology movement.  The term biopunk emphasizes the techno-progressive, political, and artistic elements of the movement.

",1
Life Science,DNA clamp,"A DNA clamp, also known as a sliding clamp or β-clamp, is a protein complex that serves as a processivity-promoting factor in DNA replication. As a critical component of the DNA polymerase III holoenzyme, the clamp protein binds DNA polymerase and prevents this enzyme from dissociating from the template DNA strand.  The clamp-polymerase protein–protein interactions are stronger and more specific than the direct interactions between the polymerase and the template DNA strand; because one of the rate-limiting steps in the DNA synthesis reaction is the association of the polymerase with the DNA template, the presence of the sliding clamp dramatically increases the number of nucleotides that the polymerase can add to the growing strand per association event. The presence of the DNA clamp can increase the rate of DNA synthesis up to 1,000-fold compared with a nonprocessive polymerase.",1
Life Science,DNA condensation,"DNA condensation refers to the process of compacting DNA molecules in vitro or in vivo. Mechanistic details of DNA packing are essential for its functioning in the process of gene regulation in living systems. Condensed DNA often has surprising properties, which one would not predict from classical concepts of dilute solutions. Therefore, DNA condensation in vitro serves as a model system for many processes of physics, biochemistry and biology. In addition, DNA condensation has many potential applications in medicine and biotechnology.DNA diameter is about 2 nm, while the length of a stretched single molecule may be up to several dozens of centimetres depending on the organism. Many features of the DNA double helix contribute to its large stiffness, including the mechanical properties of the sugar-phosphate backbone, electrostatic repulsion between phosphates (DNA bears on average one elementary negative charge per each 0.17 nm of the double helix), stacking interactions between the bases of each individual strand, and strand-strand interactions. DNA is one of the stiffest natural polymers, yet it is also one of the longest molecules. This means that at large distances DNA can be considered as a flexible rope, and on a short scale as a stiff rod. Like a garden hose, unpacked DNA would randomly occupy a much larger volume than when it is orderly packed. Mathematically, for a non-interacting flexible chain randomly diffusing in 3D, the end-to-end distance would scale as a square root of the polymer length. For real polymers such as DNA, this gives only a very rough estimate; what is important, is that the space available for the DNA in vivo is much smaller than the space that it would occupy in the case of a free diffusion in the solution. To cope with volume constraints, DNA can pack itself in the appropriate solution conditions with the help of ions and other molecules. Usually, DNA condensation is defined as ""the collapse of extended DNA chains into compact, orderly particles containing only one or a few molecules"". This definition applies to many situations in vitro and is also close to the definition of DNA condensation in bacteria as ""adoption of relatively concentrated, compact state occupying a fraction of the volume available"". In eukaryotes, the DNA size and the number of other participating players are much larger, and a DNA molecule forms millions of ordered nucleoprotein particles, the nucleosomes, which is just the first of many levels of DNA packing.

",1
Life Science,DNA-encoded chemical library,"DNA-encoded chemical libraries (DEL) is a technology for the synthesis and screening on unprecedented scale of collections of small molecule compounds. DEL is used in medicinal chemistry to bridge the fields of combinatorial chemistry and molecular biology. The aim of DEL technology is to accelerate the drug discovery process and in particular early phase discovery activities such as target validation and hit identification.
DEL technology involves the conjugation of chemical compounds or building blocks to short DNA fragments that serve as identification bar codes and in some cases also direct and control the chemical synthesis. The technique enables the mass creation and interrogation of libraries via affinity selection, typically on an immobilized protein target. A homogeneous method for screening DNA-encoded libraries has recently been developed which uses water-in-oil emulsion technology to isolate, count and identify individual ligand-target complexes in a single-tube approach. In contrast to conventional screening procedures such as high-throughput screening, biochemical assays are not required for binder identification, in principle allowing the isolation of binders to a wide range of proteins historically difficult to tackle with conventional screening technologies. So, in addition to the general discovery of target specific molecular compounds, the availability of binders to pharmacologically important, but so-far “undruggable” target proteins opens new possibilities to develop novel drugs for diseases that could not be treated so far. In eliminating the requirement to initially assess the activity of hits it is hoped and expected that many of the high affinity binders identified will be shown to be active in independent analysis of selected hits, therefore offering an efficient method to identify high quality hits and pharmaceutical leads.",1
Life Science,DNA field-effect transistor,"The field-effect transistor (FET) is a type of transistor that uses an electric field to control the flow of current in a semiconductor.  FETs are devices with three terminals: source, gate, and drain. FETs control the flow of current by the application of a voltage to the gate, which in turn alters the  conductivity between the drain and source.
FETs are also known as unipolar transistors since they involve single-carrier-type operation. That is, FETs use either electrons or holes as charge carriers in their operation, but not both. Many different types of field effect transistors exist. Field effect transistors generally display very high input impedance at low frequencies. The most widely used field-effect transistor is the MOSFET (metal-oxide-semiconductor field-effect transistor).",1
Life Science,DNA ligase,"DNA ligase is a specific type of enzyme, a ligase, (EC 6.5.1.1) that facilitates the joining of DNA strands together by catalyzing the formation of a phosphodiester bond. It plays a role in repairing single-strand breaks in duplex DNA in living organisms, but some forms (such as DNA ligase IV) may specifically repair double-strand breaks (i.e. a break in both complementary strands of DNA). Single-strand breaks are repaired by DNA ligase using the complementary strand of the double helix as a template, with DNA ligase creating the final phosphodiester bond to fully repair the DNA.
DNA ligase is used in both DNA repair and DNA replication (see Mammalian ligases).  In addition, DNA ligase has extensive use in molecular biology laboratories for recombinant DNA experiments (see Research applications). Purified DNA ligase is used in gene cloning to join DNA molecules together to form recombinant DNA.",1
Life Science,DNA separation by silica adsorption,"DNA separation by silica adsorption is a method of DNA separation that is based on DNA molecules binding to silica surfaces in the presence of certain salts and under certain pH conditions, usually conducted on a microchip coated in silica channels.",1
Life Science,DNA sequencing,"DNA sequencing is the process of determining the nucleic acid sequence – the order of nucleotides in DNA. It includes any method or technology that is used to determine the order of the four bases: adenine, guanine, cytosine, and thymine. The advent of rapid DNA sequencing methods has greatly accelerated biological and medical research and discovery.Knowledge of DNA sequences has become indispensable for basic biological research, DNA Genographic Projects and in numerous applied fields such as medical diagnosis, biotechnology, forensic biology, virology and biological systematics. Comparing healthy and mutated DNA sequences can diagnose different diseases including various cancers, characterize antibody repertoire, and can be used to guide patient treatment. Having a quick way to sequence DNA allows for faster and more individualized medical care to be administered, and for more organisms to be identified and cataloged.
The rapid speed of sequencing attained with modern DNA sequencing technology has been instrumental in the sequencing of complete DNA sequences, or genomes, of numerous types and species of life, including the human genome and other complete DNA sequences of many animal, plant, and microbial species.
The first DNA sequences were obtained in the early 1970s by academic researchers using laborious methods based on two-dimensional chromatography. Following the development of fluorescence-based sequencing methods with a DNA sequencer, DNA sequencing has become easier and orders of magnitude faster.",1
Life Science,DNA‐templated organic synthesis,"DNA‐templated organic synthesis (DTS) is a way to control the reactivity of synthetic molecules by using nature's molarity‐based approach. Historically, DTS was used as a model of prebiotic nucleic acid replication. Now however, it is capable of translating DNA sequences into complex small‐molecule and polymer products of multistep organic synthesis.",1
Life Science,Dragon silk,"Dragon silk is a material created by Kraig Biocraft Laboratories of Ann Arbor, Michigan from genetically modified silkworms to create body armor. Dragon silk combines the elasticity and strength of spider silk. It has the tensile strength as high as 1.79 gigapascals (as much as 37%) and the elasticity above 38% exceeding the maximum reported features of the spider silk. It is reported that dragon silk is more flexible than the Monster silk and stronger than the ""Big Red, recombinant spider silk designed for increased strength.",1
Life Science,DRIP-seq,"DRIP-seq (DRIP-sequencing) is a technology for genome-wide profiling of a type of DNA-RNA hybrid called an ""R-loop"". DRIP-seq utilizes a sequence-independent but structure-specific antibody for DNA-RNA immunoprecipitation (DRIP) to capture R-loops for massively parallel DNA sequencing.

",1
Life Science,Duocarmycin,The duocarmycins are members of a series of related natural products first isolated from Streptomyces bacteria in 1978. They are notable for their extreme cytotoxicity and thus represent a class of exceptionally potent antitumour antibiotics.,1
Life Science,Eadie–Hofstee diagram,"In biochemistry, an Eadie–Hofstee diagram (more usually called an Eadie–Hofstee plot) is a graphical representation of the Michaelis–Menten equation in enzyme kinetics. It has been known by various different names, including Eadie plot, Hofstee plot and Augustinsson plot. Attribution to Woolf is often omitted, because although Haldane and Stern credited Woolf with the underlying equation, it was just one of the three linear transformations of the Michaelis–Menten equation that they initially introduced. However, Haldane indicated latter that Woolf had indeed found the three linear forms: ""In 1932, Dr. Kurt Stern published a German translation of my book ""Enzymes"", with numerous additions to the English text. On pp. 119-120, I described some graphical methods, stating that they were due to my friend Dr. Barnett Woolf. [...] Woolf pointed out that linear graphs are obtained when 
  
    
      
        v
      
    
    {\displaystyle v}
   is plotted against 
  
    
      
        v
        
          x
          
            −
            1
          
        
      
    
    {\displaystyle vx^{-1}}
  , 
  
    
      
        
          v
          
            −
            1
          
        
      
    
    {\displaystyle v^{-1}}
   against 
  
    
      
        
          x
          
            −
            1
          
        
      
    
    {\displaystyle x^{-1}}
  , or 
  
    
      
        
          v
          
            −
            1
          
        
        x
      
    
    {\displaystyle v^{-1}x}
   against 
  
    
      
        x
      
    
    {\displaystyle x}
  , the first plot being most convenient unless inhibition is being studied.""",1
Life Science,Economic importance of bacteria,"Bacteria are economically important as these microorganisms are used by humans for many purposes. The beneficial uses of bacteria include the production of traditional foods such as fudge, yogurt, cheese, and vinegar. Microbes are also important in agriculture for the compost and fertilizer production. Bacteria are used in genetic engineering and genetic changes.

",1
Life Science,Eftilagimod alpha,"Eftilagimod alpha (INN; development code IMP321 or efti) is a large-molecule cancer drug being developed by the clinical-stage biotechnology company Immutep. Efti is a soluble version of the immune checkpoint molecule LAG-3. It is an APC Activator used to increase an immune response to tumors, and is administered by subcutaneous injection. Efti has three intended clinical settings:

as adjuvant to cancer vaccines (in a low, effective dose of ~250 µg)
as first-line 'chemo-immunotherapy,' that is, combined with standard chemotherapy (e.g. paclitaxel)
in combination immunotherapy with PD-1 treatments (e.g. pembrolizumab)Eftilagimod alpha is in Phase II clinical testing. Currently, the main indications for the drug are metastatic breast cancer, non-small cell lung cancer (NSCLC), and head and neck squamous cell carcinoma (HNSCC).",1
Life Science,Electrohydrogenesis,"Electrohydrogenesis or biocatalyzed electrolysis is the name given to a process for generating hydrogen gas from organic matter being decomposed by bacteria.  This process uses a modified fuel cell to contain the organic matter and water.  A small amount, 0.2–0.8 V of electricity is used, the original article reports an overall energy efficiency of 288% can be achieved (this is computed relative to the amount of electricity used, waste heat lowers the overall efficiency).  This work was reported by Cheng and Logan.",1
Life Science,Electromethanogenesis,"Electromethanogenesis is a form of electrofuel production where methane is produced by direct biological conversion of electrical current and carbon dioxide.Methane producing technologies garnered interest from the scientific community prior to 2000, but electromethanogenesis did not become a significant area of interest until 2008. Publications concerning catalytic methanation have increased from 44 to over 130 since 2008. Electromethanogenesis has drawn more research due to its proposed applications. The production of methane from electrical current may provide an approach to renewable energy storage. Electrical current produced from renewable energy sources may, through electromethanogenesis, be converted into methane which may then be used as a biofuel. It may also be a useful method for the capture of carbon dioxide which may be used for air purification.In nature, methane formation occurs biotically and abiotically. Abiogenic methane is produced on a smaller scale and the required chemical reactions do not necessitate organic materials. Biogenic methane is produced in anaerobic natural environments where methane forms as the result of the breakdown of organic materials by microbes—or microorganisms. Researchers have found that the biogenic methane production process can be replicated in a laboratory environment through electromethanogenesis. The reduction of CO2 in electromethanogenesis is facilitated by an electrical current at a biocathode in a microbial electrolysis cell (MEC) and with the help of microbes and electrons (Equation 1) or abiotically produced hydrogen (Equation 2).(1) CO2 + 8H+ + 8e− ↔ CH4 + 2H2O
(2) CO2 + 4H2 ↔ CH4 + 2H2O

",1
Life Science,Electroporation,"Electroporation, or electropermeabilization, is a microbiology technique in which an electrical field is applied to cells in order to increase the permeability of the cell membrane, allowing chemicals, drugs, electrode arrays or DNA to be introduced into the cell (also called electrotransfer). In microbiology, the process of electroporation is often used to transform bacteria, yeast, or plant protoplasts by introducing new coding DNA. If bacteria and plasmids are mixed together, the plasmids can be transferred into the bacteria after electroporation, though depending on what is being transferred, cell-penetrating peptides or CellSqueeze could also be used. Electroporation works by passing thousands of volts (~8 kV/cm) across suspended cells in an electroporation cuvette. Afterwards, the cells have to be handled carefully until they have had a chance to divide, producing new cells that contain reproduced plasmids. This process is approximately ten times more effective in increasing cell membrane's permeability than chemical transformation.Electroporation is also highly efficient for the introduction of foreign genes into tissue culture cells, especially mammalian cells. For example, it is used in the process of producing knockout mice, as well as in tumor treatment, gene therapy, and cell-based therapy.  The process of introducing foreign DNA into eukaryotic cells is known as transfection.  Electroporation is highly effective for transfecting cells in suspension using electroporation cuvettes. Electroporation has proven efficient for use on tissues in vivo, for in utero applications as well as in ovo transfection.  Adherent cells can also be transfected using electroporation, providing researchers with an alternative to trypsinizing their cells prior to transfection. One downside to electroporation, however, is that after the process the gene expression of over 7,000 genes can be affected. This can cause problems in studies where gene expression has to be controlled to ensure accurate and precise results.
Although bulk electroporation has many benefits over physical delivery methods such as microinjections and gene guns, it still has limitations, including low cell viability. Miniaturization of electroporation has been studied, leading to microelectroporation and nanotransfection of tissue utilizing electroporation-based techniques via nanochannels to minimally invasively deliver cargo to the cells.Electroporation has also been used as a mechanism to trigger cell fusion. Artificially induced cell fusion can be used to investigate and treat different diseases, like diabetes, regenerate axons of the central nerve system, and produce cells with desired properties, such as in cell vaccines for cancer immunotherapy. However, the first and most known application of cell fusion is production of monoclonal antibodies in hybridoma technology, where hybrid cell lines (hybridomas) are formed by fusing specific antibody-producing B lymphocytes with a myeloma (B lymphocyte cancer) cell line.",1
Life Science,Embryonic stem cell,"Embryonic stem cells (ES cells or ESCs) are pluripotent stem cells derived from the inner cell mass of a blastocyst, an early-stage pre-implantation embryo. Human embryos reach the blastocyst stage 4–5 days post fertilization, at which time they consist of 50–150 cells. Isolating the embryoblast, or inner cell mass (ICM) results in destruction of the blastocyst, a process which raises ethical issues, including whether or not embryos at the pre-implantation stage have the same moral considerations as embryos in the post-implantation stage of development.Researchers are currently focusing heavily on the therapeutic potential of embryonic stem cells, with clinical use being the goal for many laboratories. Potential uses include the treatment of diabetes and heart disease. The cells are being studied to be used as clinical therapies, models of genetic disorders, and cellular/DNA repair. However, adverse effects in the research and clinical processes such as tumors and unwanted immune responses have also been reported.",1
Life Science,Enadenotucirev,"Enadenotucirev is an investigational oncolytic virus that is in clinical trials for various cancers.It is an oncolytic A11/Ad3 Chimeric Group B Adenovirus, previously described as ColoAd1.Enadenotucirev has also been modified with additional genes using the tumor-specific immuno-gene therapy (T-SIGn) platform to develop novel cancer gene therapy agents.
The T-SIGn vectors at clinical study stage are:

NG-350A: This vector contains two transgenes expressing the heavy and light chains for a secreted CD40 agonist monoclonal antibody.
NG-641: This vector contains four transgenes expressing secreted Interferon alpha, the chemokines CXCL9, CXCL10 and an anti-FAP/anti-CD3 bispecific T-cell activatorIn Jan 2015 the European Medicines Agency's (EMA) Committee for Orphan Medical Products (COMP) designated enadenotucirev as an orphan medicinal product for the treatment of ovarian cancer.",1
Life Science,Engineering biology,"Engineering biology is the set of methods for designing, building, and testing engineered biological systems which have been used to manipulate information, construct materials, process chemicals, produce energy, provide food, and help maintain or enhance human health and environment.",1
Life Science,Enhanced biological phosphorus removal,"Enhanced biological phosphorus removal (EBPR) is a sewage treatment configuration applied to activated sludge systems for the removal of phosphate.The common element in EBPR implementations is the presence of an anaerobic tank (nitrate and oxygen are absent) prior to the aeration tank.  Under these conditions a group of heterotrophic bacteria, called polyphosphate-accumulating organisms (PAO) are selectively enriched in the bacterial community within the activated sludge. In the subsequent aerobic phase, these bacteria can accumulate large quantities of polyphosphate within their cells and the removal of phosphorus is said to be enhanced.Generally speaking, all bacteria contain a fraction (1-2%) of phosphorus in their biomass due to its presence in cellular components, such as membrane phospholipids and DNA. Therefore, as bacteria in a wastewater treatment plant consume nutrients in the wastewater, they grow and phosphorus is incorporated into the bacterial biomass. When PAOs grow they not only consume phosphorus for cellular components but also accumulate large quantities of polyphosphate within their cells. Thus, the phosphorus fraction of phosphorus accumulating biomass is 5-7%. In mixed bacterial cultures the phosphorus content will be maximal 3 - 4 % on total organic mass. If additional chemical precipitation takes place, for example to reach discharge limits, the P-content could be higher, but that is not affected by EBPR. This biomass is then separated from the treated (purified) water at end of the process and the phosphorus is thus removed.  Thus if PAOs are selectively enriched by the EBPR configuration, considerably more phosphorus is removed, compared to the relatively poor phosphorus removal in conventional activated sludge systems.

",1
Life Science,Environmental biotechnology,"Environmental biotechnology is biotechnology that is applied to and used to study the natural environment.  Environmental biotechnology could also imply that one try to harness biological process for commercial uses and exploitation.  The International Society for Environmental Biotechnology defines environmental biotechnology as ""the development, use and regulation of biological systems for remediation of contaminated environments (land, air, water), and for environment-friendly processes (green manufacturing technologies and sustainable development)"".Environmental biotechnology can simply be described as ""the optimal use of nature, in the form of plants, animals, bacteria, fungi and algae, to produce renewable energy, food and nutrients in a synergistic integrated cycle of profit making processes where the waste of each process becomes the feedstock for another process"".",1
Life Science,Eradication of suffering,The eradication or abolition of suffering is the concept of using biotechnology to create a permanent absence of pain and suffering in all sentient beings.,1
Life Science,Ethiopian Biotechnology Institute,"The Ethiopian Biotechnology Institute (EBTi) is a  research institute owned by the Government of Ethiopia. It was established in 2016 in Addis Ababa, Ethiopia .",1
Life Science,Expression vector,"An expression vector, otherwise known as an expression construct, is usually a plasmid or virus designed for gene expression in cells. The vector is used to introduce a specific gene into a target cell, and can commandeer the cell's mechanism for protein synthesis to produce the protein encoded by the gene.  Expression vectors are the basic tools in biotechnology for the production of proteins.
The vector is engineered to contain regulatory sequences that act as enhancer and promoter regions and lead to efficient transcription of the gene carried on the expression vector. The goal of a well-designed expression vector is the efficient production of protein, and this may be achieved by the production of significant amount of stable messenger RNA, which can then be translated into protein. The expression of a protein may be tightly controlled, and the protein is only produced in significant quantity when necessary through the use of an inducer, in some systems however the protein may be expressed constitutively. Escherichia coli is commonly used as the host for protein production, but other cell types may also be used. An example of the use of expression vector is the production of insulin, which is used for medical treatments of diabetes.

",1
Life Science,FasterCures,"FasterCures is a Washington, D.C.–based think tank that focuses on accelerating medical research. This healthcare-related non profit is the Milken Institute's Center for Accelerating Medical Solutions. FasterCures describes itself as an ""action tank"" that works on many diverse projects in collaboration with industry, academia, non profits, government agencies and philanthropic foundations. FasterCures' mission is to save lives by saving time in the discovery, development and deployment of new therapies for deadly and debilitating diseases. The executive director of FasterCures is Tanisha Carino.",1
Life Science,Fed-batch culture,"Fed-batch culture is, in the broadest sense,  defined as an operational technique in biotechnological processes where one or more nutrients (substrates) are fed (supplied) to the bioreactor during cultivation and in which the product(s) remain in the bioreactor until the end of the run.  An alternative description of the method is that of a culture in which ""a base medium supports initial cell culture and a feed medium is added to prevent nutrient depletion"".  It is also a type of semi-batch culture. In some cases, all the nutrients are fed into the bioreactor. The advantage of the fed-batch culture is that one can control concentration of fed-substrate in the culture liquid at arbitrarily desired levels (in many cases, at low levels).
Generally speaking, fed-batch culture is superior to conventional batch culture when controlling concentrations of a nutrient (or nutrients) affects the yield or productivity of the desired metabolite.

",1
Life Science,Female sperm,"The participation of women and girls in sports, physical fitness and exercise, has been recorded to have existed throughout history. However, participation rates and activities vary in accordance with nation, era, geography, and stage of economic development. While initially occurring informally, the modern era of organized sports did not begin to emerge either for men or women until the late industrial age.
Until roughly 1870, women's activities tended to be informal and recreational in nature, lacked rules codes, and emphasized physical activity rather than competition. Today, women's sports are more sport-specific and have developed into both amateur levels of sport and professional levels in various places internationally, but is found primarily within developed countries where conscious organization and accumulation of wealth has occurred. In the mid-to-latter part of the 20th century, female participation in sport and the popularization of their involvement increased, particularly during its last quarter. Very few organized sports have been invented by women. Sports such as Newcomb ball, netball, acrobatic gymnastics and tumbling, and possibly stoolball, are examples. A more recent example is BasKua.Sports involvement by women is more observable in well-developed countries and is often attributed to the presence of gender parity feminism, a feminist ideology popularized in the United States of America. Today the level of participation and performance still varies greatly by country and by sport. Despite an increase in women's participation in sport, the male demographic is still the larger of the two. These demographic differences are observed globally. Female dominated sports are the one exception. Girls' participation in sports tend to be higher in the United States than in other parts of the world like Western Europe and Latin America. Girls' participation in more violent contact sports is far less than that of their male counterparts.
Two important divisions exist in relation to female sporting categories. These sports either emerged exclusively as an organized  female sport or were developed as an organized female variant of a sport first popularized by a male demographic and therefore became a female category. In all but a few exceptional cases, such as in the case of camogie, a female variant, or ""women's game"" uses the same name of the sport popularly played by men, but is classified into a different category which is differentiated by sex: men's or women's, or girls or boys. Female variants are widely common while organized female sports by comparison are rare and include team sports such as netball, throwball, artistic (née synchronized) swimming, and ringette. In female sports, the supposed benefits of gender parity, gender equity, and gender equality feminism are controversial. Men dominate the top elite spots in the vast majority of sports worldwide due to their biological advantages and the deliberate exclusion of male athletes prevents male participants from dominating for that reason. The conscious exclusion of male athletes from female sports has enabled them to produce an elite level of female athletes rather than male. In addition, female sports provide women and girls with a unique advantage by affording them the opportunity to feature as the sport's primary athletes rather than have to compete with males for attention, an achievement undermined by the inclusion of males. The Canadian sport of ringette, created in 1963, is the last team sport in history to have been created exclusively for the female sex.
Today, female sports which have not yet become Olympic sports are blocked from IOC acceptance due to the fact that they must meet the IOC's gender parity quotas. Because the large majority of organized sports are first developed by and played predominantly by males, IOC gender parity strictly favours female variants despite their inability to pioneer an original sports model. Female sports by comparison face direct discrimination from the IOC due to the fact that female sports have a predominately female athlete base. As a result, they face IOC rejection regardless of their numbers because they are considered to be inadequate due to their female oriented programs, meaning they ""do not have enough men"", despite men dominating organized sports internationally. The IOC's Olympic Charter currently rejects any sport that isn't widely practiced by men in at least 75 countries and on 4 continents, and by women in 40 countries and on 3 continents. Due to the IOC's gender parity quotas, sports with a predominately male participation rate rather than female are automatically given priority status by the IOC. In addition, the Charter puts pressure on female sports federations to campaign for the inclusion of more male players rather than female, incentivizes male participation opportunities rather than female, and shuts female dominated sports like netball out.
Except in a few rare cases like women's professional tennis, professional women's sport rarely provide competitors with a livable income. In addition, competing for media coverage of the women's variant of a sport which is primarily popular among males, creates complex barriers. More recently, there has been an increasing amount of interest, research, investment and production in regards to equipment design for female athletes. Interest and research involving the identification of sex-specific injuries, particularly though not exclusively among high performance female athletes, has increased as well, such as in the case of concussions and the female athlete triad, a.k.a. ""Relative energy deficiency in sport"", (RED-S).At times female athletes have engaged in social activism in conjunction with their participation in sport. Protest methods have included playing strikes, social media campaigns, and in the case of America, federal lawsuits on grounds of inequality, usually as it relates to gender parity principles, American law and Title IX. Public service oriented promotional campaigns for girls in sport involve a variety of media campaign styles.",1
Life Science,Fermentek,"Fermentek Ltd. is a biotechnological company in the Atarot industrial zone of Jerusalem, Israel. It specializes in the research, development and manufacture of biologically active, natural products isolated from microorganisms as well as from other natural sources such as plants and algae.
The main microorganisms used are nonpathogenic actinomycetes, Nocardia and Streptomycetes. The fungi used are: Penicillium, Aspergillus, Fusarium and the like. None of these is a human pathogen.
Fermentek does not sell to individuals. Most of its products are marketed through major international distributors specializing in chemicals, under their own brand names. Nevertheless, Fermentek has specific impact on the biochemical market, especially in the field of mycotoxins.
Mycotoxins are toxic compounds produced by molds in human food and farm animal feeds, thus being economically important factors. Fermentek manufactures an extensive line of pure mycotoxins used as standards in food analysis. In some cases, such as Aflatoxin M2, Fermentek supplies the entire world's requirements.In 2009, Fermentek announced a product family of highly standardized calibrant solutions of main mycotoxins. These are marketed under the brand name FermaSol. In 2010, it obtained ISO 13485 accreditation in connection with the production of starting materials for experimental drug production, and with manufacturing of reference standards of food contaminants.
None of Fermentek's products have been invented by it. Fermentek's aim is to make known compounds affordable to the scientific community.
Fermentek was founded by Dr. Yosef Behrend in 1994. It moved in 2004 to its new building, quadrupling its working space and greatly enlarging its manufacturing capacities.

",1
Life Science,Fertility fraud,"Fertility fraud is the failure on the part of a fertility doctor to obtain consent from a patient before inseminating them with his own sperm.  This normally occurs in the context of people using assisted reproductive technology (ART) to address fertility issues.
The term is also used in cases where donor eggs are used without consent and more broadly, to instances where doctors and other medical professionals exploit opportunities that arise when people use assisted reproductive technology to address fertility issues. This may give rise to a number of different types of fraud involving insurance, unnecessary procedures, theft of eggs, and other issues related to fertility treatment.

",1
Life Science,FIND Technology,FIND® technology is a directed evolution technology that uses DNA recombination to improve properties of proteins. It eliminates unimportant and deleterious mutations while maintaining and combining beneficial mutations that would enhance protein function.,1
Life Science,Fixation (histology),"In the fields of histology, pathology, and cell biology, fixation is the preservation of biological tissues from decay due to autolysis or putrefaction. It terminates any ongoing biochemical reactions and may also increase the treated tissues' mechanical strength or stability. Tissue fixation is a critical step in the preparation of histological sections, its broad objective being to preserve cells and tissue components and to do this in such a way as to allow for the preparation of thin, stained sections. This allows the investigation of the tissues' structure, which is determined by the shapes and sizes of such macromolecules (in and around cells) as proteins and nucleic acids. 

",1
Life Science,Flap endonuclease,"Flap endonucleases (FENs, also known as 5' nucleases in older references) are a class of nucleolytic enzymes that act as both 5'-3' exonucleases and structure-specific endonucleases on specialised DNA structures that occur during the biological processes of DNA replication, DNA repair, and DNA recombination. Flap endonucleases have been identified in eukaryotes, prokaryotes, archaea, and some viruses. Organisms can have more than one FEN homologue; this redundancy may give an indication of the importance of these enzymes. In prokaryotes, the FEN enzyme is found as an N-terminal domain of DNA polymerase I, but some prokaryotes appear to encode a second homologue.The endonuclease activity of FENs was initially identified as acting on a DNA duplex which has a single-stranded 5' overhang on one of the strands (termed a ""5' flap"", hence the name flap endonuclease). FENs catalyse hydrolytic cleavage of the phosphodiester bond at the junction of single- and double-stranded DNA. Some FENs can also act as 5'-3' exonucleases on the 5' terminus of the flap strand and on 'nicked' DNA substrates.
Protein structure models based on X-ray crystallography data suggest that FENs have a flexible arch created by two α-helices through which the single 5' strand of the 5' flap structure can thread.Flap endonucleases have been used in biotechnology, for example the Taqman PCR assay  and the Invader Assay for mutation and single nucleotide polymorphism (SNP) detection.

",1
Life Science,Flavr Savr,"Flavr Savr (also known as CGN-89564-2; pronounced ""flavor saver""), a genetically modified tomato, was the first commercially grown genetically engineered food to be granted a license for human consumption. It was developed by the Californian company Calgene in the 1980s. The tomato has an improved shelf-life, increased fungal resistance and a slightly increased viscosity compared to its non-modified counterpart. It was meant to be harvested ripe for increased flavor for long-distance shipping. The Flavr Savr contains two genes added by Calgene; a reversed antisense polygalacturonase gene which inhibits the production of the aforementioned rotting enzyme and a gene responsible for the creation of APH(3')II, which confers resistance to certain aminoglycoside antibiotics including kanamycin and neomycin. The product was submitted to the U.S. Food and Drug Administration (FDA) in 1992. On May 18, 1994, the FDA completed its evaluation of the Flavr Savr tomato and the use of APH(3')II, concluding that the tomato ""is as safe as tomatoes bred by conventional means"" and ""that the use of aminoglycoside 3'-phosphotransferase II is safe for use as a processing aid in the development of new varieties of tomato, rapeseed oil, and cotton intended for food use."" It was first sold in 1994, and was only available for a few years before production ceased in 1997. Calgene made history, but mounting costs prevented the company from becoming profitable, and it was eventually acquired by Monsanto Company.",1
Life Science,FlowFET,"A flowFET is a microfluidic component which allows the rate of flow of liquid in a microfluidic channel to be modulated by the electrical potential applied to it. In this way, it behaves as a microfluidic analogue to  the field effect transistor, except that in the flowFET the flow of liquid takes the place of the flow of electric current. Indeed, the name of the flowFET is derived from the naming convention of electronic FETs (e.g. MOSFET, FINFET etc.).",1
Life Science,Fluorescent glucose biosensor,"Fluorescent glucose biosensors are devices that measure the concentration of glucose in diabetic patients by means of sensitive protein that relays the concentration by means of fluorescence, an alternative to amperometric sension of glucose. Due to the prevalence of diabetes, it is the prime drive in the construction of fluorescent biosensors. A recent development has been approved by the FDA allowing a new continuous glucose monitoring system called EverSense, which is a 90-day glucose monitor using fluorescent biosensors.",1
Life Science,Folate targeting,"Folate targeting is a method utilized in biotechnology for drug delivery purposes.  This Trojan Horse process, which was created by Drs. Christopher P. Leamon and Philip S. Low, involves the attachment of the vitamin, folate (folic acid), to a molecule/drug to form a ""folate conjugate"".  Based on the natural high affinity of folate for the folate receptor protein (FR), which is commonly expressed on the surface of many human cancers, folate-drug conjugates also bind tightly to the FR and trigger cellular uptake via endocytosis.  Molecules as diverse as small radiodiagnostic imaging agents to large DNA plasmid formulations have successfully been delivered inside FR-positive cells and tissues.",1
Life Science,Fragment-based lead discovery,"Fragment-based lead discovery (FBLD) also known as fragment-based drug discovery (FBDD) is a method used for finding lead compounds as part of the drug discovery process. Fragments are small organic molecules which are small in size and low in molecular weight. It is based on identifying small chemical fragments, which may bind only weakly to the biological target, and then growing them or combining them to produce a lead with a higher affinity. FBLD can be compared with high-throughput screening (HTS). In HTS, libraries with up to millions of compounds, with molecular weights of around 500 Da, are screened, and nanomolar binding affinities are sought. In contrast, in the early phase of FBLD, libraries with a few thousand compounds with molecular weights of around 200 Da may be screened, and millimolar affinities can be considered useful. FBLD is a technique being used in research for discovering novel potent inhibitors. This methodology could help to design multitarget drugs for multiple diseases. The multitarget inhibitor approach is based on designing an inhibitor for the multiple targets. This type of drug design opens up new polypharmacological avenues for discovering innovative and effective therapies. Neurodegenerative diseases like Alzheimer’s (AD) and Parkinson’s, among others, also show rather complex etiopathologies. Multitarget inhibitors are more appropriate for addressing the complexity of AD and may provide new drugs for controlling the multifactorial nature of AD, stopping its progression.",1
Life Science,Function-spacer-lipid Kode construct,"Function-Spacer-Lipid (FSL) Kode constructs (Kode Technology) are amphiphatic, water dispersible biosurface engineering constructs that can be used to engineer the surface of cells, viruses and organisms, or to modify solutions and non-biological surfaces with bioactives. FSL Kode constructs spontaneously and stably incorporate into cell membranes. FSL Kode constructs with all these aforementioned features are also known as Kode Constructs. The process of modifying surfaces with FSL Kode constructs is known as ""koding"" and the resultant ""koded"" cells, viruses and liposomes are respectively known as kodecytes, and kodevirions.",1
Life Science,Fungiculture,"Fungiculture is the cultivation of mushrooms and other fungi. Cultivating fungi can yield food, medicine, construction materials and other products. A mushroom farm is in the business of growing fungi.
The word is also commonly used to refer to the practice of cultivating fungi by leafcutter ants, termites, ambrosia beetles, and marsh periwinkles.

",1
Life Science,Gene delivery,"Gene delivery is the process of introducing foreign genetic material, such as DNA or RNA, into host cells. Gene delivery must reach the genome of the host cell to induce gene expression. Successful gene delivery requires the foreign gene delivery to remain stable within the host cell and can either integrate into the genome or replicate independently of it. This requires foreign DNA to be synthesized as part of a vector, which is designed to enter the desired host cell and deliver the transgene to that cell's genome. Vectors utilized as the method for gene delivery can be divided into two categories, recombinant viruses and synthetic vectors (viral and non-viral).In complex multicellular eukaryotes (more specifically Weissmanists), if the transgene is incorporated into the host's germline cells, the resulting host cell can pass the transgene to its progeny. If the transgene is incorporated into somatic cells, the transgene will stay with the somatic cell line, and thus its host organism.Gene delivery is a necessary step in gene therapy for the introduction or silencing of a gene to promote a therapeutic outcome in patients and also has applications in the genetic modification of crops. There are many different methods of gene delivery for various types of cells and tissues.

",1
Life Science,Gene electrotransfer,"Electroporation, or electropermeabilization, is a microbiology technique in which an electrical field is applied to cells in order to increase the permeability of the cell membrane, allowing chemicals, drugs, electrode arrays or DNA to be introduced into the cell (also called electrotransfer). In microbiology, the process of electroporation is often used to transform bacteria, yeast, or plant protoplasts by introducing new coding DNA. If bacteria and plasmids are mixed together, the plasmids can be transferred into the bacteria after electroporation, though depending on what is being transferred, cell-penetrating peptides or CellSqueeze could also be used. Electroporation works by passing thousands of volts (~8 kV/cm) across suspended cells in an electroporation cuvette. Afterwards, the cells have to be handled carefully until they have had a chance to divide, producing new cells that contain reproduced plasmids. This process is approximately ten times more effective in increasing cell membrane's permeability than chemical transformation.Electroporation is also highly efficient for the introduction of foreign genes into tissue culture cells, especially mammalian cells. For example, it is used in the process of producing knockout mice, as well as in tumor treatment, gene therapy, and cell-based therapy.  The process of introducing foreign DNA into eukaryotic cells is known as transfection.  Electroporation is highly effective for transfecting cells in suspension using electroporation cuvettes. Electroporation has proven efficient for use on tissues in vivo, for in utero applications as well as in ovo transfection.  Adherent cells can also be transfected using electroporation, providing researchers with an alternative to trypsinizing their cells prior to transfection. One downside to electroporation, however, is that after the process the gene expression of over 7,000 genes can be affected. This can cause problems in studies where gene expression has to be controlled to ensure accurate and precise results.
Although bulk electroporation has many benefits over physical delivery methods such as microinjections and gene guns, it still has limitations, including low cell viability. Miniaturization of electroporation has been studied, leading to microelectroporation and nanotransfection of tissue utilizing electroporation-based techniques via nanochannels to minimally invasively deliver cargo to the cells.Electroporation has also been used as a mechanism to trigger cell fusion. Artificially induced cell fusion can be used to investigate and treat different diseases, like diabetes, regenerate axons of the central nerve system, and produce cells with desired properties, such as in cell vaccines for cancer immunotherapy. However, the first and most known application of cell fusion is production of monoclonal antibodies in hybridoma technology, where hybrid cell lines (hybridomas) are formed by fusing specific antibody-producing B lymphocytes with a myeloma (B lymphocyte cancer) cell line.",1
Life Science,Gene knock-in,"In molecular cloning and biology, a gene knock-in (abbreviation: KI) refers to a genetic engineering method that involves the one-for-one substitution of DNA sequence information in a genetic locus or the insertion of sequence information not found within the locus. Typically, this is done in mice since the technology for this process is more refined and there is a high degree of shared sequence complexity between mice and humans. The difference between knock-in technology and traditional transgenic techniques is that a knock-in involves a gene inserted into a specific locus, and is thus a ""targeted"" insertion. It is the opposite of gene knockout.
A common use of knock-in technology is for the creation of disease models. It is a technique by which scientific investigators may study the function of the regulatory machinery (e.g. promoters) that governs the expression of the natural gene being replaced. This is accomplished by observing the new phenotype of the organism in question. The BACs and YACs are used in this case so that large fragments can be transferred.",1
Life Science,Gene knockout,"A gene knockout (abbreviation: KO) is a genetic technique in which one of an organism's genes is made inoperative (""knocked out"" of the organism). However, KO can also refer to the gene that is knocked out or the organism that carries the gene knockout. Knockout organisms or simply knockouts are used to study gene function, usually by investigating the effect of gene loss. Researchers draw inferences from the difference between the knockout organism and normal individuals.
The KO technique is essentially the opposite of a gene knock-in. Knocking out two genes simultaneously in an organism is known as a double knockout (DKO). Similarly the terms triple knockout (TKO) and quadruple knockouts (QKO) are used to describe three or four knocked out genes, respectively. However, one needs to distinguish between heterozygous and homozygous KOs. In the former, only one of two gene copies (alleles) is knocked out, in the latter both are knocked out.

",1
Life Science,Gene therapy,"Gene therapy is a medical field which focuses on the genetic modification of cells to produce a therapeutic effect or the treatment of disease by repairing or reconstructing defective genetic material. The first attempt at modifying human DNA was performed in 1980, by Martin Cline, but the first successful nuclear gene transfer in humans, approved by the National Institutes of Health, was performed in May 1989. The first therapeutic use of gene transfer as well as the first direct insertion of human DNA into the nuclear genome was performed by French Anderson in a trial starting in September 1990. It is thought to be able to cure many genetic disorders or treat them over time.
Between 1989 and December 2018, over 2,900 clinical trials were conducted, with more than half of them in phase I. As of 2017, Spark Therapeutics' Luxturna (RPE65 mutation-induced blindness) and Novartis' Kymriah (Chimeric antigen receptor T cell therapy) are the FDA's first approved gene therapies to enter the market. Since that time, drugs such as Novartis' Zolgensma and Alnylam's Patisiran have also received FDA approval, in addition to other companies' gene therapy drugs. Most of these approaches utilize adeno-associated viruses (AAVs) and lentiviruses for performing gene insertions, in vivo and ex vivo, respectively. AAVs are characterized by stabilizing the viral capsid, lower immunogenicity, ability to transduce both dividing and nondividing cells, the potential to integrate site specifically and to achieve long-term expression in the in-vivo treatment. (Gorell et al. 2014) ASO / siRNA approaches such as those conducted by Alnylam and Ionis Pharmaceuticals require non-viral delivery systems, and utilize alternative mechanisms for trafficking to liver cells by way of GalNAc transporters.
The concept of gene therapy is to fix a genetic problem at its source. If, for instance, a mutation in a certain gene causes the production of a dysfunctional protein resulting (usually recessively) in an inherited disease, gene therapy could be used to deliver a copy of this gene that does not contain the deleterious mutation and thereby produces a functional protein. This strategy is referred to as gene replacement therapy and is employed to treat inherited retinal diseases.While the concept of gene replacement therapy is mostly suitable for recessive diseases, novel strategies have been suggested that are capable of also treating conditions with a dominant pattern of inheritance.

The introduction of CRISPR gene editing has opened new doors for its application and utilization in gene therapy, as instead of pure replacement of a gene, it enables correction of the particular genetic defect. Solutions to medical hurdles, such as the eradication of latent human immunodeficiency virus (HIV) reservoirs and correction of the mutation that causes sickle cell disease, may be available as a therapeutic option in the future.
Prosthetic gene therapy aims to enable cells of the body to take over functions they physiologically do not carry out. One example is the so-called vision restoration gene therapy, that aims to restore vision in patients with end-stage retinal diseases. In end-stage retinal diseases, the photoreceptors, as the primary light sensitive cells of the retina are irreversibly lost. By the means of prosthetic gene therapy light sensitive proteins are delivered into the remaining cells of the retina, to render them light sensitive and thereby enable them to signal visual information towards the brain. Clinical trials are ongoing. (NCT02556736, NCT03326336 at clinicaltrials.gov)Not all medical procedures that introduce alterations to a patient's genetic makeup can be considered gene therapy. Bone marrow transplantation and organ transplants in general have been found to introduce foreign DNA into patients.",1
Life Science,Template:Genealogical DNA test,"Initial visibility: currently defaults to autocollapse
To set this template's initial visibility, the |state= parameter may be used:

|state=collapsed: {{Genealogical DNA test|state=collapsed}} to show the template collapsed, i.e., hidden apart from its title bar
|state=expanded: {{Genealogical DNA test|state=expanded}} to show the template expanded, i.e., fully visible
|state=autocollapse: {{Genealogical DNA test|state=autocollapse}}
shows the template collapsed to the title bar if there is a {{navbar}}, a {{sidebar}}, or some other table on the page with the collapsible attribute
shows the template in its expanded state if there are no other collapsible items on the page
If the |state= parameter in the template on this page is not set, the template's initial visibility is taken from the |default= parameter in the Collapsible option template. For the template on this page, that currently evaluates to autocollapse.",1
Life Science,Genetic engineering,"Genetic engineering, also called genetic modification or genetic manipulation, is the direct manipulation of an organism's genes using biotechnology. It is a set of technologies used to change the genetic makeup of cells, including the transfer of genes within and across species boundaries to produce improved or novel organisms. New DNA is obtained by either isolating and copying the genetic material of interest using recombinant DNA methods or by artificially synthesising the DNA. A construct is usually created and used to insert this DNA into the host organism. The first recombinant DNA molecule was made by Paul Berg in 1972 by combining DNA from the monkey virus SV40 with the lambda virus. As well as inserting genes, the process can be used to remove, or ""knock out"", genes. The new DNA can be inserted randomly, or targeted to a specific part of the genome.An organism that is generated through genetic engineering is considered to be genetically modified (GM) and the resulting entity is a genetically modified organism (GMO). The first GMO was a bacterium generated by Herbert Boyer and Stanley Cohen in 1973. Rudolf Jaenisch created the first GM animal when he inserted foreign DNA into a mouse in 1974. The first company to focus on genetic engineering, Genentech, was founded in 1976 and started the production of human proteins. Genetically engineered human insulin was produced in 1978 and insulin-producing bacteria were commercialised in 1982. Genetically modified food has been sold since 1994, with the release of the Flavr Savr tomato. The Flavr Savr was engineered to have a longer shelf life, but most current GM crops are modified to increase resistance to insects and herbicides. GloFish, the first GMO designed as a pet, was sold in the United States in December 2003. In 2016 salmon modified with a growth hormone were sold.
Genetic engineering has been applied in numerous fields including research, medicine, industrial biotechnology and agriculture. In research GMOs are used to study gene function and expression through loss of function, gain of function, tracking and expression experiments. By knocking out genes responsible for certain conditions it is possible to create animal model organisms of human diseases. As well as producing hormones, vaccines and other drugs, genetic engineering has the potential to cure genetic diseases through gene therapy. The same techniques that are used to produce drugs can also have industrial applications such as producing enzymes for laundry detergent, cheeses and other products.
The rise of commercialised genetically modified crops has provided economic benefit to farmers in many different countries, but has also been the source of most of the controversy surrounding the technology. This has been present since its early use; the first field trials were destroyed by anti-GM activists. Although there is a scientific consensus that currently available food derived from GM crops poses no greater risk to human health than conventional food, GM food safety is a leading concern with critics. Gene flow, impact on non-target organisms, control of the food supply and intellectual property rights have also been raised as potential issues. These concerns have led to the development of a regulatory framework, which started in 1975. It has led to an international treaty, the Cartagena Protocol on Biosafety, that was adopted in 2000. Individual countries have developed their own regulatory systems regarding GMOs, with the most marked differences occurring between the US and Europe.",1
Life Science,Genetically encoded voltage indicator,"Genetically encoded voltage indicator (or GEVI) is a protein that can sense membrane potential in a cell and relate the change in voltage to a form of output, often fluorescent level. It is a promising optogenetic recording tool that enables exporting electrophysiological signals from cultured cells, live animals, and ultimately human brain. Examples of notable GEVIs include ArcLight, ASAP1, ASAP3, Archons, SomArchon, and Ace2N-mNeon.",1
Life Science,GeneXpert Infinity,"The GeneXpert Infinity is an automated cartridge-based nucleic acid amplification test (NAAT) which is able to tell whether the subject fluid contains shreds of the SARS-CoV-2 virus, amongst others. It is manufactured by Cepheid Inc.",1
Life Science,GenoCAD,"GenoCAD is one of the earliest computer assisted design tools for synthetic biology. The software is a bioinformatics tool developed and maintained by GenoFAB, Inc.. GenoCAD facilitates the design of protein expression vectors, artificial gene networks and other genetic constructs for genetic engineering and is based on the theory of formal languages. GenoCAD can be used online by accessing the GenoFAB Client Portal at https://genofab.com/.",1
Life Science,Genome Project-Write,"The Genome Project - Write (also known as GP-Write) is a large-scale collaborative research project (an extension of Genome Projects, aimed at reading genomes since 1984) that focuses on the development of technologies for the synthesis and testing of genomes of many different species of microbes, plants, and animals, including the human genome in a sub-project known as Human Genome Project-Write (HGP-Write). Formally announced on 2 June 2016, the project leverages two decades of work on synthetic biology and artificial gene synthesis.
The newly created GP-Write project will be managed by the Center of Excellence for Engineering Biology, an American nonprofit organization. Researchers expect that the ability to artificially synthesize large portions of many genomes will result in many scientific and medical advances.",1
Life Science,Genome sequencing of endangered species,"Genome sequencing of endangered species is the application of Next Generation Sequencing (NGS) technologies in the field of conservative biology, with the aim of generating life history, demographic and phylogenetic data of relevance to the management of endangered wildlife.",1
Life Science,Genome Valley,"Genome Valley is an Indian high-technology business district spread across 600 km² in Hyderabad, India. It is located across the suburbs, Turakapally, Shamirpet, Medchal, Uppal, Patancheru, Jeedimetla, Gachibowli and Keesara. The Genome Valley has developed as a cluster for Biomedical research, training and manufacturing.",1
Life Science,Genotyping by sequencing,"In the field of genetic sequencing, genotyping by sequencing, also called GBS, is a method to discover single nucleotide polymorphisms (SNP) in order to perform genotyping studies, such as genome-wide association studies (GWAS). GBS uses restriction enzymes to reduce genome complexity and genotype multiple DNA samples. After digestion, PCR is performed to increase fragments pool and then GBS libraries are sequenced using next generation sequencing technologies, usually resulting in about 100bp single-end reads. It is relatively inexpensive and has been used in plant breeding. Although GBS presents an approach similar to restriction-site-associated DNA sequencing (RAD-seq) method, they differ in some substantial ways.

",1
Life Science,Hachimoji DNA,"Hachimoji DNA (from Japanese 八文字 hachimoji, ""eight letters"") is a synthetic nucleic acid analog that uses four synthetic nucleotides in addition to the four present in the natural nucleic acids, DNA and RNA.  This leads to four allowed base pairs: two unnatural base pairs formed by the synthetic nucleobases in addition to the two normal pairs.  Hachimoji bases have been demonstrated in both DNA and RNA analogs, using deoxyribose and ribose respectively as the backbone sugar.Benefits of such a nucleic acid system may include an enhanced ability to store data, as well as insights into what may be possible in the search for extraterrestrial life.The hachimoji DNA system produced one type of catalytic RNA (ribozyme or aptamer) in vitro.",1
Life Science,Helicase-dependent amplification,Helicase-dependent amplification (HDA) is a method for in vitro DNA amplification (like the polymerase chain reaction) that takes place at a constant temperature.,1
Life Science,Helicos single molecule fluorescent sequencing,"The Helicos Genetic Analysis System platform was the first commercial NGS (Next Generation Sequencing) implementation to use the principle of single molecule fluorescent sequencing, a method of identifying the exact sequence of a piece of DNA. It was marketed by the now defunct Helicos Biosciences.
The fragments of DNA molecules are first hybridized in place on disposable glass flow cells. Fluorescent nucleotides are then added one-by-one, with a terminating nucleotide used to pause the process until an image has been captured. From the image, one nucleotide from each DNA sequence can be determined. The fluorescent molecule is then cut away, and the process is repeated until the fragments have been completely sequenced.This sequencing method and equipment were used to sequence the genome of the M13 bacteriophage.",1
Life Science,Heterologous expression,"Heterologous expression refers to the expression of a gene or part of a gene in a host organism which does not naturally have this gene or gene fragment. Insertion of the gene in the heterologous host is performed by recombinant DNA technology. After being inserted in the host, the gene may be integrated into the host DNA, causing permanent expression, or not integrated, causing transient expression. Heterologous expression can be done in many type of host organisms. The host organism can be a bacterium, yeast, mammalian cell, or plant cell. This host is called the ""expression system"".
Homologous expression, on the other hand, refers to the overexpression of a gene in a system from where it originates.
Genes are subjected to heterologous expression often to study specific protein interactions. E. coli, yeast (S. cerevisiae, P. pastoris), immortalized mammalian cells, and amphibian oocytes (i.e. unfertilized eggs) are commonly for studies that require heterologous expression.",1
Life Science,High Resolution Melt,"High Resolution Melt (HRM) analysis is a powerful technique in molecular biology for the detection of mutations, polymorphisms and epigenetic differences in double-stranded DNA samples.  It was discovered and developed by Idaho Technology and the University of Utah. It has advantages over other genotyping technologies, namely:

It is cost-effective vs. other genotyping technologies such as sequencing and TaqMan SNP typing. This makes it ideal for large scale genotyping projects.
It is fast and powerful thus able to accurately genotype many samples rapidly.
It is simple.  With a good quality HRM assay, powerful genotyping can be performed by non-geneticists in any laboratory with access to an HRM capable real-time PCR machine.

",1
Life Science,High-performance Integrated Virtual Environment,"The High-performance Integrated Virtual Environment (HIVE) is a distributed computing environment used for healthcare-IT and biological research, including analysis of Next Generation Sequencing (NGS) data, preclinical, clinical and post market data, adverse events, metagenomic data, etc.  Currently it is supported and continuously developed by US Food and Drug Administration (government domain), George Washington University (academic domain), and by DNA-HIVE, WHISE-Global and Embleema (commercial domain). HIVE currently operates fully functionally within the US FDA supporting wide variety (+60) of regulatory research and regulatory review projects as well as for supporting MDEpiNet medical device postmarket registries. Academic deployments of HIVE are used for research activities and publications in NGS analytics, cancer research, microbiome research and in educational programs for students at GWU. Commercial enterprises use HIVE for oncology, microbiology, vaccine manufacturing, gene editing,  healthcare-IT, harmonization of real-world data, in preclinical research and clinical studies.

",1
Life Science,Homing endonuclease,"The homing endonucleases are a collection of endonucleases encoded either as freestanding genes within introns, as fusions with host proteins, or as self-splicing inteins. They catalyze the hydrolysis of genomic DNA within the cells that synthesize them, but do so at very few, or even singular, locations. Repair of the hydrolyzed DNA by the host cell frequently results in the gene encoding the homing endonuclease having been copied into the cleavage site, hence the term 'homing' to describe the movement of these genes. Homing endonucleases can thereby transmit their genes horizontally within a host population, increasing their allele frequency at greater than Mendelian rates.

",1
Life Science,Host cell protein,"Host cell proteins (HCPs) are process-related protein impurities that are produced by the host organism during biotherapeutic manufacturing and production. During the purification process, a majority of produced HCPs are removed from the final product (>99% of impurities removed). However, residual HCPs still remain in the final distributed pharmaceutical drug. Examples of HCPs that may remain in the desired pharmaceutical product include: monoclonal antibodies (mAbs), antibody-drug-conjugates (ADCs), therapeutic proteins, vaccines, and other protein-based biopharmaceuticals.HCPs may cause immunogenicity in individuals or reduce the potency, stability or overall effectiveness of a drug. National regulatory organisations, such as the FDA and EMA provide guidelines on acceptable levels of HCPs that may remain in pharmaceutical products before they are made available to the public. Currently, the acceptable level of HCPs in pharmaceutical drugs range from 1-100ppm (1–100 ng/mg product). However, the accepted level of HCPs in a final product is evaluated on a case-by-case basis, and depends on multiple factors including: dose, frequency of drug administration, type of drug and severity of disease.
The acceptable range of HCPs in a final pharmaceutical product is large due to limitations with the detection and analytical methods that currently exist. Analysis of HCPs is complex as the HCP mixture consists of a large variety of protein species, all of which are unique to the specific host organisms, and unrelated to the intended and desired recombinant protein. Analysing these large varieties of protein species at very minute concentrations is difficult and requires extremely sensitive equipment which has not been fully developed yet. The reason that HCP levels need to be monitored is due to the uncertain effects they have on the body. At trace amounts, the effects of HCPs on patients are unknown and specific HCPs may affect protein stability and drug effectiveness, or cause immunogenicity in patients. If the stability of the drug is affected, durability of the active substance in the pharmaceutical product could decrease. The effects that the drug is intended to have on patients could also possibly be increased or decreased, leading to health complications that may arise. The degree of immunogenicity on a long-term basis is difficult, and almost impossible, to determine and consequences can include severe threats to the patient’s health.",1
Life Science,Human cloning,"Human cloning is the creation of a genetically identical copy (or clone) of a human. The term is generally used to refer to artificial human cloning, which is the reproduction of human cells and tissue. It does not refer to the natural conception and delivery of identical twins. The possibility of human cloning has raised controversies. These ethical concerns have prompted several nations to pass laws regarding human cloning.
Two commonly discussed types of human cloning are therapeutic cloning and reproductive cloning.
Therapeutic cloning would involve cloning cells from a human for use in medicine and transplants. It is an active area of research, but is not in medical practice anywhere in the world, as of 2022. Two common methods of therapeutic cloning that are being researched are somatic-cell nuclear transfer and (more recently) pluripotent stem cell induction.
Reproductive cloning would involve making an entire cloned human, instead of just specific cells or tissues.",1
Life Science,Human genetic enhancement,"Human genetic enhancement or human genetic engineering refers to human enhancement by means of a genetic modification. This could be done in order to cure diseases (gene therapy), prevent the possibility of getting a particular disease (similarly to vaccines),  to improve athlete performance in sporting events (gene doping), or to change physical appearance, metabolism, and even improve physical capabilities and mental faculties such as memory and intelligence.
These genetic enhancements may or may not be done in such a way that the change is heritable (which has raised concerns within the scientific community).

",1
Life Science,Human Genome Project,"The  Human Genome Project (HGP) was an international scientific research project with the goal of determining the  base pairs that make up human DNA, and of identifying, mapping and sequencing all of the  genes of the human genome from both a physical and  a functional standpoint. It remains the world's largest collaborative biological project. Planning started after the idea was picked up in 1984 by the US government, the project formally launched in 1990, and was declared essentially complete on April 14, 2003, but included only about 85% of the genome. Level ""complete genome"" was achieved in May 2021, with a remaining only 0.3% bases covered by potential issues. The missing Y chromosome was added in January 2022.
Funding came from the American government through the National Institutes of Health (NIH) as well as numerous other groups from around the world. A parallel project was conducted outside the government by the Celera Corporation, or Celera Genomics, which was formally launched in 1998. Most of the government-sponsored sequencing was performed in twenty universities and research centres in the United States, the United Kingdom, Japan, France, Germany, and China.The Human Genome Project originally aimed to map the nucleotides contained in a human haploid reference genome (more than three billion).  The ""genome"" of any given individual is unique; mapping the ""human genome"" involved sequencing a small number of individuals and then assembling  to get a complete sequence for each chromosome. Therefore, the finished human genome is a mosaic, not representing any one individual.  The utility of the project comes from the fact that the vast majority of the human genome is the same in all humans.",1
Life Science,Human HGF plasmid DNA therapy,"Human HGF plasmid DNA therapy of cardiomyocytes is being examined as a potential treatment for coronary artery disease (a major cause of myocardial infarction (MI)), as well as treatment for the damage that occurs to the heart after MI.  After MI, the myocardium suffers from reperfusion injury which leads to death of cardiomyocytes and detrimental remodelling of the heart, consequently reducing proper cardiac function. Transfection of cardiac myocytes with human HGF reduces ischemic reperfusion injury after MI. The benefits of HGF therapy include preventing improper remodelling of the heart and ameliorating heart dysfunction post-MI.",1
Life Science,Human Medicines Regulations 2012,"The Human Medicines Regulations 2012  in the United Kingdom were created, under statutory authority of the European Communities Act 1972 and the Medicines Act 1968 in 2012. The body responsible for their upkeep is the Medicines and Healthcare products Regulatory Agency. The regulations partially repealed the Medicines Act 1968 in line with EU legislation.",1
Life Science,Hyderabad Pharma City,"Hyderabad Pharma City, is the ""world’s largest pharmaceuticals industrial park"" being established near Hyderabad, India by the Government of Telangana. Spread over 19,000 acres, the Park is touted to be biggest of its kind industrial cluster for the pharmaceuticals for pharmaceutical companies in manufacturing and development needs.Telangana Government has embarked on this project. It is expected to attract USD 9.7 billion Investment and generate employment to 560,000 people.Hyderabad Pharma City is an integrated facility being set up with effluent treatment facilities, solid waste management, secured landfill, use of natural gas for heating requirement thereby ensuring minimum air pollution, incinerators, testing facilities, real-time monitoring and control center, etc.

",1
Life Science,Hyperhydricity,"Hyperhydricity (previously known as vitrification) is a physiological malformation that results in excessive hydration, low lignification, impaired stomatal function and reduced mechanical strength of tissue culture-generated plants. The consequence is poor regeneration of such plants without intensive greenhouse acclimation for outdoor growth.  Additionally, it may also lead to leaf-tip and bud necrosis in some cases, which often leads to loss of apical dominance in the shoots. In general, the main symptom of hyperhydricity is translucent characteristics signified by a shortage of chlorophyll and high water content. Specifically, the presence of a thin or absent cuticular layer, reduced number of palisade cells, irregular stomata, less developed cell wall and large intracellular spaces in the mesophyll cell layer have been described as some of the anatomic changes associated with hyperhydricity.",1
Life Science,I-CreI,"I-CreI is a homing endonuclease whose gene was first discovered in the chloroplast genome of Chlamydomonas reinhardtii, a species of unicellular green algae. It is named for the facts that: it resides in an Intron; it was isolated from Clamydomonas reinhardtii; it was the first (I) such gene isolated from C. reinhardtii. Its gene resides in a group I intron in the 23S ribosomal RNA gene of the C. reinhardtii chloroplast, and I-CreI is only expressed when its mRNA is spliced from the primary transcript of the 23S gene. I-CreI enzyme, which functions as a homodimer, recognizes a 22-nucleotide sequence of duplex DNA and cleaves one phosphodiester bond on each strand at specific positions. I-CreI is a member of the LAGLIDADG family of homing endonucleases, all of which have a conserved LAGLIDADG amino acid motif that contributes to their associative domains and active sites. When the I-CreI-containing intron encounters a 23S gene lacking the intron, I-CreI enzyme ""homes"" in on the ""intron-minus"" allele of 23S and effects its parent intron's insertion into the intron-minus allele. Introns with this behavior are called mobile introns. Because I-CreI provides for its own propagation while conferring no benefit on its host, it is an example of selfish DNA.",1
Life Science,ILCD,"iLCD (Lighting Cell Display) is a device developed by a research team from Universidad Politecnica de Valencia, a MIT educated bioengineer, undergraduate students of the Universidad Politéctica de Valencia and Universitat de València and several members of the faculty and research staff from Universidad de València (Manuel Porcar), UPV (Pedro De Cordoba) and University of Malaga (Emilio Navarro).
It is based on yeast cells expressing aequorin protein sensitive to change in intracellular calcium. Upon electrical stimulation, the transient calcium wave emerges inside the yeast cells and translates into a measurable light signal. Assembly of multiple electrodes over lawn of yeast cells yields
Thanks to electronic control and sub-second timescale it is one of the first examples of bioelectronic devices capable of bi-directional communication between a computer and a living system. It is also one of the first examples of design of simple synthetic biology circuits operating on orders of magnitude faster timescale than those based on gene expression. Fast response to a stimulus is essential in variety of applications such as biosensing, medical technology, or as stated before - bioelectronics.
The project has been awarded a third place in 2009 iGEM competition",1
Life Science,Immune Therapy Holdings,"Immune Therapy Holdings AB or ITH is a Swedish biotechnology R&D holding company headquartered at the Karolinska Institutet and Karolinska University Hospital in Stockholm.
ITH's research is primarily focused on its Tailored Leukapheresis  (TLA) treatment for immune mediated inflammatory diseases (IMIDs).

",1
Life Science,Immunoscreening,"Immunoscreening is a method of biotechnology to detect a polypeptide produced from a cloned gene. The term encompasses several different techniques designed for protein identification, for example Western blotting.
Clones are screened for the presence of the gene product (a protein).
This strategy requires first that a gene library is implemented in an expression vector, and that antiserum to the protein is available.
Radioactivity or an enzyme is coupled generally with the secondary
antibody.
The radioactivity/enzyme linked secondary antibody can be purchased commercially and can detect different antigens.
In commercial diagnostics labs, labelled primary antibodies are
also used. The antigen-antibody interaction is used in the immunoscreening of several diseases.",1
Life Science,Impalefection,"Impalefection is a method of gene delivery using nanomaterials, such as carbon nanofibers, carbon nanotubes, nanowires. Needle-like nanostructures are synthesized perpendicular to the surface of a substrate. Plasmid DNA containing the gene and intended for intracellular delivery, is attached to the nanostructure surface. A chip with arrays of these needles is then pressed against cells or tissue. Cells that are impaled by nanostructures can express the delivered gene(s).
As one of the types of transfection, the term is derived from two words – impalement and infection.",1
Life Science,In situ bioremediation,"Bioremediation is the process of decontaminating polluted sites through the usage of either endogenous or external microorganism. In situ is a term utilized within a variety of fields meaning ""on site"" and refers to the location of an event. Within the context of bioremediation, in situ indicates that the location of the bioremediation has occurred at the site of contamination without the translocation of the polluted materials.  Bioremediation is used to neutralize pollutants including Hydrocarbons, chlorinated compounds, nitrates, toxic metals and other pollutants through a variety of chemical mechanisms. Microorganism used in the process of bioremediation can either be implanted or cultivated within the site through the application of fertilizers and other nutrients. Common polluted sites targeted by bioremediation are groundwater/aquifers and polluted soils. Aquatic ecosystems  affected by oil spills have also shown improvement through the application of bioremediation. The most notable cases being the Deepwater Horizon oil spill in 2010 and the Exxon Valdez oil spill in 1989. Two variations of bioremediation exist defined by the location where the process occurs. Ex situ bioremediation occurs at a location separate from the contaminated site and involves the translocation of the contaminated material. In situ occurs within the site of contamination In situ bioremediation can further be categorized by the metabolism occurring, aerobic and anaerobic, and by the level of human involvement.

",1
Life Science,In vitro compartmentalization,"In vitro compartmentalization (IVC) is an emulsion-based technology that generates cell-like compartments in vitro. These compartments are designed such that each contains no more than one gene.  When the gene is transcribed and/or translated, its products (RNAs and/or proteins) become 'trapped' with the encoding gene inside the compartment. By coupling the genotype (DNA) and phenotype (RNA, protein), compartmentalization allows the selection and evolution of phenotype.",1
Life Science,Inclusion bodies,"Inclusion bodies are aggregates of specific types of protein found in neurons, a number of tissue cells including red blood cells, bacteria, viruses, and plants. Inclusion bodies of aggregations of multiple proteins are also found in muscle cells affected by inclusion body myositis and hereditary inclusion body myopathy.Inclusion bodies in neurons may be accumulated in the cytoplasm or nucleus, and are associated with many neurodegenerative diseases. 
Inclusion bodies in neurodegenerative diseases are aggregates of misfolded proteins (aggresomes) and are hallmarks of many of these diseases, including Lewy bodies in Lewy body dementias, and Parkinson's disease, neuroserpin inclusion bodies called Collins bodies in familial encephalopathy with neuroserpin inclusion bodies, inclusion bodies in Huntington's disease, Papp-Lantos inclusions in multiple system atrophy, and various inclusion bodies in frontotemporal dementia including Pick bodies. Bunina bodies in motor neurons are a core feature of amyotrophic lateral sclerosis.Other usual cell inclusions are often temporary inclusions of accumulated proteins, fats, secretory granules or other insoluble components.Inclusion bodies are found in bacteria as particles of aggregated protein. They have a higher density than many other cell components but are porous. They typically represent sites of viral multiplication in a bacterium or a eukaryotic cell and usually consist of viral capsid proteins.
Inclusion bodies contain very little host protein, ribosomal components or DNA/RNA fragments. They often almost exclusively contain the over-expressed protein and aggregation and has been reported to be reversible. It has been suggested that inclusion bodies are dynamic structures formed by an unbalanced equilibrium between aggregated and soluble proteins of Escherichia coli. There is a growing body of information indicating that formation of inclusion bodies occurs as a result of intracellular accumulation of partially folded expressed proteins which aggregate through non-covalent hydrophobic or ionic interactions or a combination of both.",1
Life Science,Individualized cancer immunotherapy,"Individualized cancer immunotherapy, also referred to as individualized immuno-oncology, is a novel concept for therapeutic cancer vaccines that are truly personalized to a single individual.
The human immune system is generally able to recognize and fight cancer cells. However, this ability is usually insufficient and the cancer continues to spread. Cancer immunotherapy is based on harnessing and potentiating the ability of the immune system to fight cancer.
Each tumor has its own individual genetic fingerprint, the mutanome,  that includes numerous genetic alterations. As opposed to a preformed drug, individualized cancer vaccination is a therapy that targets specific cancer mutations of the individual patient's tumor. The production of vaccines tailored to match a person's individual constellation of cancer mutations has become a new field of research.The concept of individualized cancer immunotherapy aims to identify individual mutations in the tumor of a patient, that are crucial for the proliferation, survival or metastasis of tumor cells. For this purpose, the individual genetic blueprint of the tumor is decrypted by sequencing and, using this blueprint as a template, a synthetic vaccine tailored to the tumor of the individual patient is prepared. This vaccine is designed to control and train the body's immune system to fight the cancer.",1
Life Science,Induced stem cells,"Induced pluripotent stem cells (also known as iPS cells or iPSCs) are a type of pluripotent stem cell that can be generated directly from a somatic cell. The iPSC technology was pioneered by Shinya Yamanaka’s lab in Kyoto, Japan, who showed in 2006 that the introduction of four specific genes (named Myc, Oct3/4, Sox2 and Klf4), collectively known as Yamanaka factors, encoding transcription factors could convert somatic cells into pluripotent stem cells. He was awarded the 2012 Nobel Prize along with Sir John Gurdon ""for the discovery that mature cells can be reprogrammed to become pluripotent.""Pluripotent stem cells hold promise in the field of regenerative medicine. Because they can propagate indefinitely, as well as give rise to every other cell type in the body (such as neurons, heart, pancreatic, and liver cells), they represent a single source of cells that could be used to replace those lost to damage or disease.
The most well-known type of pluripotent stem cell is the embryonic stem cell. However, since the generation of embryonic stem cells involves destruction (or at least manipulation) of the pre-implantation stage embryo, there has been much controversy surrounding their use. Patient-matched embryonic stem cell lines can now be derived using somatic cell nuclear transfer (SCNT).
Since iPSCs can be derived directly from adult tissues, they not only bypass the need for embryos, but can be made in a patient-matched manner, which means that each individual could have their own pluripotent stem cell line. These unlimited supplies of autologous cells could be used to generate transplants without the risk of immune rejection. While the iPSC technology has not yet advanced to a stage where therapeutic transplants have been deemed safe, iPSCs are readily being used in personalized drug discovery efforts and understanding the patient-specific basis of disease.Yamanaka named iPSCs with a lower case ""i"" due to the popularity of the iPod and other products.In his Nobel seminar, Yamanaka cited the earlier seminal work of Harold Weintraub on the role of MyoD in reprogramming cell fate to a muscle lineage as an important precursor to the discovery of iPSCs.",1
Life Science,Induced-charge electrokinetics,"Induced-charge electrokinetics in physics is the electrically driven fluid flow and particle motion in a liquid electrolyte. Consider a metal particle (which is neutrally charged but electrically conducting) in contact with an aqueous solution in a chamber/channel. If different voltages apply to the end of this chamber/channel, electric field will generate in this chamber/channel. This applied electric field passes through this metal particle and causes the free charges inside the particle migrate under the skin of particle. As a result of this migration, the negative charges moves to the side which is close to the positive (or higher) voltage while the positive charges moves to the opposite side of the particle. These charges under the skin of conducting particle attract the counter-ions of the aqueous solution; thus, the electric double layer (EDL) forms around the particle. The EDL sign on the surface of the conducting particle changes from positive to negative and the distribution of the charges varies along the particle geometry. Due to these variations, the EDL is non-uniform and has different signs. Thus, the induced zeta potential around the particle, and consequently slip velocity on the surface of the particle, vary as a function of local electric field. Differences in magnitude and direction of slip velocity on the surface of the conducting particle effects the flow pattern around this particle and causes micro vortices. Yasaman Daghighi and Dongqing Li, for the first time, experimentally illustrated these induced vortices around a 1.2mm diameter carbon-steel sphere under the 40V/cm direct current (DC) external electric filed.
Chenhui Peng et al. also experimentally showed the patterns of electro-osmotic flow around an Au sphere when alternating current (AC) is involved (E=10mV/μm, f=1 kHz).
Electrokinetics here refers to a branch of science related to the motion and reaction of charged particles to the applied electric filed and its effects on its environment. It is sometimes referred as non-linear electrokinetic phenomena as well.

",1
Life Science,Industrial fermentation,"Industrial fermentation is the intentional use of fermentation by microorganisms such as bacteria and fungi as well as eukaryotic cells like CHO cells and insect cells, to make products useful to humans. Fermented products have applications as food as well as in general industry. Some commodity chemicals, such as acetic acid, citric acid, and ethanol are made by fermentation. The rate of fermentation depends on the concentration of microorganisms, cells, cellular components, and enzymes as well as temperature, pH and level of oxygen for aerobic fermentation. Product recovery frequently involves the concentration of the dilute solution. Nearly all commercially produced enzymes, such as lipase, invertase and rennet, are made by fermentation with genetically modified microbes. In some cases, production of biomass itself is the objective, as is the case for single-cell proteins, baker's yeast and starter cultures for lactic acid bacteria used in cheesemaking. In general, fermentations can be divided into four types:
Production of biomass (viable cellular material)
Production of extracellular metabolites (chemical compounds)
Production of intracellular components (enzymes and other proteins)
Transformation of substrate (in which the transformed substrate is itself the product)These types are not necessarily disjoint from each other, but provide a framework for understanding the differences in approach. The organisms used may be bacteria, yeasts, molds, algae, animal cells, or plant cells. Special considerations are required for the specific organisms used in the fermentation, such as the dissolved oxygen level, nutrient levels, and temperature.

",1
Life Science,Integrated fluidic circuit,Integrated fluidic circuit (IFC) is a type of integrated circuit (IC) using fluids.,1
Life Science,Intestine-on-a-chip,"Intestines-on-a-chip (gut-on-a-chip, mini-intestine) are microfluidic bioengineered 3D-models of the real organ, which better mimic physiological features than conventional 3D intestinal organoid culture... A variety of different intestine-on-a-chip models systems have been developed and refined, all holding their individual strengths and weaknesses and collectively holding great promise to the ultimate goal of establishing these systems as reliable high-throughput platforms for drug testing and personalised medicine. The intestine is a highly complex organ system performing a diverse set of vital tasks, from nutrient digestion and absorption, hormone secretion, and immunological processes to neuronal activity, which makes it particularly challenging to model in vitro.",1
Life Science,Intron-encoded endonuclease I-SceI,Intron-encoded endonuclease I-Sce I is a homing endonuclease. The enzyme is used in biotechnology as a meganuclease. It recognises an 18-base pair sequence TAGGGATAACAGGGTAAT and leaves a 4 base pair 3' hydroxyl overhang. It is a rare cutting endonuclease. Statistically an 18-bp sequence will occur once in every 6.9*1010 base pairs (a frequency of 1 in 418). This sequence does not normally occur in a human or mouse genome.,1
Life Science,Ion semiconductor sequencing,"Ion semiconductor sequencing is a method of DNA sequencing based on the detection of hydrogen ions that are released during the polymerization of DNA. This is a method of ""sequencing by synthesis"", during which a complementary strand is built based on the sequence of a template strand.

A microwell containing a template DNA strand to be sequenced is flooded with a single species of deoxyribonucleotide triphosphate (dNTP). If the introduced dNTP is complementary to the leading template nucleotide, it is incorporated into the growing complementary strand. This causes the release of a hydrogen ion that triggers an ISFET ion sensor, which indicates that a reaction has occurred. If homopolymer repeats are present in the template sequence, multiple dNTP molecules will be incorporated in a single cycle. This leads to a corresponding number of released hydrogens and a proportionally higher electronic signal.
This technology differs from other sequencing-by-synthesis technologies in that no modified nucleotides or optics are used. Ion semiconductor sequencing may also be referred to as Ion Torrent sequencing, pH-mediated sequencing, silicon sequencing, or semiconductor sequencing.

",1
Life Science,IRIS (biosensor),"Interferometric reflectance imaging sensor (IRIS), formerly known as the spectral reflectance imaging biosensor (SRIB), is a system that can be used as a biosensing platform capable of high-throughput multiplexing of protein–protein, protein–DNA, and DNA–DNA  interactions without the use of any fluorescent labels. The sensing surface is prepared by robotic spotting of biological probes that are immobilized on functionalized Si/SiO2 substrates. IRIS is capable of quantifying biomolecular mass accumulated on the surface.",1
Life Science,Isoschizomer,"Isoschizomers are pairs of restriction enzymes specific to the same recognition sequence. For example, SphI (CGTAC/G) and BbuI (CGTAC/G) are isoschizomers of each other. The first enzyme discovered which recognizes a given sequence is known as the prototype; all subsequently identified enzymes that recognize that sequence are isoschizomers. Isoschizomers are isolated from different strains of bacteria and therefore may require different reaction conditions.
In some cases, only one out of a pair of isoschizomers can recognize both the methylated as well as unmethylated forms of restriction sites. In contrast, the other restriction enzyme can recognize only the unmethylated form of the restriction site.
This property of some isoschizomers allows identification of methylation state of the restriction site while isolating it from a bacterial strain.
For example, the restriction enzymes HpaII and MspI are isoschizomers, as they both recognize the sequence 5'-CCGG-3' when it is unmethylated. But when the second C of the sequence is methylated, only MspI can recognize it while HpaII cannot.
An enzyme that recognizes the same sequence but cuts it differently is a neoschizomer.  Neoschizomers are a specific type (subset) of isoschizomer. For example, SmaI (CCC/GGG) and XmaI (C/CCGGG) are neoschizomers of each other. Similarly Kpn1 (GGTAC/C) and Acc651 (G/GTACC) are neoschizomers of each other. 
An enzyme that recognizes a slightly different sequence, but produces the same ends is an isocaudomer.",1
Life Science,Biotechnology industry in Italy,"Biotechnology industry in Italy is a highly innovative and fast-growing sector dedicated to research.
At the end of 2019, there are 696 biotech companies active in Italy.",1
Life Science,Jeevamrutha,"Jeevamrutha is a natural liquid fertilizer. It is made by mixing water, dung (in the form of manure) and urine from cows with some mud from the same area as the manure will be applied in later. Food is then added to speed the growth of microbes: jaggery or flour can be used.
Jeevamrutha has been used by Indian farmers for centuries, falling out of use for some time before being revived in the 2000s.",1
Life Science,Knockout moss,"A knockout moss is one kind of genetically modified moss, which are GM plants. One or more of the moss's specific genes are deleted or inactivated (""knocked out""), for example by gene targeting or other methods. After the deletion of a gene, the knockout moss has lost the trait encoded by this gene. Thus, the function of this gene can be inferred. This scientific approach is called reverse genetics as the scientist wants to unravel the function of a specific gene. In classical genetics the scientist starts with a phenotype of interest and searches for the gene that causes this phenotype. Knockout mosses are relevant for basic research in biology as well as in biotechnology.",1
Life Science,Kodecyte,"A kodecyte (ko•de•cyte) is a living cell that has been modified (koded) by the incorporation of one or more function-spacer-lipid constructs (FSL constructs) to gain a new or novel biological, chemical or technological function. The cell is modified by the lipid tail of the FSL construct incorporating into the bilipid membrane of the cell.
All kodecytes retain their normal vitality and functionality while gaining the new function of the inserted FSL constructs. The combination of dispersibility in biocompatible media, spontaneous incorporation into cell membranes, and apparent low toxicity, makes FSL constructs suitable as research tools and for the development of new diagnostic and therapeutic applications.",1
Life Science,Kodevirion,"The term kodecyte is used to describe cells with detectable Function-Spacer-Lipid (FSL) constructs, and in concert, the term kodevirion (pronounced co-da-virion), is used to describe virions with detectable FSL constructs.The method for labeling virions with FSL constructs is simple, non covalent and only involves incubation of the virion with the FSL construct in saline for a few hours – nothing further is required. The FSL construct will spontaneously, stably and quantitatively incorporate into the virion membrane. Virions have been labelled with fluorescent (FSL-FLRO4) and radioactive iodine  (FSL-125I). FSL-FLRO4 could be shown to label virions in a dose dependent manner and could be visualized by flow cytometry either directly, or indirectly if the virion had bound to the cell or fused with the cell membrane. FSLs do not appear to significantly affect the virions infectivity or their ability to bind target cells, probably because they integrate into the membrane without exposing the virion to chemical agents or covalent modification.

",1
Life Science,L-Ribonucleic acid aptamer,"An L-ribonucleic acid aptamer (L-RNA aptamer, trade name Spiegelmer) is an RNA-like molecule built from L-ribose units. It is an artificial oligonucleotide named for being a mirror image of natural oligonucleotides. L-RNA aptamers are a form of aptamers. Due to their L-nucleotides, they are highly resistant to degradation by nucleases. L-RNA aptamers are considered potential drugs and are currently being tested in clinical trials.",1
Life Science,Lateral flow test,"A lateral flow test (LFT), is an Assay also known as a lateral flow device (LFD), lateral flow immunochromatographic assay, or rapid test, is a simple device intended to detect the presence of a target substance in a liquid sample without the need for specialized and costly equipment.  LFTs are widely used in medical diagnostics in the home, at the point of care, and in the laboratory. For instance, the home pregnancy test is an LFT that detects a specific hormone. These tests are simple and economical and generally show results in around five to thirty minutes. Many lab-based applications increase the sensitivity of simple LFTs by employing additional dedicated equipment. Because the target substance is often a biological antigen, many lateral flow tests are rapid antigen tests (RAT or ART). 
LFTs operate on the same principles of affinity chromatography as the enzyme-linked immunosorbent assays (ELISA). In essence, these tests run the liquid sample along the surface of a pad with reactive molecules that show a visual positive or negative result.   The pads are based on a series of capillary beds, such as pieces of porous paper, microstructured polymer, or sintered polymer. Each of these pads has the capacity to transport fluid (e.g., urine, blood, saliva) spontaneously.The sample pad acts as a sponge and holds an excess of sample fluid. Once soaked, the fluid flows to the second conjugate pad in which the manufacturer has stored freeze dried bio-active particles called conjugates (see below) in a salt–sugar matrix. The conjugate pad contains all the reagents required for an optimized chemical reaction between the target molecule (e.g., an antigen) and its chemical partner (e.g., antibody) that has been immobilized on the particle's surface. This marks target particles as they pass through the pad and continue across to the test and control lines. The test line shows a signal, often a color as in pregnancy tests. The control line contains affinity ligands which show whether the sample has flowed through and the bio-molecules in the conjugate pad are active. After passing these reaction zones, the fluid enters the final porous material, the wick, that simply acts as a waste container.
LFTs can operate as either competitive or sandwich assays.",1
Life Science,LEAPER gene editing,"LEAPER (Leveraging endogenous ADAR for programmable editing of RNA) is a genetic engineering technique in molecular biology by which RNA can be edited. The technique relies on engineered strands of RNA to recruit native ADAR enzymes to swap out different compounds in RNA. Developed by researchers at Peking University in 2019, the technique, some have claimed, is more efficient than the CRISPR gene editing technique. Initial studies have claimed that editing efiiciencies of up to 80% can be achieved.",1
Life Science,List of life sciences,"This list of life sciences comprises the branches of science that involve the scientific study of life – such as microorganisms, plants, and animals including human beings. This science is one of the two major branches of natural science, the other being physical science, which is concerned with non-living matter. Biology is the overall natural science that studies life, with the other life sciences as its sub-disciplines. 
Some life sciences focus on a specific type of organism. For example, zoology is the study of animals, while botany is the study of plants. Other life sciences focus on aspects common to all or many life forms, such as anatomy and genetics. Some focus on the micro-scale (e.g. molecular biology, biochemistry) other on larger scales (e.g. cytology, immunology, ethology, pharmacy, ecology). Another major branch of life sciences involves understanding the mind – neuroscience. Life sciences discoveries are helpful in improving the quality and standard of life and have applications in health, agriculture, medicine, and the pharmaceutical and food science industries.",1
Life Science,Lipid bilayer mechanics,"Lipid bilayer mechanics is the study of the physical material properties of lipid bilayers, classifying bilayer behavior with stress and strain rather than biochemical interactions. Local point deformations such as membrane protein interactions are typically modelled with the complex theory of biological liquid crystals but the mechanical properties of a homogeneous bilayer are often characterized in terms of only three mechanical elastic moduli: the area expansion modulus Ka, a bending modulus Kb and an edge energy 
  
    
      
        Λ
      
    
    {\displaystyle \Lambda }
  . For fluid bilayers the shear modulus is by definition zero, as the free rearrangement of molecules within plane means that the structure will not support shear stresses. These mechanical properties affect several membrane-mediated biological processes. In particular, the values of Ka and Kb affect the ability of proteins and small molecules to insert into the bilayer. Bilayer mechanical properties have also been shown to alter the function of mechanically activated ion channels.",1
Life Science,Lipidomics,"Lipidomics is the large-scale study of pathways and networks of cellular lipids in biological systems The word ""lipidome"" is used to describe the complete lipid profile within a cell, tissue, organism, or ecosystem and is a subset of the ""metabolome"" which also includes the three other major classes of biological molecules: proteins/amino-acids, sugars and nucleic acids. Lipidomics is a relatively recent research field that has been driven by rapid advances in technologies such as mass spectrometry (MS), nuclear magnetic resonance (NMR) spectroscopy, fluorescence spectroscopy, dual polarisation interferometry and computational methods, coupled with the recognition of the role of lipids in many metabolic diseases such as obesity, atherosclerosis, stroke, hypertension and diabetes. This rapidly expanding field complements the huge progress made in genomics and proteomics, all of which constitute the family of systems biology.
Lipidomics research involves the identification and quantification of the thousands of cellular lipid molecular species and their interactions with other lipids, proteins, and other metabolites. Investigators in lipidomics examine the structures, functions, interactions, and dynamics of cellular lipids and the changes that occur during perturbation of the system.
Han and Gross first defined the field of lipidomics through integrating the specific chemical properties inherent in lipid molecular species with a comprehensive mass spectrometric approach. Although lipidomics is under the umbrella of the more general field of ""metabolomics"", lipidomics is itself a distinct discipline due to the uniqueness and functional specificity of lipids relative to other metabolites.
In lipidomic research, a vast amount of information quantitatively describing the spatial and temporal alterations in the content and composition of different lipid molecular species is accrued after perturbation of a cell through changes in its physiological or pathological state. Information obtained from these studies facilitates mechanistic insights into changes in cellular function. Therefore, lipidomic studies play an essential role in defining the biochemical mechanisms of lipid-related disease processes through identifying alterations in cellular lipid metabolism, trafficking and homeostasis. The growing attention on lipid research is also seen from the initiatives underway of the LIPID Metabolites And Pathways Strategy (LIPID MAPS Consortium). and The European Lipidomics Initiative (ELIfe).

",1
Life Science,List of gene therapies,"Gene therapy is a medical field which focuses on the genetic modification of cells to produce a therapeutic effect or the treatment of disease by repairing or reconstructing defective genetic material. The first attempt at modifying human DNA was performed in 1980, by Martin Cline, but the first successful nuclear gene transfer in humans, approved by the National Institutes of Health, was performed in May 1989. The first therapeutic use of gene transfer as well as the first direct insertion of human DNA into the nuclear genome was performed by French Anderson in a trial starting in September 1990. It is thought to be able to cure many genetic disorders or treat them over time.
Between 1989 and December 2018, over 2,900 clinical trials were conducted, with more than half of them in phase I. As of 2017, Spark Therapeutics' Luxturna (RPE65 mutation-induced blindness) and Novartis' Kymriah (Chimeric antigen receptor T cell therapy) are the FDA's first approved gene therapies to enter the market. Since that time, drugs such as Novartis' Zolgensma and Alnylam's Patisiran have also received FDA approval, in addition to other companies' gene therapy drugs. Most of these approaches utilize adeno-associated viruses (AAVs) and lentiviruses for performing gene insertions, in vivo and ex vivo, respectively. AAVs are characterized by stabilizing the viral capsid, lower immunogenicity, ability to transduce both dividing and nondividing cells, the potential to integrate site specifically and to achieve long-term expression in the in-vivo treatment. (Gorell et al. 2014) ASO / siRNA approaches such as those conducted by Alnylam and Ionis Pharmaceuticals require non-viral delivery systems, and utilize alternative mechanisms for trafficking to liver cells by way of GalNAc transporters.
The concept of gene therapy is to fix a genetic problem at its source. If, for instance, a mutation in a certain gene causes the production of a dysfunctional protein resulting (usually recessively) in an inherited disease, gene therapy could be used to deliver a copy of this gene that does not contain the deleterious mutation and thereby produces a functional protein. This strategy is referred to as gene replacement therapy and is employed to treat inherited retinal diseases.While the concept of gene replacement therapy is mostly suitable for recessive diseases, novel strategies have been suggested that are capable of also treating conditions with a dominant pattern of inheritance.

The introduction of CRISPR gene editing has opened new doors for its application and utilization in gene therapy, as instead of pure replacement of a gene, it enables correction of the particular genetic defect. Solutions to medical hurdles, such as the eradication of latent human immunodeficiency virus (HIV) reservoirs and correction of the mutation that causes sickle cell disease, may be available as a therapeutic option in the future.
Prosthetic gene therapy aims to enable cells of the body to take over functions they physiologically do not carry out. One example is the so-called vision restoration gene therapy, that aims to restore vision in patients with end-stage retinal diseases. In end-stage retinal diseases, the photoreceptors, as the primary light sensitive cells of the retina are irreversibly lost. By the means of prosthetic gene therapy light sensitive proteins are delivered into the remaining cells of the retina, to render them light sensitive and thereby enable them to signal visual information towards the brain. Clinical trials are ongoing. (NCT02556736, NCT03326336 at clinicaltrials.gov)Not all medical procedures that introduce alterations to a patient's genetic makeup can be considered gene therapy. Bone marrow transplantation and organ transplants in general have been found to introduce foreign DNA into patients.",1
Life Science,List of genetically modified crops,"Genetically modified crops are plants used in agriculture, the DNA of which has been modified using genetic engineering techniques.  In most cases, the aim is to introduce a new trait to the plant which does not occur naturally in the species. As of 2015, 26 plant species have been genetically modified and approved for commercial release in at least one country. The majority of these species contain genes that make them either tolerant to herbicides or resistant to insects. Other common traits include virus resistance, delayed ripening, modified flower colour or altered composition. In 2014, 28 countries grew GM crops, and 39 countries imported but did not grow them.

",1
Life Science,List of homing endonuclease cutting sites,"The homing endonucleases are a special type of restriction enzymes encoded by introns or inteins. They act on the cellular DNA of the cell that synthesizes them; to be precise, in the opposite allele of the gene that encode them.",1
Life Science,List of recombinant proteins,"The following is a list of notable proteins that are produced from recombinant DNA, using biomolecular engineering. In many cases, recombinant human proteins have replaced the original animal-derived version used in medicine. The prefix ""rh"" for ""recombinant human"" appears less and less in the literature. A much larger number of recombinant proteins is used in the research laboratory. These include both commercially available proteins (for example most of the enzymes used in the molecular biology laboratory), and those that are generated in the course specific research projects.

",1
Life Science,List of restriction enzyme cutting sites,"A restriction enzyme or restriction endonuclease is a special type of biological macromolecule that functions as part of the ""immune system"" in bacteria. One special kind of restriction enzymes is the class of ""homing endonucleases"", these being present in all three domains of life, although their function seems to be very different from one domain to another.
The classical restriction enzymes cut up, and hence render harmless, any unknown (non-cellular) DNA that enters a bacterial cell as a result of a viral infection. They recognize a specific DNA sequence, usually short (3 to 8 bp), and cut it, producing either blunt or overhung ends, either at or nearby the recognition site.
Restriction enzymes are quite variable in the short DNA sequences they recognize. An organism often has several different enzymes, each specific to a distinct short DNA sequence.
See the main article on restriction enzyme.
Further reading: Homing endonuclease.

",1
Life Science,List of restriction enzyme cutting sites: A,"This article contains a list of restriction enzymes whose names start with A and have a clearly defined cutting site.
The following information is given for each enzyme:

Name of Restriction Enzyme: Accepted name of the molecule, according to the internationally adopted nomenclature, and bibliographical references. Note: When alphabetizing, enzymes are first ordered alphabetically by the acronyms (everything before the roman numeral); then enzymes of a given acronym are ordered alphabetically by the roman numeral, treating the numeral as a number and not a string of letters. This helps keep the entries ordered hierarchically while also alphabetic.(Further reading: see the section ""Nomenclature"" in the article ""Restriction enzyme"".)
PDB code: Code used to identify the structure of a protein in the PDB database of protein structures. The 3D atomic structure of a protein provides highly valuable information to understand the intimate details of its mechanism of action.
REBASE Number: Number used to identify restriction enzymes in the REBASE restriction enzyme database. This database includes important information about the enzyme such as Recognition sequence, source, and Isoschizomers, as well as other data, such as the commercial suppliers of the enzyme.
Source: Organism that naturally produces the enzyme.
Recognition sequence: Sequence of DNA recognized by the enzyme and to which it specifically binds.
Cut: Displays the cut site and pattern and products of the cut. The recognition sequence and the cut site usually match, but sometimes the cut site can be dozens of nucleotides away from the recognition site.
Isoschizomers and neoschizomers: An isoschizomer is a restriction enzyme that recognizes the same sequence as another. A neoschizomer is a special type of isoschizomer that recognizes the same sequence as another, but cuts in a different manner. A maximum number of 8–10 most common isoschizomers are indicated for every enzyme but there may be many more. Neoschizomers are shown in bold and green color font (e.g.: BamHI). When ""None as of [date]"" is indicated, that means that there were no registered isoschizomers in the databases on that date with a clearly defined cutting site. Isoschizomers indicated in white font and grey background correspond to enzymes not listed in the current lists, as in this not listed enzyme:   Abc123I",1
Life Science,List of restriction enzyme cutting sites: Ba–Bc,"This article contains a list of the most studied restriction enzymes whose names start with Ba to Bc inclusive.  It contains approximately 120 enzymes.
The following information is given:

Enzyme: Accepted name of the molecule, according to the internationally adopted nomenclature, and bibliographical references. (Further reading: see the section ""Nomenclature"" in the article ""Restriction enzyme"".)
PDB code: Code used to identify the structure of a protein in the PDB database of protein structures. The 3D atomic structure of a protein provides highly valuable information to understand the intimate details of its mechanism of action.
Source: Organism that naturally produces the enzyme.
Recognition sequence: Sequence of DNA recognized by the enzyme and to which it specifically binds.
Cut: Cutting site and DNA products of the cut. The recognition sequence and the cutting site usually match, but sometimes the cutting site can be dozens of nucleotides away from the recognition site.
Isoschizomers and neoschizomers: An isoschizomer is an enzyme that recognizes the same sequence as another. A neoschizomer is a special type of isoschizomer that recognizes the same sequence as another, but cuts in a different manner. A maximum number of 8-10 most common isoschizomers are indicated for every enzyme but there may be many more. Neoschizomers are shown in bold and green color font (e.g.: BamHI). When ""None on date"" is indicated, that means that there were no registered isoschizomers in the databases on that date with a clearly defined cutting site. Isoschizomers indicated in white font and grey background correspond to enzymes not listed in the current lists:",1
Life Science,List of restriction enzyme cutting sites: Bd–Bp,"A restriction enzyme or restriction endonuclease is a special type of biological macromolecule that functions as part of the ""immune system"" in bacteria. One special kind of restriction enzymes is the class of ""homing endonucleases"", these being present in all three domains of life, although their function seems to be very different from one domain to another.
The classical restriction enzymes cut up, and hence render harmless, any unknown (non-cellular) DNA that enters a bacterial cell as a result of a viral infection. They recognize a specific DNA sequence, usually short (3 to 8 bp), and cut it, producing either blunt or overhung ends, either at or nearby the recognition site.
Restriction enzymes are quite variable in the short DNA sequences they recognize. An organism often has several different enzymes, each specific to a distinct short DNA sequence.
See the main article on restriction enzyme.
Further reading: Homing endonuclease.

",1
Life Science,List of restriction enzyme cutting sites: Bsa–Bso,"A restriction enzyme or restriction endonuclease is a special type of biological macromolecule that functions as part of the ""immune system"" in bacteria. One special kind of restriction enzymes is the class of ""homing endonucleases"", these being present in all three domains of life, although their function seems to be very different from one domain to another.
The classical restriction enzymes cut up, and hence render harmless, any unknown (non-cellular) DNA that enters a bacterial cell as a result of a viral infection. They recognize a specific DNA sequence, usually short (3 to 8 bp), and cut it, producing either blunt or overhung ends, either at or nearby the recognition site.
Restriction enzymes are quite variable in the short DNA sequences they recognize. An organism often has several different enzymes, each specific to a distinct short DNA sequence.
See the main article on restriction enzyme.
Further reading: Homing endonuclease.

",1
Life Science,List of restriction enzyme cutting sites: Bsp–Bss,"This article contains a list of restriction enzymes whose names start with A and have a clearly defined cutting site.
The following information is given for each enzyme:

Name of Restriction Enzyme: Accepted name of the molecule, according to the internationally adopted nomenclature, and bibliographical references. Note: When alphabetizing, enzymes are first ordered alphabetically by the acronyms (everything before the roman numeral); then enzymes of a given acronym are ordered alphabetically by the roman numeral, treating the numeral as a number and not a string of letters. This helps keep the entries ordered hierarchically while also alphabetic.(Further reading: see the section ""Nomenclature"" in the article ""Restriction enzyme"".)
PDB code: Code used to identify the structure of a protein in the PDB database of protein structures. The 3D atomic structure of a protein provides highly valuable information to understand the intimate details of its mechanism of action.
REBASE Number: Number used to identify restriction enzymes in the REBASE restriction enzyme database. This database includes important information about the enzyme such as Recognition sequence, source, and Isoschizomers, as well as other data, such as the commercial suppliers of the enzyme.
Source: Organism that naturally produces the enzyme.
Recognition sequence: Sequence of DNA recognized by the enzyme and to which it specifically binds.
Cut: Displays the cut site and pattern and products of the cut. The recognition sequence and the cut site usually match, but sometimes the cut site can be dozens of nucleotides away from the recognition site.
Isoschizomers and neoschizomers: An isoschizomer is a restriction enzyme that recognizes the same sequence as another. A neoschizomer is a special type of isoschizomer that recognizes the same sequence as another, but cuts in a different manner. A maximum number of 8–10 most common isoschizomers are indicated for every enzyme but there may be many more. Neoschizomers are shown in bold and green color font (e.g.: BamHI). When ""None as of [date]"" is indicated, that means that there were no registered isoschizomers in the databases on that date with a clearly defined cutting site. Isoschizomers indicated in white font and grey background correspond to enzymes not listed in the current lists, as in this not listed enzyme:   Abc123I",1
Life Science,List of restriction enzyme cutting sites: Bst–Bv,"A restriction enzyme or restriction endonuclease is a special type of biological macromolecule that functions as part of the ""immune system"" in bacteria. One special kind of restriction enzymes is the class of ""homing endonucleases"", these being present in all three domains of life, although their function seems to be very different from one domain to another.
The classical restriction enzymes cut up, and hence render harmless, any unknown (non-cellular) DNA that enters a bacterial cell as a result of a viral infection. They recognize a specific DNA sequence, usually short (3 to 8 bp), and cut it, producing either blunt or overhung ends, either at or nearby the recognition site.
Restriction enzymes are quite variable in the short DNA sequences they recognize. An organism often has several different enzymes, each specific to a distinct short DNA sequence.
See the main article on restriction enzyme.
Further reading: Homing endonuclease.

",1
Life Science,List of restriction enzyme cutting sites: C–D,"This article contains a list of restriction enzymes whose names start with A and have a clearly defined cutting site.
The following information is given for each enzyme:

Name of Restriction Enzyme: Accepted name of the molecule, according to the internationally adopted nomenclature, and bibliographical references. Note: When alphabetizing, enzymes are first ordered alphabetically by the acronyms (everything before the roman numeral); then enzymes of a given acronym are ordered alphabetically by the roman numeral, treating the numeral as a number and not a string of letters. This helps keep the entries ordered hierarchically while also alphabetic.(Further reading: see the section ""Nomenclature"" in the article ""Restriction enzyme"".)
PDB code: Code used to identify the structure of a protein in the PDB database of protein structures. The 3D atomic structure of a protein provides highly valuable information to understand the intimate details of its mechanism of action.
REBASE Number: Number used to identify restriction enzymes in the REBASE restriction enzyme database. This database includes important information about the enzyme such as Recognition sequence, source, and Isoschizomers, as well as other data, such as the commercial suppliers of the enzyme.
Source: Organism that naturally produces the enzyme.
Recognition sequence: Sequence of DNA recognized by the enzyme and to which it specifically binds.
Cut: Displays the cut site and pattern and products of the cut. The recognition sequence and the cut site usually match, but sometimes the cut site can be dozens of nucleotides away from the recognition site.
Isoschizomers and neoschizomers: An isoschizomer is a restriction enzyme that recognizes the same sequence as another. A neoschizomer is a special type of isoschizomer that recognizes the same sequence as another, but cuts in a different manner. A maximum number of 8–10 most common isoschizomers are indicated for every enzyme but there may be many more. Neoschizomers are shown in bold and green color font (e.g.: BamHI). When ""None as of [date]"" is indicated, that means that there were no registered isoschizomers in the databases on that date with a clearly defined cutting site. Isoschizomers indicated in white font and grey background correspond to enzymes not listed in the current lists, as in this not listed enzyme:   Abc123I",1
Life Science,List of restriction enzyme cutting sites: E–F,"This article contains a list of restriction enzymes whose names start with A and have a clearly defined cutting site.
The following information is given for each enzyme:

Name of Restriction Enzyme: Accepted name of the molecule, according to the internationally adopted nomenclature, and bibliographical references. Note: When alphabetizing, enzymes are first ordered alphabetically by the acronyms (everything before the roman numeral); then enzymes of a given acronym are ordered alphabetically by the roman numeral, treating the numeral as a number and not a string of letters. This helps keep the entries ordered hierarchically while also alphabetic.(Further reading: see the section ""Nomenclature"" in the article ""Restriction enzyme"".)
PDB code: Code used to identify the structure of a protein in the PDB database of protein structures. The 3D atomic structure of a protein provides highly valuable information to understand the intimate details of its mechanism of action.
REBASE Number: Number used to identify restriction enzymes in the REBASE restriction enzyme database. This database includes important information about the enzyme such as Recognition sequence, source, and Isoschizomers, as well as other data, such as the commercial suppliers of the enzyme.
Source: Organism that naturally produces the enzyme.
Recognition sequence: Sequence of DNA recognized by the enzyme and to which it specifically binds.
Cut: Displays the cut site and pattern and products of the cut. The recognition sequence and the cut site usually match, but sometimes the cut site can be dozens of nucleotides away from the recognition site.
Isoschizomers and neoschizomers: An isoschizomer is a restriction enzyme that recognizes the same sequence as another. A neoschizomer is a special type of isoschizomer that recognizes the same sequence as another, but cuts in a different manner. A maximum number of 8–10 most common isoschizomers are indicated for every enzyme but there may be many more. Neoschizomers are shown in bold and green color font (e.g.: BamHI). When ""None as of [date]"" is indicated, that means that there were no registered isoschizomers in the databases on that date with a clearly defined cutting site. Isoschizomers indicated in white font and grey background correspond to enzymes not listed in the current lists, as in this not listed enzyme:   Abc123I",1
Life Science,List of restriction enzyme cutting sites: G–K,"This article contains a list of the most studied restriction enzymes whose names start with G to K inclusive.  It contains approximately 90 enzymes.
The following information is given:

Enzyme: Accepted name of the molecule, according to the internationally adopted nomenclature, and bibliographical references. (Further reading: see the section ""Nomenclature"" in the article ""Restriction enzyme"".)
PDB code: Code used to identify the structure of a protein in the PDB database of protein structures. The 3D atomic structure of a protein provides highly valuable information to understand the intimate details of its mechanism of action.
Source: Organism that naturally produces the enzyme.
Recognition sequence: Sequence of DNA recognized by the enzyme and to which it specifically binds.
Cut: Cutting site and DNA products of the cut. The recognition sequence and the cutting site usually match, but sometimes the cutting site can be dozens of nucleotides away from the recognition site.
Isoschizomers and neoschizomers: An isoschizomer is an enzyme that recognizes the same sequence as another. A neoschizomer is a special type of isoschizomer that recognizes the same sequence as another, but cuts in a different manner. A maximum number of 8-10 most common isoschizomers are indicated for every enzyme but there may be many more. Neoschizomers are shown in bold and green color font (e.g.: BamHI). When ""None on date"" is indicated, that means that there were no registered isoschizomers in the databases on that date with a clearly defined cutting site. Isoschizomers indicated in white font and grey background correspond to enzymes not listed in the current lists:",1
Life Science,List of restriction enzyme cutting sites: L–N,"This article contains a list of the most studied restriction enzymes whose names start with L to N inclusive. It contains approximately 120 enzymes.
The following information is given:

Enzyme: Accepted name of the molecule, according to the internationally adopted nomenclature, and bibliographical references. (Further reading: see the section ""Nomenclature"" in the article ""Restriction enzyme"".)
PDB code: Code used to identify the structure of a protein in the PDB database of protein structures. The 3D atomic structure of a protein provides highly valuable information to understand the intimate details of its mechanism of action.
Source: Organism that naturally produces the enzyme.
Recognition sequence: Sequence of DNA recognized by the enzyme and to which it specifically binds.
Cut: Cutting site and DNA products of the cut. The recognition sequence and the cutting site usually match, but sometimes the cutting site can be dozens of nucleotides away from the recognition site.
Isoschizomers and neoschizomers: An isoschizomer is an enzyme that recognizes the same sequence as another. A neoschizomer is a special type of isoschizomer that recognizes the same sequence as another, but cuts in a different manner. A maximum number of 8-10 most common isoschizomers are indicated for every enzyme but there may be many more. Neoschizomers are shown in bold and green color font (e.g.: BamHI). When ""None on date"" is indicated, that means that there were no registered isoschizomers in the databases on that date with a clearly defined cutting site. Isoschizomers indicated in white font and grey background correspond to enzymes not listed in the current lists:",1
Life Science,List of restriction enzyme cutting sites: O–R,"This article contains a list of the most studied restriction enzymes whose names start with O to R inclusive.  It contains approximately 130 enzymes.
The following information is given:

Enzyme: Accepted name of the molecule, according to the internationally adopted nomenclature, and bibliographical references. (Further reading: see the section ""Nomenclature"" in the article ""Restriction enzyme"".)
PDB code: Code used to identify the structure of a protein in the PDB database of protein structures. The 3D atomic structure of a protein provides highly valuable information to understand the intimate details of its mechanism of action.
Source: Organism that naturally produces the enzyme.
Recognition sequence: Sequence of DNA recognized by the enzyme and to which it specifically binds.
Cut: Cutting site and DNA products of the cut. The recognition sequence and the cutting site usually match, but sometimes the cutting site can be dozens of nucleotides away from the recognition site.
Isoschizomers and neoschizomers: An isoschizomer is an enzyme that recognizes the same sequence as another. A neoschizomer is a special type of isoschizomer that recognizes the same sequence as another, but cuts in a different manner. A maximum number of 8-10 most common isoschizomers are indicated for every enzyme but there may be many more. Neoschizomers are shown in bold and green color font (e.g.: BamHI). When ""None on date"" is indicated, that means that there were no registered isoschizomers in the databases on that date with a clearly defined cutting site. Isoschizomers indicated in white font and grey background correspond to enzymes not listed in the current lists:

",1
Life Science,List of restriction enzyme cutting sites: S,"This article contains a list of the most studied restriction enzymes whose names start with S.  It contains approximately 130 enzymes.
The following information is given:

Enzyme: Accepted name of the molecule, according to the internationally adopted nomenclature, and bibliographical references. (Further reading: see the section ""Nomenclature"" in the article ""Restriction enzyme"".)
PDB code: Code used to identify the structure of a protein in the PDB database of protein structures. The 3D atomic structure of a protein provides highly valuable information to understand the intimate details of its mechanism of action.
Source: Organism that naturally produces the enzyme.
Recognition sequence: Sequence of DNA recognized by the enzyme and to which it specifically binds.
Cut: Cutting site and DNA products of the cut. The recognition sequence and the cutting site usually match, but sometimes the cutting site can be dozens of nucleotides away from the recognition site.
Isoschizomers and neoschizomers: An isoschizomer is an enzyme that recognizes the same sequence as another. A neoschizomer is a special type of isoschizomer that recognizes the same sequence as another, but cuts in a different manner. A maximum number of 8-10 most common isoschizomers are indicated for every enzyme but there may be many more. Neoschizomers are shown in bold and green color font (e.g.: BamHI). When ""None on date"" is indicated, that means that there were no registered isoschizomers in the databases on that date with a clearly defined cutting site. Isoschizomers indicated in white font and grey background correspond to enzymes not listed in the current lists:",1
Life Science,List of restriction enzyme cutting sites: T–Z,"This article contains a list of restriction enzymes whose names start with A and have a clearly defined cutting site.
The following information is given for each enzyme:

Name of Restriction Enzyme: Accepted name of the molecule, according to the internationally adopted nomenclature, and bibliographical references. Note: When alphabetizing, enzymes are first ordered alphabetically by the acronyms (everything before the roman numeral); then enzymes of a given acronym are ordered alphabetically by the roman numeral, treating the numeral as a number and not a string of letters. This helps keep the entries ordered hierarchically while also alphabetic.(Further reading: see the section ""Nomenclature"" in the article ""Restriction enzyme"".)
PDB code: Code used to identify the structure of a protein in the PDB database of protein structures. The 3D atomic structure of a protein provides highly valuable information to understand the intimate details of its mechanism of action.
REBASE Number: Number used to identify restriction enzymes in the REBASE restriction enzyme database. This database includes important information about the enzyme such as Recognition sequence, source, and Isoschizomers, as well as other data, such as the commercial suppliers of the enzyme.
Source: Organism that naturally produces the enzyme.
Recognition sequence: Sequence of DNA recognized by the enzyme and to which it specifically binds.
Cut: Displays the cut site and pattern and products of the cut. The recognition sequence and the cut site usually match, but sometimes the cut site can be dozens of nucleotides away from the recognition site.
Isoschizomers and neoschizomers: An isoschizomer is a restriction enzyme that recognizes the same sequence as another. A neoschizomer is a special type of isoschizomer that recognizes the same sequence as another, but cuts in a different manner. A maximum number of 8–10 most common isoschizomers are indicated for every enzyme but there may be many more. Neoschizomers are shown in bold and green color font (e.g.: BamHI). When ""None as of [date]"" is indicated, that means that there were no registered isoschizomers in the databases on that date with a clearly defined cutting site. Isoschizomers indicated in white font and grey background correspond to enzymes not listed in the current lists, as in this not listed enzyme:   Abc123I",1
Life Science,Living medicine,"A living medicine is a type of biologic that consists of a living organism that is used to treat a disease. This usually takes the form of a cell (animal, bacterial, or fungal) or a virus that has been genetically engineered to possess therapeutic properties that is injected into a patient. Perhaps the oldest use of a living medicine is the use of leeches for bloodletting, though living medicines have advanced tremendously since this time.
Examples of living medicines include cellular therapeutics (including immunotherapeutics), phage therapeutics, and bacterial therapeutics, a subset of the latter being probiotics.

",1
Life Science,Long non-coding RNA,"Long non-coding RNAs (long ncRNAs, lncRNA) are a type of RNA, generally defined as transcripts more than 200 nucleotides that are not translated into protein. This  arbitrary limit distinguishes long ncRNAs from small non-coding RNAs, such as microRNAs (miRNAs), small interfering RNAs (siRNAs), Piwi-interacting RNAs (piRNAs), small nucleolar RNAs (snoRNAs), and other short RNAs. Long intervening/intergenic noncoding RNAs (lincRNAs) are sequences of lncRNA which do not overlap protein-coding genes.Long non-coding RNAs include intergenic lincRNAs, intronic ncRNAs, and sense and antisense lncRNAs, each type showing different genomic positions in relation to genes and exons.",1
Life Science,Lung-on-a-chip,"The lung-on-a-chip is a complex, three-dimensional model of a living, breathing human lung on a microchip. The device is made using human lung and blood vessel cells and it can predict absorption of airborne nanoparticles and mimic the inflammatory response triggered by microbial pathogens. It can be used to test the effects of environmental toxins, absorption of aerosolized therapeutics, and the safety and efficacy of new drugs. It is expected to become an alternative to animal testing.
The lung-on-a-chip places two layers of living tissues—the lining of the lung's air sacs and the blood vessels that surround them—across a porous, flexible boundary. Air is delivered to the lung lining cells, a rich culture medium flows in the capillary channel to mimic blood, and cyclic mechanical stretching is generated by a vacuum applied to the chambers adjacent to the cell culture channels to mimic breathing.
The research findings for lung-on-a-chip were published in the June 25, 2010, issue of Science, the academic journal of the American Association for the Advancement of Science.  The research was funded by the National Institutes of Health, the American Heart Association, and the Wyss Institute for Biologically Inspired Engineering at Harvard University.",1
Life Science,Lysozyme PEGylation,"Lysozyme PEGylation is the covalent attachment of Polyethylene glycol (PEG) to Lysozyme, which is one of the most widely investigated PEGylated proteins. 
The PEGylation of proteins has become a common practice of modern therapeutic drugs, as the process is capable of enhancing solubility, thermal stability, enzymatic degradation resistance, and serum half-life of the proteins of interest. Lysozyme, as a natural bactericidal enzyme, lyses the cell wall of various gram-positive bacteria and offers protection against microbial infections. Lysozyme has six lysine residues which are accessible for PEGylation reactions. Thus, the PEGylation of lysozyme, or lysozyme PEGylation, can be a good model system for the PEGylation of other proteins with enzymatic activities by showing the enhancement of its physical and thermal stability while retaining its activity. Previous works on lysozyme PEGylation showed various chromatographic schemes in order to purify PEGylated lysozyme, which included ion exchange chromatography, hydrophobic interaction chromatography, and size-exclusion chromatography (fast protein liquid chromatography), and proved its stable conformation via circular dichroism and improved thermal stability by enzymatic activity assays, SDS-PAGE, and size-exclusion chromatography (high-performance liquid chromatography).",1
Life Science,Magnet-assisted transfection,"Magnet-assisted transfection is a transfection method which uses magnetic interactions to deliver DNA into target cells. Nucleic acids are associated with magnetic nanoparticles, and magnetic fields drive the nucleic acid-particle complexes into target cells, where the nucleic acids are released.",1
Life Science,Magnotech,"Magnotech is a type of biosensor using magnetic nanoparticles to measure target molecules in blood and saliva in a matter of minutes. The technology is based on magnetic nanoparticles that are actuated by magnetic fields.A cartridge is inserted into a hand-held analyzer. The cartridge is constructed entirely from plastic components, has no moving parts or embedded electronics, and is disposable. It automatically fills itself from a single drop of blood or saliva. Once filled, no other fluid movement is required. The entire assay process within the cartridge is executed by controlled movement of the magnetic nanoparticles, using magnetic fields generated by the hand-held analyzer. The analyzer unit contains the electromagnets, an optical detection system, control electronics, software and the read-out display. Tests have shown that the cardiac marker Troponin I can be measured in blood plasma in around five minutes.
Magnotech was used in the Minicare product of Philips Handheld Diagnostics, which was commercially launched in 2016. In 2018 the technology was spun out into Minicare BV, which was acquired by Siemens Healthineers in July 2019.Magnotech technology is used in Siemens' Atellica VTLi Patient-Side Immunoassay Analyzer, a test for high-sensitivity cardiac troponin I.The technology behind Magnotech was initiated by Philips Research Fellow, Menno Prins. In 2014 he became full professor at Eindhoven University of Technology.

",1
Life Science,MALBAC,"Multiple Annealing and Looping Based Amplification Cycles (MALBAC) is a quasilinear whole genome amplification method. Unlike conventional DNA amplification methods that are non-linear or exponential (in each cycle, DNA copied can serve as template for subsequent cycles), MALBAC utilizes special primers that allow amplicons to have complementary ends and therefore to loop, preventing DNA from being copied exponentially. This results in amplification of only the original genomic DNA and therefore reduces amplification bias. MALBAC is “used to create overlapped shotgun amplicons covering most of the genome”. For next generation sequencing, MALBAC is followed by regular PCR which is used to further amplify amplicons.

",1
Life Science,Male egg,"Male egg can refer to either:

An egg that artificially contains genetic material from a male.
An egg from a haplodiploid species such as an ant or bee that is unfertilized and will hatch a male
A fertilized egg that a male organism is developing inThis article focuses on the first definition.
Male eggs are the result of a process in which the eggs of a female would be emptied of their genetic contents (a technique similar to that used in the cloning process), and those contents would be replaced with male DNA. Such eggs could then be fertilized by sperm.  The procedure was conceived by Calum MacKellar, a Scottish bioethicist. With this technique, two males could be the biological parents of a child. However, such a procedure would additionally require an artificial womb or a female gestational carrier.

",1
Life Science,"Medical Devices Park, Hyderabad","Medical Devices Park, Hyderabad is a medical devices industrial estate located in Hyderabad, Telangana, India. The largest such Park in India spread over 250 acres. The dedicated park's ecosystem supports medical technology innovation and manufacturing.",1
Life Science,Medicon Valley,"Medicon Valley is a leading international life-sciences cluster in Europe, spanning the Greater Copenhagen region of eastern Denmark and southern Sweden. It is one of Europe's strongest life science clusters, with many life science companies and research institutions located within a relatively small geographical area. The name has officially been in use since 1997.
Major life science sectors of the Medicon Valley cluster includes pharmacology, biotechnology, health tech and medical technology. It is specifically known for its research strengths in the areas of neurological disorders, inflammatory diseases, cancer and diabetes.",1
Life Science,Meganuclease,"Meganucleases are endodeoxyribonucleases characterized by a large recognition site (double-stranded DNA sequences of 12 to 40 base pairs); as a result this site generally occurs only once in any given genome. For example, the 18-base pair sequence recognized by the I-SceI meganuclease would on average require a genome twenty times the size of the human genome to be found once by chance (although sequences with a single mismatch occur about three times per human-sized genome). Meganucleases are therefore considered to be the most specific naturally occurring restriction enzymes.
Among meganucleases, the LAGLIDADG family of homing endonucleases has become a valuable tool for the study of genomes and genome engineering over the past fifteen years. 
Meganucleases are ""molecular DNA scissors"" that can be used to replace, eliminate or modify sequences in a highly targeted way. By modifying their recognition sequence through protein engineering, the targeted sequence can be changed. Meganucleases are used to modify all genome types, whether bacterial, plant or animal. They open up wide avenues for innovation, particularly in the field of human health, for example the elimination of viral genetic material or the ""repair"" of damaged genes using gene therapy.",1
Life Science,Melomics,"Melomics (derived from ""genomics of melodies"") is a computational system for the automatic composition of music (with no human intervention), based on bioinspired algorithms.",1
Life Science,Metagenics,"In genetics, a mutagen is a physical or chemical agent that permanently changes genetic material, usually DNA, in an organism and thus increases the frequency of mutations above the natural background level. As many mutations can cause cancer, such mutagens are therefore carcinogens, although not all necessarily are. All mutagens have characteristic mutational signatures with some chemicals becoming mutagenic through cellular processes. 
The process of DNA becoming modified is called mutagenesis. Not all mutations are caused by mutagens: so-called ""spontaneous mutations"" occur due to spontaneous hydrolysis, errors in DNA replication, repair and recombination.",1
Life Science,Michigan Life Sciences Corridor,"The Michigan Life Sciences Corridor (MLSC) is a $1 billion biotechnology initiative in the U.S. state of Michigan. 
The MLSC invests in biotech research at four Michigan institutions: the University of Michigan in Ann Arbor; Michigan State University in East Lansing; Wayne State University in Detroit; and the Van Andel Institute in Grand Rapids. 
The Michigan Economic Development Corporation administers the program.  It began in 1999 with money from the state's settlement with the tobacco industry. When the program's funds distributions are completed in 2019, the goal is that the investments in high tech research will have notably expanded the state's economic base.",1
Life Science,Microbead (research),"Microbeads, also called Ugelstad particles after the Norwegian chemist, professor dr.philos. John Ugelstad, who invented them in 1977 and patented the method in 1978, are uniform polymer particles, typically 0.5 to 500 micrometres in diameter. Bio-reactive molecules can be absorbed or coupled to their surface, and used to separate biological materials such as cells, proteins, or nucleic acids.
Microbeads have been used for isolation and handling of specific material or molecules, as well as for analyzing sensitive molecules, or those that are in low abundance, e.g. in miniaturized and automated settings.

",1
Life Science,Microbial biodegradation,"Microbial biodegradation is the use of bioremediation and biotransformation methods to harness the naturally occurring ability of microbial xenobiotic metabolism to degrade, transform or accumulate environmental pollutants, including hydrocarbons (e.g. oil), polychlorinated biphenyls (PCBs), polyaromatic hydrocarbons (PAHs), heterocyclic compounds (such as pyridine or quinoline), pharmaceutical substances, radionuclides and metals.
Interest in the microbial biodegradation of pollutants has intensified in recent years, and recent major methodological breakthroughs have enabled detailed genomic, metagenomic, proteomic, bioinformatic and other high-throughput analyses of environmentally relevant microorganisms, providing new insights into biodegradative pathways and the ability of organisms to adapt to changing environmental conditions.
Biological processes play a major role in the removal of contaminants and take advantage of the catabolic versatility of microorganisms to degrade or convert such compounds.  In environmental microbiology, genome-based global studies are increasing the understanding of metabolic and regulatory networks, as well as providing new information on the evolution of degradation pathways and molecular adaptation strategies to changing environmental conditions.",1
Life Science,Microbial electrolysis cell,"A microbial electrolysis cell (MEC) is a technology related to Microbial fuel cells (MFC). Whilst MFCs produce an electric current from the microbial decomposition of organic compounds, MECs partially reverse the process to generate hydrogen or methane from organic material by applying an electric current. The electric current would ideally be produced by a renewable source of power. The hydrogen or methane produced can be used to produce electricity by means of an additional PEM fuel cell or internal combustion engine.",1
Life Science,Microbial electrosynthesis,"Microbial electrosynthesis (MES) is a form of microbial electrocatalysis in which electrons are supplied to living microorganisms via a cathode in an electrochemical cell by applying an electric current. The electrons are then used by the microorganisms to reduce carbon dioxide to yield industrially relevant products. The electric current would ideally be produced by a renewable source of power. This process is the opposite to that employed in a microbial fuel cell, in which microorganisms transfer electrons from the oxidation of compounds to an anode to generate an electric current.

",1
Life Science,Microcarrier,"A microcarrier is a support matrix that allows for the growth of adherent cells in bioreactors. Instead of on a flat surface, cells are cultured on the surface of spherical microcarriers so that each particle carries several hundred cells, and therefore expansion capacity can be multiplied several times over. It provides a straightforward way to scale up culture systems for industrial production of cell or protein-based therapies, or for research purposes.These solid or porous spherical matrices range anywhere between 100-300 um in diameter to allow sufficient surface area while retaining enough cell adhesion and support, and their density is minimally above that of water (1 g/ml) so that they remain in suspension in a stirred tank. They can be composed of either synthetic materials such as acrylamide or natural materials such as gelatin.The advantages of microcarrier technology in the biotech industry include (a) ease of scale-up, (b) ability to precisely control cell growth conditions in sophisticated, computer-controlled bioreactors, (c) an overall reduction in the floor space and incubator volume required for a given-sized manufacturing operation, (d) a drastic reduction in technician labor, and (e) a more natural environment for cell culture that promotes differentiation.",1
Life Science,Microfluidics,"Microfluidics refers to the behavior, precise control, and manipulation of fluids that are geometrically constrained to a small scale (typically sub-millimeter) at which surface forces dominate volumetric forces. It is a multidisciplinary field that involves engineering, physics, chemistry, biochemistry, nanotechnology, and biotechnology. It has practical applications in the design of systems that process low volumes of fluids to achieve multiplexing, automation, and high-throughput screening. Microfluidics emerged in the beginning of the 1980s and is used in the development of inkjet printheads, DNA chips, lab-on-a-chip technology, micro-propulsion, and micro-thermal technologies.
Typically, micro means one of the following features:

Small volumes (μL, nL, pL, fL)
Small size
Low energy consumption
Microdomain effectsTypically microfluidic systems transport, mix, separate, or otherwise process fluids. Various applications rely on passive fluid control using capillary forces, in the form of capillary flow modifying elements, akin to flow resistors and flow accelerators. In some applications, external actuation means are additionally used for a directed transport of the media. Examples are rotary drives applying centrifugal forces for the fluid transport on the passive chips. Active microfluidics refers to the defined manipulation of the working fluid by active (micro) components such as micropumps or microvalves. Micropumps supply fluids in a continuous manner or are used for dosing. Microvalves determine the flow direction or the mode of movement of pumped liquids. Often, processes normally carried out in a lab are miniaturised on a single chip, which enhances efficiency and mobility, and reduces sample and reagent volumes.",1
Life Science,Microinjection,"Microinjection is the use of a glass micropipette to inject a liquid substance at a microscopic or borderline macroscopic level.  The target is often a living cell but may also include intercellular space. Microinjection is a simple mechanical process usually involving an inverted microscope with a magnification power of around 200x (though sometimes it is performed using a dissecting stereo microscope at 40–50x or a traditional compound upright microscope at similar power to an inverted model).
For processes such as cellular or pronuclear injection the target cell is positioned under the microscope and two micromanipulators—one holding the pipette and one holding a microcapillary needle usually between 0.5 and 5 µm in diameter (larger if injecting stem cells into an embryo)—are used to penetrate the cell membrane and/or the nuclear envelope.  In this way the process can be used to introduce a vector into a single cell. Microinjection can also be used in the cloning of organisms, in the study of cell biology and viruses, and for treating male subfertility through intracytoplasmic sperm injection (ICSI,  IK-see).",1
Life Science,Microphysiometry,"Microphysiometry is the in vitro measurement of the functions and activities of life or of living matter (as organs, tissues, or cells) and of the physical and chemical phenomena involved on a very small (micrometer) scale. The term microphysiometry emerged in the scientific literature at the end of the 1980s.The primary parameters assessed in microphysiometry comprise pH and the concentration of dissolved oxygen, glucose, and lactic acid, with an emphasis on the first two. Measuring these parameters experimentally in combination with a fluidic system for cell culture maintenance and a defined application of drugs or toxins provides the quantitative output parameters extracellular acidification rates (EAR), oxygen consumption rates (OUR), and rates of glucose consumption or lactate release to characterize the metabolic situation.
Due to the label-free nature of sensor-based measurements, dynamic monitoring of cells or tissues for several days or even longer is feasible. On an extended timescale, a dynamic analysis of a cell’s metabolic response to an experimental treatment can distinguish acute effects (e.g., one hour after a treatment), early effects (e.g., at 24 hours), and delayed, chronic responses (e.g., at 96 hours). As stated by Alajoki et al., ""The concept is that it is possible to detect receptor activation and other physiological changes in living cells by monitoring the activity of energy metabolism"".",1
Life Science,Microraft,"A Microraft (Isoraft) is an arrays of microwells for cell sorting, isolating cells, analyzing cells over time, and generating clonal populations. This platform provides biomedical scientists with access to diverse cell culture surfaces with integrated, easy-to-use cell separating capabilities at low cost.",1
Life Science,Minigene,"A minigene is a minimal gene fragment that includes an exon and the control regions necessary for the gene to express itself in the same way as a wild type gene fragment. This is a minigene in its most basic sense. More complex minigenes can be constructed containing multiple exons and intron(s). Minigenes provide a valuable tool for researchers evaluating splicing patterns both in vivo and in vitro biochemically assessed experiments. Specifically, minigenes are used as splice reporter vectors (also called exon-trapping vectors) and act as a probe to determine which factors are important in splicing outcomes. They can be constructed to test the way both cis-regulatory elements (RNA effects) and trans-regulatory elements (associated proteins/splicing factors) affect gene expression.

",1
Life Science,Molecular diagnostics,"Molecular diagnostics is a collection of techniques used to analyze biological markers in the genome and proteome, and how their cells express their genes as proteins, applying molecular biology to medical testing. In medicine the technique is used to diagnose and monitor disease, detect risk, and decide which therapies will work best for individual patients,: foreword  and in agricultural biosecurity similarly to monitor crop- and livestock disease, estimate risk, and decide what quarantine measures must be taken.By analysing the specifics of the patient and their disease, molecular diagnostics offers the prospect of personalised medicine.
These tests are useful in a range of medical specialties, including infectious disease, oncology, human leucocyte antigen typing (which investigates and predicts immune function), coagulation, and pharmacogenomics—the genetic prediction of which drugs will work best.: v-vii  They overlap with clinical chemistry (medical tests on bodily fluids).",1
Life Science,Molecular-weight size marker,"A molecular-weight size marker, also referred to as a protein ladder, DNA ladder, or RNA ladder, is a set of standards that are used to identify the approximate size of a molecule run on a gel during electrophoresis, using the principle that molecular weight is inversely proportional to migration rate through a gel matrix. Therefore, when used in gel electrophoresis, markers effectively provide a logarithmic scale by which to estimate the size of the other fragments (providing the fragment sizes of the marker are known).
Protein, DNA, and RNA markers with pre-determined fragment sizes and concentrations are commercially available. These can be run in either agarose or polyacrylamide gels. The markers are loaded in lanes adjacent to sample lanes before the commencement of the run.

",1
Life Science,MolecularLab,"MolecularLab is an Italian website of science, specialized in science, biotechnology, molecular biology, with news, forums, and events. With over 4 million page views in May 2009 it is the most visited Italian science webzine.",1
Life Science,Monoclonal antibody,"A monoclonal antibody (mAb, more rarely called moAb) is an antibody made by cloning a unique white blood cell. All subsequent antibodies derived this way trace back to a unique parent cell.
Monoclonal antibodies can have monovalent affinity, binding only to the same epitope (the part of an antigen that is recognized by the antibody). In contrast, polyclonal antibodies bind to multiple epitopes and are usually made by several different antibody-secreting plasma cell lineages. Bispecific monoclonal antibodies can also be engineered, by increasing the therapeutic targets of one monoclonal antibody to two epitopes.
It is possible to produce monoclonal antibodies that specifically bind to virtually any suitable substance; they can then serve to detect or purify it. This capability has become an important tool in biochemistry, molecular biology, and medicine. Monoclonal antibodies are being used on a clinical level for both the diagnosis and therapy of several diseases. The administration of monoclonal antibodies was authorized by several countries for the treatment of moderate COVID-19 symptoms.

",1
Life Science,Morpholino,"Morpholine is an organic chemical compound having the chemical formula O(CH2CH2)2NH. This heterocycle features both amine and ether functional groups. Because of the amine, morpholine is a base; its conjugate acid is called morpholinium. For example, treating morpholine with hydrochloric acid makes the salt morpholinium chloride. It is a colorless liquid with a weak, ammonia- or fish-like odor. The naming of morpholine is attributed to Ludwig Knorr, who incorrectly believed it to be part of the structure of morphine.",1
Life Science,Moss bioreactor,A moss bioreactor is a photobioreactor used for the cultivation and propagation of mosses. It is usually used in molecular farming for the production of recombinant protein using transgenic moss. In environmental science moss bioreactors are used to multiply peat mosses e.g. by the Mossclone consortium to monitor air pollution.Moss is a very frugal photoautotrophic organism that has been kept in vitro for research purposes since the beginning of the 20th century.The first moss bioreactors for the model organism Physcomitrella patens were developed in the 1990s to comply with the safety standards regarding the handling of genetically modified organisms and to gain sufficient biomass for experimental purposes.,1
Life Science,Mutanome,The mutanome is the entirety of somatic cancer mutations in an individual tumor.,1
Life Science,Mutation breeding,"Mutation breeding, sometimes referred to as ""variation breeding"", is the process of exposing seeds to chemicals, radiation, or enzymes in order to generate mutants with desirable traits to be bred with other cultivars.  Plants created using mutagenesis are sometimes called mutagenic plants or mutagenic seeds.
From 1930 to 2014 more than 3200 mutagenic plant varieties were released that have been derived either as direct mutants (70%) or from their progeny (30%). Crop plants account for 75% of released mutagenic species with the remaining 25% ornamentals or decorative plants.  However, although the FAO/IAEA reported in 2014 that over 1,000 mutant varieties of major staple crops were being grown worldwide, it is unclear how many of these varieties are currently used in agriculture or horticulture around the world, as these seeds are not always identified or labeled as having a mutagenic provenance.",1
Life Science,Mycoremediation,"Mycoremediation (from ancient Greek μύκης (mukēs), meaning ""fungus"" and the suffix -remedium, in Latin meaning 'restoring balance') is a form of bioremediation in which fungi-based remediation methods are used to decontaminate the environment. Fungi have been proven to be a cheap, effective and environmentally sound way for removing a wide array of contaminants from damaged environments or wastewater. These contaminants include heavy metals, organic pollutants, textile dyes, leather tanning chemicals and wastewater, petroleum fuels, polycyclic aromatic hydrocarbons, pharmaceuticals and personal care products, pesticides and herbicides in land, fresh water, and marine environments.
The byproducts of the remediation can be valuable materials themselves, such as enzymes (like laccase), edible or medicinal mushrooms, making the remediation process even more profitable. Some fungi are useful in the biodegradation of contaminants in extremely cold or radioactive environments where traditional remediation methods prove too costly or are unusable due to the extreme conditions. Mycoremediation can even be used for fire management with the encapsulation method. This process consists of using fungal spores coated with agarose in a pellet form. This pellet is introduced to a substrate in the burnt forest, breaking down the toxins in the environment and stimulating growth.",1
Life Science,NAIL-MS,"NAIL-MS (short for nucleic acid isotope labeling coupled mass spectrometry) is a technique based on mass spectrometry used for the investigation of nucleic acids and its modifications. It enables a variety of experiment designs to study the underlying mechanism of RNA biology in vivo. For example, the dynamic behaviour of nucleic acids in living cells, especially of RNA modifications, can be followed in more detail.",1
Life Science,Nanoneuronics,"Nanoneuronics is an emerging discipline involving the application of nanometer-scale methods, materials, science and technology to neurons and neural tissue in order to design and develop advanced medical applications.",1
Life Science,National Institute of Biotechnology,National Institute of Biotechnology (NIB)(Bengali: ন্যাশনাল ইনস্টিটিউট অব বায়োটেকনোলজি) is a governmental institute in Bangladesh under the Ministry of Science and Technology.,1
Life Science,Nb.BbvCI,Nb.BbvCI is a nicking endonuclease used to cut one strand of double-stranded DNA. It has been successfully used to incorporate fluorochrome-labeled nucleotides into specific spots of a DNA sequence via nick translation.,1
Life Science,Neoepitope,Neoepitopes are a class of major histocompatibility complex (MHC) bounded peptides. They represent the antigenic determinants of neoantigens. Neoepitopes are recognized by the immune system as targets for T cells and can elicit immune response to cancer.,1
Life Science,New Research Building,"The New Research Building (or NRB for short) is the largest building ever built by Harvard University, and was dedicated on September 24, 2003 by the then president of Harvard University, Lawrence H. Summers and the dean of the Harvard Medical School, Joseph Martin.
It is an integrated biomedical research facility, located at 77 Avenue Louis Pasteur, Boston, Massachusetts, at the Longwood Medical Area and has 525,000 square feet (48,800 m2) of space. It cost $260 million to build, and houses more than 800 researchers, and many more graduate students, lab assistants, and staff workers. The building sits across the street from the Boston Latin School on the former site of Boston English High School.
It constitutes the largest expansion of Harvard Medical school witnessed in the last 100 years. It houses the Department of Genetics of the Harvard Medical School, among many other centers and institutes it houses. It is also home to many meetings, and has a 500-seat auditorium.
The architects were Architectural Resources Cambridge, Inc. (ARC) who are active in the Boston/Cambridge area and have built other educational and research facilities.

",1
Life Science,Nic site,"A nick is a discontinuity in a double stranded DNA molecule where there is no phosphodiester bond between adjacent nucleotides of one strand typically through damage or enzyme action. Nicks allow DNA strands to untwist during replication, and are also thought to play a role in the DNA mismatch repair mechanisms that fix errors on both the leading and lagging daughter strands.",1
Life Science,Nicking enzyme,"A nicking enzyme (or nicking endonuclease) is an enzyme that cuts one strand of a double-stranded DNA at a specific recognition nucleotide sequences known as a restriction site. Such enzymes hydrolyse (cut) only one strand of the DNA duplex, to produce DNA molecules that are “nicked”, rather than cleaved.They can be used for strand-displacement amplification, Nicking Enzyme Amplification Reaction, exonucleotyic degradation, the creation of small gaps, or nick translation. The latter process has been successfully used to incorporate both radioactively labelled nucleotides and fluorescent nucleotides allowing specific regions on a double stranded DNA to be studied. Over 200 nicking enzymes have been studied, and 13 of these are available commercially and are routinely used for research and in commercial products.",1
Life Science,NK-92,"The NK-92 cell line is an immortal cell line that has the features and characteristics of a type of immune cell found in human blood called ’natural killer’ (NK) cells. Blood NK cells and NK-92 cells recognize and attack cancer cells as well as cells that have been infected with a virus, bacteria or fungus. These unique NK cells were originally isolated in 1992 in the laboratory of Hans Klingemann at the British Columbia Cancer Agency in Vancouver, Canada, from a patient who had a rare NK cell lymphoma and were subsequently transformed in culture into a continuously growing NK cell line and characterized for the first time. NK-92 cells are distinguished by their suitability for expansion to large numbers, ability to consistently kill cancer cells and testing in clinical trials. When NK-92 cells recognize a cancerous or infected cell, they secrete perforin that, punches holes into the diseased cells and releases granzymes that kill the target cells. NK-92 cells are also capable of producing cytokines such as cancer cell-killing tumor necrosis alpha factor (TNF-a) and interferon, gamma (IFN-y), which stimulates proliferation and activation of other immune cells.",1
Life Science,NlaIII,"NlaIII is a type II restriction enzyme isolated from Neisseria lactamica. As part of the restriction modification system, NlaIII is able to prevent foreign DNA from integrating into the host genome by cutting double stranded DNA into fragments at specific sequences. This results in further degradation of the fragmented foreign DNA and prevents it from infecting the host genome.
NlaIII recognizes the palindromic and complementary DNA sequence of CATG/GTAC and cuts outside of the G-C base pairs. This cutting pattern results in sticky ends with GTAC overhangs at the 3' end.",1
Life Science,Nuclear transfer,"Nuclear transfer is a form of cloning.  The step involves removing the DNA from an oocyte (unfertilised egg), and injecting the nucleus which contains the DNA to be cloned. In rare instances, the newly constructed cell will divide normally, replicating the new DNA while remaining in a pluripotent state.  If the cloned cells are placed in the uterus of a female mammal, a cloned organism develops to term in rare instances. This is how Dolly the Sheep and many other species were cloned. Cows are commonly cloned to select those that have the best milk production. On 24 January 2018, two monkey clones were reported to have been created with the technique for the first time.Despite this, the low efficiency of the technique has prompted some researchers, notably Ian Wilmut, creator of Dolly the cloned sheep, to abandon it.",1
Life Science,Nucleic acid thermodynamics,"Nucleic acid thermodynamics is the study of how temperature affects the nucleic acid structure of double-stranded DNA (dsDNA). The melting temperature (Tm) is defined as the temperature at which half of the DNA strands are in the random coil or single-stranded (ssDNA) state. Tm depends on the length of the DNA molecule and its specific nucleotide sequence. DNA, when in a state where its two strands are dissociated (i.e., the dsDNA molecule exists as two independent strands), is referred to as having been denatured by the high temperature.

",1
Life Science,Oncogenomics,"Oncogenomics is a sub-field of genomics that characterizes cancer-associated genes. It focuses on genomic, epigenomic and transcript alterations in cancer.
Cancer is a genetic disease caused by accumulation of DNA mutations and epigenetic alterations leading to unrestrained cell proliferation and neoplasm formation. The goal of oncogenomics is to identify new oncogenes or tumor suppressor genes that may provide new insights into cancer diagnosis, predicting clinical outcome of cancers and new targets for cancer therapies. The success of targeted cancer therapies such as Gleevec, Herceptin and Avastin raised the hope for oncogenomics to elucidate new targets for cancer treatment.

Besides understanding the underlying genetic mechanisms that initiate or drive cancer progression, oncogenomics targets personalized cancer treatment. Cancer develops due to DNA mutations and epigenetic alterations that accumulate randomly. Identifying and targeting the mutations in an individual patient may lead to increased treatment efficacy.
The completion of the Human Genome Project facilitated the field of oncogenomics and increased the abilities of researchers to find oncogenes. Sequencing technologies and global methylation profiling techniques have been applied to the study of oncogenomics.",1
Life Science,Oncolytic adenovirus,"Adenovirus varieties have been explored extensively as a viral vector for gene therapy and also as an oncolytic virus.Of the many different viruses being explored for oncolytic potential, an adenovirus was the first to be approved by a regulatory agency, the genetically modified H101 strain. It gained regulatory approval in 2005 from China's State Food and Drug Administration (SFDA) for the treatment of head and neck cancer.",1
Life Science,Oncolytic herpes virus,"An oncolytic virus is a virus that preferentially infects and kills cancer cells. As the infected cancer cells are destroyed by oncolysis, they release new infectious virus particles or virions to help destroy the remaining tumour. Oncolytic viruses are thought not only to cause direct destruction of the tumour cells, but also to stimulate host anti-tumour immune system responses. Oncolytic viruses also have the ability to affect the tumor micro-environment in multiples ways.The potential of viruses as anti-cancer agents was first realised in the early twentieth century, although coordinated research efforts did not begin until the 1960s. A number of viruses including adenovirus, reovirus, measles, herpes simplex, Newcastle disease virus, and vaccinia have been clinically tested as oncolytic agents. Most current oncolytic viruses are engineered for tumour selectivity, although there are naturally occurring examples such as reovirus and the senecavirus, resulting in clinical trials.The first oncolytic virus to be approved by a national regulatory agency was genetically unmodified ECHO-7 strain enterovirus RIGVIR, which was approved in Latvia in 2004 for the treatment of skin melanoma; the approval was withdrawn in 2019. An oncolytic adenovirus, a genetically modified adenovirus named H101, was approved in China in 2005 for the treatment of head and neck cancer. In 2015, talimogene laherparepvec (OncoVex, T-VEC), an oncolytic herpes virus which is a modified herpes simplex virus, became the first oncolytic virus to be approved for use in the U.S. and European Union, for the treatment of advanced inoperable melanoma.",1
Life Science,Oncomatryx,"Oncomatryx Biopharma S. L. is a pharmaceutical biotechnology company that develops personalized treatments against invasive cancer as well as tests for its early detection. Established by Laureano Simón, PhD, Oncomatryx thus engages twofold in the fight against invasive kinds of cancer, such as pancreatic cancer or invasive breast cancer, all of which have high mortality rates.
Oncomatryx's research focuses on peritumoral stroma, which has been found to take part in promoting cancer invasiveness and curtailing treatment efficacy.",1
Life Science,Open Insulin Project,"The Open Insulin Project is a collection of researchers and advocates working to develop an open source protocol for producing insulin that is affordable, has transparent pricing, and is community-owned.",1
Life Science,Optoelectrowetting,"Optoelectrowetting (OEW) is a method of liquid droplet manipulation used in microfluidics applications. This technique builds on the principle of electrowetting, which has proven useful in liquid actuation due to fast switching response times and low power consumption. Where traditional electrowetting runs into challenges, however, such as in the simultaneous manipulation of multiple droplets, OEW presents a lucrative alternative that is both simpler and cheaper to produce. OEW surfaces are easy to fabricate, since they require no lithography, and have real-time, reconfigurable, large-scale manipulation control, due to its reaction to light intensity.",1
Life Science,Organ-on-a-chip,"An organ-on-a-chip (OOC) is a multi-channel 3-D microfluidic cell culture, integrated circuit (chip) that simulates the activities, mechanics and physiological response of an entire organ or an organ system, a type of artificial organ.  It constitutes the subject matter of significant biomedical engineering research, more precisely in bio-MEMS. The convergence of labs-on-chips (LOCs) and cell biology has permitted the study of human physiology in an organ-specific context, introducing a novel model of in vitro multicellular human organisms. One day, they will perhaps abolish the need for animals in drug development and toxin testing.
Although multiple publications claim to have translated organ functions onto this interface, the movement towards this microfluidic application is still in its infancy. Organs-on-chips will vary in design and approach between different researchers. As such, validation and optimization of these systems will likely be a long process. Organs that have been simulated by microfluidic devices include brain, lung, heart, kidney, liver, prostate, vessel(artery), skin, bone, cartilage and more.
Nevertheless, building valid artificial organs requires not only a precise cellular manipulation, but a detailed understanding of the human body's fundamental intricate response to any event.  A common concern with organs-on-chips lies in the isolation of organs during testing. The body is a complex network of physiological processes, making it challenging to simulate a single organ. Microfabrication, microelectronics and microfluidics offer the prospect of modeling sophisticated in vitro physiological responses under accurately simulated conditions.The development of organ chips has enabled the study of the complex pathophysiology of human viral infections. An example is the liver chip platform that has enabled studies of viral hepatitis.",1
Life Science,Out of autoclave composite manufacturing,"Out of autoclave composite manufacturing is an alternative to the traditional high pressure autoclave (industrial) curing process commonly used by the aerospace manufacturers for manufacturing composite material. Out of autoclave (OOA) is a process that achieves the same quality as an autoclave but through a different process. OOA curing achieves the desired fiber content and elimination of voids by placing the layup within a closed mold and applying vacuum, pressure, and heat by means other than an autoclave. An RTM press is the typical method of applying heat and pressure to the closed mold. There are several out of autoclave technologies in current use including resin transfer molding (RTM), Same Qualified Resin Transfer Molding (SQRTM), vacuum-assisted resin transfer molding (VARTM), and balanced pressure fluid molding. The most advanced of these processes can produce high-tech net shape aircraft components.

",1
Life Science,Palivizumab,"Palivizumab, sold under the brand name Synagis, is a monoclonal antibody produced by recombinant DNA technology used to prevent severe disease caused by respiratory syncytial virus (RSV) infections. It is recommended for infants at high-risk for RSV due to conditions such as prematurity or other medical problems including heart or lung diseases.The most common side effects include fever and rash.Palivizumab is a humanized monoclonal antibody (IgG) directed against an epitope in the A antigenic site of the F protein of RSV.  In two phase III clinical trials in the pediatric population, palivizumab reduced the risk of hospitalization due to RSV infection by 55% and 45%.  Palivizumab is dosed once a month via intramuscular (IM) injection, to be administered throughout the duration of the RSV season, which in based on past trends has started in Mid-September to Mid-November.Palivizumab targets the fusion protein of RSV, inhibiting its entry into the cell and thereby preventing infection. Palivizumab was approved for medical use in 1998.",1
Life Science,Pegol,"Certolizumab pegol, sold under the brand name Cimzia, is a biologic medication for the treatment of Crohn's disease, rheumatoid arthritis, psoriatic arthritis and ankylosing spondylitis. It is a fragment of a monoclonal antibody specific to tumor necrosis factor alpha (TNF-α) and is manufactured by UCB.It is on the World Health Organization's List of Essential Medicines.",1
Life Science,PEGylation,"PEGylation (or pegylation) is the process of both covalent and non-covalent attachment or amalgamation of polyethylene glycol (PEG, in pharmacy called macrogol) polymer chains to molecules and macrostructures, such as a drug, therapeutic protein or vesicle, which is then described as PEGylated. PEGylation affects the resulting derivatives or aggregates interactions, which typically slows down their coalescence and degradation as well as elimination in vivo. PEGylation is routinely achieved by the incubation of a reactive derivative of PEG with the target molecule. The covalent attachment of PEG to a drug or therapeutic protein can ""mask"" the agent from the host's immune system (reducing immunogenicity and antigenicity), and increase its hydrodynamic size (size in solution), which prolongs its circulatory time by reducing renal clearance. PEGylation can also provide water solubility to hydrophobic drugs and proteins. Having proven its pharmacological advantages and acceptability, PEGylation technology is the foundation of a growing multibillion-dollar industry.

",1
Life Science,Personalized medicine,"Personalized medicine, also referred to as precision medicine, is a medical model that separates people into different groups—with medical decisions, practices, interventions and/or products being tailored to the individual patient based on their predicted response or risk of disease. The terms personalized medicine, precision medicine, stratified medicine and P4 medicine are used interchangeably to describe this concept though some authors and organisations use these expressions separately to indicate particular nuances.While the tailoring of treatment to patients dates back at least to the time of Hippocrates, the term has risen in usage in recent years given the growth of new diagnostic and informatics approaches that provide understanding of the molecular basis of disease, particularly genomics. This provides a clear evidence base on which to stratify (group) related patients.Among 14 Grand Challenges for Engineering, initiative sponsored by National Academy of Engineering (NAE), personalized medicine has been identified as a key and prospective approach to “achieve optimal individual health decisions”, therefore overcoming the challenge of “Engineer better medicines”.",1
Life Science,PGreen,"Green is the color between cyan and yellow on the visible spectrum. It is evoked by light which has a dominant wavelength of roughly 495–570 nm. In subtractive color systems, used in painting and color printing, it is created by a combination of yellow and cyan; in the RGB color model, used on television and computer screens, it is one of the additive primary colors, along with red and blue, which are mixed in different combinations to create all other colors. By far the largest contributor to green in nature is chlorophyll, the chemical by which plants photosynthesize and convert sunlight into chemical energy. Many creatures have adapted to their green environments by taking on a green hue themselves as camouflage. Several minerals have a green color, including the emerald, which is colored green by its chromium content.
During post-classical and early modern Europe, green was the color commonly associated with wealth, merchants, bankers and the gentry, while red was reserved for the nobility. For this reason, the costume of the Mona Lisa by Leonardo da Vinci and the benches in the British House of Commons are green while those in the House of Lords are red. It also has a long historical tradition as the color of Ireland and of Gaelic culture. It is the historic color of Islam, representing the lush vegetation of Paradise. It was the color of the banner of Muhammad, and is found in the flags of nearly all Islamic countries.In surveys made in American, European, and Islamic countries, green is the color most commonly associated with nature, life, health, youth, spring, hope, and envy. In the European Union and the United States, green is also sometimes associated with toxicity and poor health, but in China and most of Asia, its associations are very positive, as the symbol of fertility and happiness. Because of its association with nature, it is the color of the environmental movement. Political groups advocating environmental protection and social justice describe themselves as part of the Green movement, some naming themselves Green parties. This has led to similar campaigns in advertising, as companies have sold green, or environmentally friendly, products. Green is also the traditional color of safety and permission; a green light means go ahead, a green card permits permanent residence in the United States.",1
Life Science,Pharming (genetics),"Pharming, a portmanteau of ""farming"" and ""pharmaceutical"", refers to the use of genetic engineering to insert genes that code for useful pharmaceuticals into host animals or plants that would otherwise not express those genes, thus creating a genetically modified organism (GMO). Pharming is also known as molecular farming, molecular pharming or biopharming.The products of pharming are recombinant proteins or their metabolic products.  Recombinant proteins are most commonly produced using bacteria or yeast in a bioreactor, but pharming offers the advantage to the producer that it does not require expensive infrastructure, and production capacity can be quickly scaled to meet demand, at greatly reduced cost.",1
Life Science,Phosphoproteomics,"Phosphoproteomics is a branch of proteomics that identifies, catalogs, and characterizes proteins containing a phosphate group as a posttranslational modification.  Phosphorylation is a key reversible modification that regulates protein function, subcellular localization, complex formation, degradation of proteins and therefore cell signaling networks.  With all of these modification results, it is estimated that between 30%–65% of all proteins may be phosphorylated, some multiple times. Based on statistical estimates from many datasets, 230,000, 156,000 and 40,000 phosphorylation sites should exist in human, mouse, and yeast, respectively.Compared to expression analysis, phosphoproteomics provides two additional layers of information.  First, it provides clues on what protein or pathway might be activated because a change in phosphorylation status almost always reflects a change in protein activity.  Second, it indicates what proteins might be potential drug targets as exemplified by the kinase inhibitor Gleevec.  While phosphoproteomics will greatly expand knowledge about the numbers and types of phosphoproteins, its greatest promise is the rapid analysis of entire phosphorylation based signalling networks.",1
Life Science,Photo-reactive amino acid analog,"Photo-reactive amino acid analogs are artificial analogs of natural amino acids that can be used for crosslinking of protein complexes. Photo-reactive amino acid analogs may be incorporated into proteins and peptides in vivo or in vitro. Photo-reactive amino acid analogs in common use are photoreactive diazirine analogs to leucine and methionine, and para-benzoylphenylalanine. Upon exposure to ultraviolet light, they are activated and covalently bind to interacting proteins that are within a few angstroms of the photo-reactive amino acid analog.
L-Photo-leucine and L-photo-methionine are analogs of the naturally occurring L-leucine and L-methionine amino acids that are endogenously incorporated into the primary sequence of proteins during synthesis using the normal translation machinery. They are then ultraviolet light (UV)-activated to covalently crosslink proteins within protein–protein interaction domains in their native in-vivo environment. The method enables the determination and characterization of both stable and transient protein interactions in cells without the addition of chemical crosslinkers and associated solvents that can adversely affect the cell biology being studied in the experiment. 
When used in combination with limiting media that is devoid of leucine and methionine, the photo-activatable derivatives are treated like naturally occurring amino acids by the cellular protein synthesis machinery. As a result, they can be substituted for leucine or methionine in the primary structure of proteins. Photo-leucine and photo-methionine derivatives contain diazirine rings that are activated when exposed to UV light to become reactive intermediates that form covalent bonds with nearby protein side chains and backbones. Naturally interacting proteins within the cell can be instantly trapped by photoactivation of the diazirine-containing proteins in the cultured cells. Crosslinked protein complexes can be detected by decreased mobility on SDS-PAGE followed by Western blotting, size exclusion chromatography, sucrose density gradient sedimentation or mass spectrometry.",1
Life Science,Photobiotin,"Photobiotin is a derivative of biotin used as a biochemical tool.  It is composed of a biotin group, a linker group, and a photoactivatable aryl azide group.
The photoactivatable group provides nonspecific labeling of proteins, DNA and RNA probes or other molecules. Biotinylation of DNA and RNA with photoactivatable biotin is easier and less expensive than enzymatic methods since the DNA and RNA does not degrade.  Photobiotin is most effectively activated by light at 260-475 nm.",1
Life Science,Phycotechnology,"Phycotechnology refers to the  technological applications of algae, both microalgae and macroalgae.",1
Life Science,Phytoremediation,"Phytoremediation  technologies use living plants to clean up soil, air, and water contaminated with hazardous contaminants. It is defined as ""the use of green plants and the associated microorganisms, along with proper soil amendments and agronomic techniques to either contain, remove or render toxic environmental contaminants harmless"". The term is an amalgam of the Greek phyto (plant) and Latin remedium (restoring balance). Although attractive for its cost, phytoremediation has not been demonstrated to redress any significant environmental challenge to the extent that contaminated space has been reclaimed.
Phytoremediation is proposed as a cost-effective plant-based approach of environmental remediation that takes advantage of the ability of plants to concentrate elements and compounds from the environment and to detoxify various compounds. The concentrating effect results from the ability of certain plants called hyperaccumulators to bioaccumulate chemicals. The remediation effect is quite different. Toxic heavy metals cannot be degraded, but organic pollutants can be and are generally the major targets for phytoremediation. Several field trials confirmed the feasibility of using plants for environmental cleanup.",1
Life Science,Phytotechnology,"Phytotechnology (from Ancient Greek  φυτο (phyto) 'plant', and  τεχνολογία (technología); from  τέχνη (téchnē) 'art, skill, craft', and  -λογία (-logía) 'study of-') implements solutions to scientific and engineering problems in the form of plants. It is distinct from ecotechnology and biotechnology as these fields encompass the use and study of ecosystems and living beings, respectively. Current study of this field has mostly been directed into contaminate removal (phytoremediation), storage (phytosequestration) and accumulation (see hyperaccumulators). Plant-based technologies have become alternatives to traditional cleanup procedures because of their low capital costs, high success rates, low maintenance requirements, end-use value, and aesthetic nature.",1
Life Science,Pittsburgh Life Sciences Greenhouse,"Pittsburgh Life Sciences Greenhouse (PLSG) is an investment firm based in the South Side neighborhood of Pittsburgh, Pennsylvania that provides resources and tools to entrepreneurial life sciences enterprises in Pittsburgh and western Pennsylvania in order to advance research and patient care.",1
Life Science,Plant virus,"Plant viruses are viruses that affect plants. Like all other viruses, plant viruses are obligate intracellular parasites that do not have the molecular machinery to replicate without a host. Plant viruses can be pathogenic to higher plants.
Most plant viruses are rod-shaped, with protein discs forming a tube surrounding the viral genome; isometric particles are another common structure. They rarely have an envelope. The great majority have an RNA genome, which is usually small and single stranded (ss), but some viruses have double-stranded (ds) RNA, ssDNA or dsDNA genomes. Although plant viruses are not as well understood as their animal counterparts, one plant virus has become very recognizable: tobacco mosaic virus (TMV), the first virus to be discovered. This and other viruses cause an estimated US$60 billion loss in crop yields worldwide each year. Plant viruses are grouped into 73 genera and 49 families. However, these figures relate only to cultivated plants, which represent only a tiny fraction of the total number of plant species. Viruses in wild plants have not been well-studied, but the interactions between wild plants and their viruses often do not appear to cause disease in the host plants.To transmit from one plant to another and from one plant cell to another, plant viruses must use strategies that are usually different from animal viruses. Most plants do not move, and so plant-to-plant transmission usually involves vectors (such as insects). Plant cells are surrounded by solid cell walls, therefore transport through plasmodesmata is the preferred path for virions to move between plant cells. Plants have specialized mechanisms for transporting mRNAs through plasmodesmata, and these mechanisms are thought to be used by RNA viruses to spread from one cell to another. Plant defenses against viral infection include, among other measures, the use of siRNA in response to dsRNA. Most plant viruses encode a protein to suppress this response. Plants also reduce transport through plasmodesmata in response to injury.",1
Life Science,Plasmonic lens,"In nano-optics, a plasmonic lens generally refers to a lens for surface plasmon polaritons (SPPs), i.e. a device that redirects SPPs to converge towards a single focal point. Because SPPs can have very small wavelength, they can converge into a very small and very intense spot, much smaller than the free space wavelength and the diffraction limit.A simple example of a plasmonic lens is a series of concentric rings on a metal film. Any light that hits the film from free space at a 90 degree angle, known as the normal, will get coupled into a SPP (this part works like a diffraction grating coupler), and that SPP will be heading towards the center of the circles, which is the focal point. Another example is a tapered ""dimple"".In 2007, a novel, or technologically new, plasmonic lenses and waveguide by modulating light a mesoscale dielectric structure on a metallic film with arrayed nano-slits, which have constant depth but variant widths. The slits transport electromagnetic energy in the form of SPPs in nano meter sized waveguides and provide desired phase adjustments for manipulating the beam of light. The scientists claim that it is an improvement over other subwavelength imaging techniques, such as ""superlenses"", where the object and image are confined to the near field.These devices have been suggested for various applications that take advantage of the small size and high intensity of the SPPs at the focal point. These include photolithography, heat-assisted magnetic recording, microscopy, biophotonics, biological molecule sensors, and solar cells, as well as other applications.The term ""plasmonic lens"" is also sometimes used to describe something different: Any free-space lens (i.e., a lens that focuses free-space light, rather than SPPs), that has something to do with plasmonics. These often come up in discussions of superlenses.",1
Life Science,Polly and Molly,"Polly and Molly (born 1997), two ewes, were the first mammals to have been successfully cloned from an adult somatic cell and to be transgenic animals at the same time. This is not to be confused with Dolly the Sheep, the first animal to be successfully cloned from an adult somatic cell where there wasn’t modification carried out on the adult donor nucleus. Polly and Molly, like Dolly the Sheep, were cloned at the Roslin Institute in Edinburgh, Scotland.
The creation of Polly and Molly built on the somatic nuclear transfer experiments that led to the cloning of Dolly the Sheep. The crucial difference was that in creating Polly and Molly, scientists used cells into which a new gene had been inserted. The gene chosen was a therapeutic protein to demonstrate the potential of such recombinant DNA technology combined with animal cloning. This could hopefully be used to produce pharmacological and therapeutic proteins to treat human diseases. The protein in question was the human blood clotting factor IX. Another difference from Dolly the Sheep was the source cell type of the nucleus that was transferred. Although Polly and Molly were nuclear clones, they had different mtDNA that was different from the nuclear cells where they received their DNA.Prior to the production of Polly and Molly, the only demonstrated way to make a transgenic animal was by microinjection of DNA into the pronuclei of fertilized oocytes (eggs). However, only a small proportion of the animals will integrate the injected DNA into their genome. In the rare cases that they do integrate this new genetic information, the pattern of expression of the injected transgene's protein due to the random integration is very variable. As the aim of such research is to produce an animal that expresses a particular protein in high levels in, for example, its milk, microinjection is a very costly procedure that does not usually produce the desired animal.
In mice, there is an additional option for genetic transfer that is not available in other animals. Embryonic stem cells provide a means to transfer new DNA into the germline. They also allow precise genetic modifications by gene targeting. Modified embryonic stem cells can be selected in vitro before the experiment moves on further for the production of an animal. Embryonic stem cells capable of contributing to the germline of livestock species such as sheep have not been isolated.
The production of Dolly the Sheep and also Megan and Morag, the two sheep that led to the production of Dolly, demonstrated that viable sheep can be produced by nuclear transfer from a variety of somatic cell types which have been cultured in vitro. Polly and Molly represented the further step in which somatic cells were cultured in vitro, just as in the case with the previous sheep. However, in this case they were transfected with foreign DNA, and the transfected cells which stably integrated this new piece of genetic information were selected. The nuclei of these somatic cells was then transferred into an empty oocyte, as in the procedure of nuclear transfer, and this was used to produce several transgenic animals. A cell type PDFF was used. PDFF5 would produce male animals and were not transduced. Cell type PDFF2 produced female animals and were transduced. Of the gestations that occurred, three PDFF2 animals were born, two of which survived birth, 7LL8 and 7LL12. These animals were transfected but contained a marker gene not the cloned gene of interest. These were named ""Holly"" and ""Olly"". Two more subsets of female-producing PDFF2 cells, PDFF2-12 and PDFF2-13, also produced animals which had the cell of interest together with the marker. Of these lambs, 7LL12, 7LL15, and 7LL13 were born alive and healthy. Two of these were named Polly and Molly.",1
Life Science,Polyclonal antibodies,"Polyclonal antibodies (pAbs) are antibodies that are secreted by different B cell lineages within the body (whereas monoclonal antibodies come from a single cell lineage). They are a collection of immunoglobulin molecules that react against a specific antigen, each identifying a different epitope.

",1
Life Science,Polymerase chain reaction,"Polymerase chain reaction (PCR) is a method widely used to rapidly make millions to billions of copies (complete or partial) of a specific DNA sample, allowing scientists to take a very small sample of DNA and amplify it (or a part of it) to a large enough amount to study in detail. PCR was invented in 1983 by the American biochemist Kary Mullis at Cetus Corporation; Mullis and biochemist Michael Smith, who had developed other essential ways of manipulating DNA, were jointly awarded the Nobel Prize in Chemistry in 1993.
PCR is fundamental to many of the procedures used in genetic testing and research, including analysis of ancient samples of DNA and identification of infectious agents. Using PCR, copies of very small amounts of DNA sequences are exponentially amplified in a series of cycles of temperature changes. PCR is now a common and often indispensable technique used in medical laboratory research for a broad variety of applications including biomedical research and criminal forensics.The majority of PCR methods rely on thermal cycling. Thermal cycling exposes reactants to repeated cycles of heating and cooling to permit different temperature-dependent reactions—specifically, DNA melting and enzyme-driven DNA replication. PCR employs two main reagents—primers (which are short single strand DNA fragments known as oligonucleotides that are a complementary sequence to the target DNA region) and a DNA polymerase. In the first step of PCR, the two strands of the DNA double helix are physically separated at a high temperature in a process called nucleic acid denaturation. In the second step, the temperature is lowered and the primers bind to the complementary sequences of DNA. The two DNA strands then become templates for DNA polymerase to enzymatically assemble a new DNA strand from free nucleotides, the building blocks of DNA. As PCR progresses, the DNA generated is itself used as a template for replication, setting in motion a chain reaction in which the original DNA template is exponentially amplified.
Almost all PCR applications employ a heat-stable DNA polymerase, such as Taq polymerase, an enzyme originally isolated from the thermophilic bacterium Thermus aquaticus. If the polymerase used was heat-susceptible, it would denature under the high temperatures of the denaturation step. Before the use of Taq polymerase, DNA polymerase had to be manually added every cycle, which was a tedious and costly process.Applications of the technique include DNA cloning for sequencing, gene cloning and manipulation, gene mutagenesis; construction of DNA-based phylogenies, or functional analysis of genes; diagnosis and monitoring of genetic disorders; amplification of ancient DNA; analysis of genetic fingerprints for DNA profiling (for example, in forensic science and parentage testing); and detection of pathogens in nucleic acid tests for the diagnosis of infectious diseases.",1
Life Science,Polyphosphate-accumulating organisms,"Polyphosphate-accumulating organisms (PAOs) are a group of bacteria that, under certain conditions, facilitate the removal of large amounts of phosphorus from wastewater in a process, called enhanced biological phosphorus removal (EBPR). PAOs accomplish this removal of phosphate by accumulating it within their cells as polyphosphate.
PAOs are by no means the only bacteria that can accumulate polyphosphate within their cells and in fact, the production of polyphosphate is a widespread ability among bacteria.  However, the PAOs have many characteristics that other organisms that accumulate polyphosphate do not have, that make them amenable to use in wastewater treatment.  Specifically, this is the ability to consume simple carbon compounds (energy source) without the presence of an external electron acceptor (such as nitrate or oxygen) by generating energy from internally stored polyphosphate and glycogen.  Most other bacteria cannot consume under these conditions and therefore PAOs gain a selective advantage within the mixed microbial community present in the activated sludge. Therefore, wastewater treatment plants that operate for enhanced biological phosphorus removal have an anaerobic tank (where there is no nitrate or oxygen present as external electron acceptor) prior to the other tanks to give PAOs preferential access to the simple carbon compounds in the wastewater that is influent to the plant.
A PAO related to the Betaproteobacteria has been identified and named Candidatus Accumulibacter Phosphatis.  Accumulibacter has been shown to remove phosphorus from EBPR plants in Australia, Europe and the USA. It can consume a range of carbon compounds, such as acetate and propionate, under anaerobic conditions and store these compounds as polyhydroxyalkanoates (PHA) which it consumes as a carbon and energy source for growth using oxygen or nitrate as electron acceptor.
Recently, another PAO related to the Actinobacteria has been identified in wastewater treatment plants. These organisms appear to be limited to certain amino acids as carbon and energy source.  The storage compound that they use to store the amino acids that these organisms take up under anaerobic conditions has not been identified. These bacteria have been observed in some EBPR plants in Denmark (where they were discovered) but their wider distribution is unknown.",1
Life Science,Prime editing,"Prime editing is a ‘search-and-replace’ genome editing technology in molecular biology by which the genome of living organisms may be modified. The technology directly writes new genetic information into a targeted DNA site. It uses a fusion protein, consisting of a catalytically impaired Cas9 endonuclease fused to an engineered reverse transcriptase enzyme, and a prime editing guide RNA (pegRNA), capable of identifying the target site and providing the new genetic information to replace the target DNA nucleotides. It mediates targeted insertions, deletions, and base-to-base conversions without the need for double strand breaks (DSBs) or donor DNA templates.The technology is an early-stage, experimental genome editing method that has received mainstream press attention due to its potential uses in medical genetics. It utilizes methodologies similar to precursor genome editing technologies, including CRISPR/Cas9 and base editors. Prime editing has been used on some animal models of genetic disease.

",1
Life Science,Primer dimer,"A primer dimer (PD) is a potential by-product in the polymerase chain reaction (PCR), a common biotechnological method. As its name implies, a PD consists of two primer molecules that have attached (hybridized) to each other because of strings of complementary bases in the primers. As a result, the DNA polymerase amplifies the PD, leading to competition for PCR reagents, thus potentially inhibiting amplification of the DNA sequence targeted for PCR amplification. In quantitative PCR, PDs may interfere with accurate quantification.

",1
Life Science,Prochymal,"Prochymal is a stem cell therapy made by Osiris Therapeutics. It is the first stem cell therapy approved by Canada. It is also the first therapy approved by Canada for acute graft-vs-host disease (GvHD). Also known as remestemcel-L, Prochymal was sold to Australia-based Mesoblast in 2013 at which time its brand name was changed to Ryoncil.
It is an allogeneic stem therapy based on mesenchymal stem cells (also medicinal signalling cells, mesenchymal stromal cells, and MSCs) derived from the bone marrow of adult donors. MSCs are purified from the marrow, cultured and packaged, with up to 10,000 doses derived from a single donor. The doses are stored frozen until needed.",1
Life Science,Progenitor cell,"A progenitor cell is a biological cell that can differentiate into a specific cell type. Stem cells and progenitor cells have this ability in common. However, stem cells are less specified than progenitor cells. Progenitor cells can only differentiate into their ""target"" cell type. The most important difference between stem cells and progenitor cells is that stem cells can replicate indefinitely, whereas progenitor cells can divide only a limited number of times.  Controversy about the exact definition remains and the concept is still evolving.The terms ""progenitor cell"" and ""stem cell"" are sometimes equated.",1
Life Science,Protein engineering,"Protein engineering is the process of developing useful or valuable proteins. It is a young discipline, with much research taking place into the understanding of protein folding and recognition for protein design principles.  It is also a product and services market, with an estimated value of $168 billion by 2017.There are two general strategies for protein engineering: rational protein design and directed evolution.  These methods are not mutually exclusive; researchers will often apply both.  In the future, more detailed knowledge of protein structure and function, and advances in high-throughput screening, may greatly expand the abilities of protein engineering.  Eventually, even unnatural amino acids may be included, via newer methods, such as expanded genetic code, that allow encoding novel amino acids in genetic code.",1
Life Science,Protein precipitation,"Protein precipitation is widely used in downstream processing of biological products in order to concentrate proteins and purify them from various contaminants. For example, in the biotechnology industry protein precipitation is used to eliminate contaminants commonly contained in blood. The underlying mechanism of precipitation is to alter the solvation potential of the solvent, more specifically, by lowering the solubility of the solute by addition of a reagent.

",1
Life Science,Protein production,"Protein production is the biotechnological process of generating a specific protein. It is typically achieved by the manipulation of gene expression in an organism such that it expresses large amounts of a recombinant gene. This includes the transcription of the recombinant DNA to messenger RNA (mRNA), the translation of mRNA into polypeptide chains, which are ultimately folded into functional proteins and may be targeted to specific subcellular or extracellular locations.Protein production systems (also known as expression systems) are used in the life sciences, biotechnology, and medicine. Molecular biology research uses numerous proteins and enzymes, many of which are from expression systems; particularly DNA polymerase for PCR, reverse transcriptase for RNA analysis, restriction endonucleases for cloning, and to make proteins that are screened in drug discovery as biological targets or as potential drugs themselves. There are also significant applications for expression systems in industrial fermentation, notably the production of biopharmaceuticals such as human insulin to treat diabetes, and to manufacture enzymes.

",1
Life Science,Protein purification,"Protein purification is a series of processes intended to isolate one or a few proteins from a complex mixture, usually cells, tissues or whole organisms.  Protein purification is vital for the specification  of the function, structure and interactions of the protein of interest. The purification process may separate the protein and non-protein parts of the mixture, and finally separate the desired protein from all other proteins. Separation of one protein from all others is typically the most laborious aspect of protein purification. Separation steps usually exploit differences in protein size, physico-chemical properties, binding affinity and biological activity.  The pure result may be termed protein isolate.

",1
Life Science,Protein–DNA interaction,"DNA-binding proteins are proteins that have DNA-binding domains and thus have a specific or general affinity for single- or double-stranded DNA. Sequence-specific DNA-binding proteins generally interact with the major groove of B-DNA, because it exposes more functional groups that identify a base pair. However, there are some known minor groove DNA-binding ligands such as netropsin, distamycin, Hoechst 33258, pentamidine, DAPI and others.",1
Life Science,Protein–protein interaction,"Protein–protein interactions (PPIs) are physical contacts of high specificity established between two or more protein molecules as a result of biochemical events steered by interactions that include electrostatic forces, hydrogen bonding and the hydrophobic effect. Many are physical contacts with molecular associations between chains that occur in a cell or in a living organism in a specific biomolecular context.
Proteins rarely act alone as their functions tend to be regulated. Many molecular processes within a cell are carried out by molecular machines that are built from numerous protein components organized by their PPIs. These physiological interactions make up the so-called interactomics of the organism, while aberrant PPIs are the basis of multiple aggregation-related diseases, such as Creutzfeldt–Jakob and Alzheimer's diseases.
PPIs have been studied with many methods and from different perspectives: biochemistry, quantum chemistry, molecular dynamics, signal transduction, among others. All this information enables the creation of large protein interaction networks – similar to metabolic or genetic/epigenetic networks – that empower the current knowledge on biochemical cascades and molecular etiology of disease, as well as the discovery of putative protein targets of therapeutic interest.",1
Life Science,Proteolysis targeting chimera,"A proteolysis targeting chimera (PROTAC) is a heterobifunctional small molecule composed of two active domains and a linker, capable of removing specific unwanted proteins. Rather than acting as a conventional enzyme inhibitor, a PROTAC works by inducing selective intracellular proteolysis. PROTACs consist of two covalently linked protein-binding molecules: one capable of engaging an E3 ubiquitin ligase, and another that binds to a target protein meant for degradation. Recruitment of the E3 ligase to the target protein results in ubiquitination and subsequent degradation of the target protein via the proteasome. Because PROTACs need only to bind their targets with high selectivity (rather than inhibit the target protein's enzymatic activity), there are currently many efforts to retool previously ineffective inhibitor molecules as PROTACs for next-generation drugs.Initially described by Kathleen Sakamoto, Craig Crews and Ray Deshaies in 2001, the PROTAC technology has been applied by a number of drug discovery labs using various E3 ligases, including pVHL, CRBN, Mdm2, beta-TrCP1, DCAF15, DCAF16, RNF114, and c-IAP1. Yale University licensed the PROTAC technology to Arvinas in 2013–14.In 2019, Arvinas put two PROTACs into clinical trials: ARV-110, an androgen receptor degrader, and ARV-471, an estrogen receptor degrader.",1
Life Science,PTC Therapeutics,"PTC Therapeutics is a US pharmaceutical company focused on the development of orally administered small molecule drugs and gene therapy which regulate gene expression by targeting post-transcriptional control (PTC) mechanisms in orphan diseases.In September 2009, PTC entered into an agreement with Roche for the development of orally bioavailable small molecules for central nervous system diseases. In 2020, PTC announced the FDA approval of Evrysdi™ (risdiplam) for the treatment of spinal muscular atrophy (SMA) in adults and children 2 months and older, in partnership with the SMA Foundation and Roche.
PTC acquired the Bio-e platform in 2019. The Bio-e platform utilizes expertise in electron-transfer chemistry to modulate key biological processes beyond the reach of current drug development approaches. The lead compounds from the Bio-e platform, PTC743 and PTC857, target the enzyme 15-lipoxygenase – a key enzymatic hub that regulates the inflammation and oxidative stress that underpin mitochondrial disease and CNS pathology. Two pivotal studies will investigate the safety and efficacy of PTC743: a Phase 2/3 trial in refractory mitochondrial epilepsy and a Phase 3 trial in Friedreich’s ataxia.

",1
Life Science,PubGene,"PubGene AS is a bioinformatics company located in Oslo, Norway and is the daughter company of PubGene Inc.
In 2001, PubGene founders demonstrated one of the first
applications of text mining to research in biomedicine (i.e., biomedical text mining). They went on to create the PubGene public search engine, exemplifying the approach they pioneered by presenting biomedical terms as graphical networks based on their co-occurrence in MEDLINE texts. The PubGene search engine has since been discontinued and incorporated into a commercial product. Co-occurrence networks provide a visual overview of possible relationships between terms and facilitate medical literature retrieval for relevant sets of articles implied by the network display. Commercial applications of the technology are available.Original development of PubGene technologies was undertaken in collaboration between the Norwegian Cancer Hospital (Radiumhospitalet) and the Norwegian University of Science and Technology. The work is supported by the Research Council of Norway and commercialization assisted by Innovation Norway.

",1
Life Science,Pulse vaccination strategy,"The pulse vaccination strategy is a method used to eradicate an epidemic by repeatedly vaccinating a group at risk, over a defined age range, until the spread of the pathogen has been stopped. It is most commonly used during measles and polio epidemics to quickly stop the spread and contain the outbreak.",1
Life Science,Purpureocillium lilacinum,"Purpureocillium lilacinum is a species of filamentous fungus in the family Ophiocordycipitaceae. It has been isolated from a wide range of habitats, including cultivated and uncultivated soils, forests, grassland, deserts, estuarine sediments and sewage sludge, and insects. It has also been found in nematode eggs, and occasionally from females of root-knot and cyst nematodes. In addition, it has frequently been detected in the rhizosphere of many crops. The species can grow at a wide range of temperatures – from 8 to 38 °C (46 to 100 °F) for a few isolates, with optimal growth in the range 26 to 30 °C (79 to 86 °F). It also has a wide pH tolerance and can grow on a variety of substrates. P. lilacinum has shown promising results for use as a biocontrol agent to control the growth of destructive root-knot nematodes.",1
Life Science,PVSRIPO,"PVSRIPO, or PVS-RIPO, is the name of a modified polio virus that has recently shown promise for treating cancer. It is the focus of clinical trials being conducted at Duke University.PVS-RIPO consists of a genetically modified nonpathogenic version of the oral poliovirus Sabin type 1. The internal ribosome entry site (IRES) on the poliovirus was replaced with the IRES from human rhinovirus type 2 (HRV2), to avoid neurovirulence. Once administered, the virus enters and begins replicating within cells that express CD155/Necl5, which is an onco-fetal cell adhesion molecule that is common across solid tumors.A website at Duke University describes many of properties of PVSRIPO, and historical background about using viruses to oppose cancer. According to that website,

The FDA approved clinical trials with PVS-RIPO in brain tumor patients recently. Since May 2012, five brain tumor patients have been treated. Remarkably, there have been no toxic side effects with PVS-RIPO whatsoever, even at the highest possible dose (10 billion infectious virus particles).

The potential value of PVSRIPO was the focus of a 2015 story on 60 Minutes.In May 2016, the US FDA granted it Breakthrough therapy designation for Glioblastoma.",1
Life Science,Pyrosequencing,"Pyrosequencing is a method of DNA sequencing (determining the order of nucleotides in DNA) based on the ""sequencing by synthesis"" principle, in which the sequencing is performed by detecting the nucleotide incorporated by a DNA polymerase. Pyrosequencing relies on light detection based on a chain reaction when pyrophosphate is released. Hence, the name pyrosequencing.
The principle of pyrosequencing was first described in 1993 by, Bertil Pettersson, Mathias Uhlen and Pål Nyren by combining the solid phase sequencing method using streptavidin coated magnetic beads with recombinant DNA polymerase lacking 3´to 5´exonuclease activity (proof-reading) and luminescence detection using the firefly luciferase enzyme.  A mixture of three enzymes (DNA polymerase, ATP sulfurylase and firefly luciferase) and a nucleotide (dNTP) are added to single stranded DNA to be sequenced and the incorporation of nucleotide is followed by measuring the light emitted. The intensity of the light determines if 0, 1 or more nucleotides have been incorporated, thus showing how many complementary nucleotides are present on the template strand. The nucleotide mixture is removed before the next nucleotide mixture is added. This process is repeated with each of the four nucleotides until the DNA sequence of the single stranded template is determined.
A second solution-based method for pyrosequencing was described in 1998 by Mostafa Ronaghi, Mathias Uhlen and Pål Nyren. In this alternative method, an additional enzyme apyrase is introduced to remove nucleotides that are not incorporated by the DNA polymerase. This enabled the enzyme mixture including the DNA polymerase, the luciferase and the apyrase to be added at the start and kept throughout the procedure, thus providing a simple set-up suitable for automation. An automated instrument based on this principle was introduced to the market the following year by the company Pyrosequencing.
A third microfluidic variant of the pyrosequencing method was described in 2005 by Jonathan Rothberg and co-workers at the company 454 Life Sciences. This alternative approach for pyrosequencing was based on the original principle of attaching the DNA to be sequenced to a solid support and they showed that sequencing could be performed in a highly parallel manner using a microfabricated microarray. This allowed for high-throughput DNA sequencing and an automated instrument was introduced to the market. This became the first next generation sequencing instrument starting a new era in genomics research, with rapidly falling prices for DNA sequencing allowing whole genome sequencing at affordable prices.

",1
Life Science,QMCF Technology,"QMCF Technology is an episomal protein production system that uses genetically modified mammalian cells and specially designed plasmids. QMCF plasmids carry a combination of regulatory sequences from mouse polyomavirus (Py) DNA replication origin which in combination with Epstein-Barr virus (EBV) EBNA-1 protein binding site as nuclear retention elements ensure stable propagation of plasmids in mammalian cells. In addition the vectors carry the selection marker operational for selection of plasmid carrying bacteria and QMCF cells, bacterial ColE1 origin of replication, and cassette for expression of protein of interest. QMCF cell lines express Large-T antigen and EBNA-1 proteins which bind the viral sequences on the QMCF plasmid and hence support plasmid replication and maintenance in the cells. QMCF Technology has several important differences compared to commonly known transient expression and stable cell line expression systems. Unlike in transient expression system, QMCF Technology enables to maintain episomally replicating QMCF plasmids inside the cells for up to 50 days thus providing an option for production phase of 2–3 weeks. Therefore, the production levels of QMCF Technology are higher (up to 1g/L). Another difference is the option of establishing expression cell banks within one week, which is not feasible with transient system. Compared to usage of stable cell line, QMCF technology is a rapid method leaving out time-consuming clone selection step during cell line development.",1
Life Science,R bodies,"R bodies (from refractile bodies, also R-bodies) are polymeric protein inclusions formed inside the cytoplasm of bacteria. Initially discovered in kappa particles, bacterial endosymbionts of the ciliate Paramecium, R bodies (and genes encoding them) have since been discovered in a variety of taxa.",1
Life Science,Rabbit hybridoma,A rabbit hybridoma is a hybrid cell line formed by the fusion of an antibody producing rabbit B cell with a cancerous B-cell (myeloma).,1
Life Science,Rapid antigen test,"A rapid antigen test (RAT), sometimes called a rapid antigen detection test (RADT), antigen rapid test (ART), or loosely just a rapid test, is a rapid diagnostic test suitable for point-of-care testing that directly detects the presence or absence of an antigen. Such tests are a type of lateral flow test that detect antigens, distinguishing them from other medical tests that detect antibodies (antibody tests) or nucleic acid (nucleic acid tests), of either laboratory or point-of-care types. Rapid tests generally give a result in 5 to 30 minutes, require minimal training or infrastructure, and have significant cost advantages. Rapid antigen tests for the detection of SARS-CoV-2, the virus that causes COVID-19, have been commonly used during the COVID-19 pandemic. 
For many years, an early and major class of RATs—the rapid strep tests for streptococci—were so often the referent when RATs or RADTs were mentioned that the two latter terms were often loosely treated as synonymous with those. Since the COVID-19 pandemic, awareness of RATs is no longer limited to health professionals and COVID-19 has become the expected referent, so more precise usage is required in other circumstances. 
RATs are based on the principle of antigen-antibody interaction. Antibodies directed against the target antigen (generally a protein on the surface of a virus) are fixed to the indicator line of the chromatography substrate and the visualisation marker (generally a dye marker, though sometimes these antibodies are modified to fluoresce). When the dissolved sample is added it mobilises the visualisation marker, which then travels through the chromatography substrate with the sample. Virus particles (which the antibodies linked to the visualisation marker have already bound to) are immobilised by the antibodies fixed to the indicator line as they travel through the substrate. This also immobilises the bound visualisation marker, allowing visual detection of significant levels of virus in a sample.
A positive result with an antigen test should generally be confirmed by RT-qPCR or some other test with higher sensitivity and specificity.",1
Life Science,Rare-cutter enzyme,"A rare-cutter enzyme is a restriction enzyme with a recognition sequence which occurs only rarely in a genome.  An example is NotI, which cuts after the first GC of a 5'-GCGGCCGC-3' sequence; restriction enzymes with seven and eight base pair recognition sequences are often also called rare-cutter enzymes (six bp recognition sequences are much more common).
For example, rare-cutter enzymes with 7-nucleotide recognition sites cut once every 47 bp (16,384 bp), and those with 8-nucleotide recognition sites cut every 48 bp (65,536 bp) respectively. They are used in top-down mapping to cut a chromosome into chunks of these sizes on average.",1
Life Science,Rational design,"In chemical biology and biomolecular engineering, rational design (RD) is an umbrella term which invites the strategy of creating new molecules with a certain functionality, based upon the ability to predict how the molecule's structure (specifically derived from motifs) will affect its behavior through physical models.  This can be done either from scratch or by making calculated variations on a known structure, and usually complements directed evolution.",1
Life Science,Recombinant DNA,"Recombinant DNA (rDNA) molecules are DNA molecules formed by laboratory methods of genetic recombination (such as molecular cloning) that bring together genetic material from multiple sources, creating sequences that would not otherwise be found in the genome.
Recombinant DNA is the general name for a piece of DNA that has been created by combining at least two fragments from  two different sources. Recombinant DNA is possible because DNA molecules from all organisms share the same chemical structure, and differ only in the nucleotide sequence within that identical overall structure. Recombinant DNA molecules are sometimes called chimeric DNA, because they can be made of material from two different species, like the mythical chimera. R-DNA technology uses palindromic sequences and leads to the production of sticky and blunt ends.
The DNA sequences used in the construction of recombinant DNA molecules can originate from any species. For example, plant DNA may be joined to bacterial DNA, or human DNA may be joined with fungal DNA. In addition, DNA sequences that do not occur anywhere in nature may be created by the chemical synthesis of DNA, and incorporated into recombinant molecules. Using recombinant DNA technology and synthetic DNA, literally any DNA sequence may be created and introduced into any of a very wide range of living organisms.
Proteins that can result from the expression of recombinant DNA within living cells are termed recombinant proteins. When recombinant DNA encoding a protein is introduced into a host organism, the recombinant protein is not necessarily produced. Expression of foreign proteins requires the use of specialized expression vectors and often necessitates significant restructuring by
foreign coding sequences.Recombinant DNA differs from genetic recombination in that the former results from artificial methods in the test tube, while the latter is a normal biological process that results in the remixing of existing DNA sequences in essentially all organisms.

",1
Life Science,Recombinase polymerase amplification,"Recombinase polymerase amplification (RPA) is a single tube, isothermal alternative to the polymerase chain reaction (PCR). By adding a reverse transcriptase enzyme to an RPA reaction it can detect RNA as well as DNA, without the need for a separate step to produce cDNA,. Because it is isothermal, RPA can use much simpler equipment than PCR, which requires a thermal cycler. Operating best at temperatures of 37–42 °C and still working, albeit more slowly, at room temperature means RPA reactions can in theory be run quickly simply by holding a tube. This makes RPA an excellent candidate for developing low-cost, rapid, point-of-care molecular tests. An international quality assessment of molecular detection of Rift Valley fever virus performed as well as the best RT-PCR tests, detecting less concentrated samples missed by some PCR tests and an RT-LAMP test.
RPA was developed and launched by TwistDx Ltd. (formerly known as ASM Scientific Ltd), a biotechnology company based in Cambridge, UK.",1
Life Science,Registry of Standard Biological Parts,"The Registry of Standard Biological Parts is a collection of genetic parts that are used in the assembly of systems and devices in synthetic biology. The registry was founded in 2003 at the Massachusetts Institute of Technology. The registry, as of 2018, contains over 20,000 parts. Recipients of the genetic parts include academic labs, established scientists, and student teams participating in the iGEM Foundation's annual synthetic biology competition.The Registry of Standard Biological Parts conforms to the BioBrick standard, a standard for interchangeable genetic parts. BioBrick was developed by a nonprofit composed of researchers from MIT, Harvard, and UCSF. The registry offers genetic parts with the expectation that recipients will contribute data and new parts to improve the resource. The registry records and indexes biological parts and offers services including the synthesis and assembly of biological parts, systems, and devices.
The registry offers many types of biological parts, including DNA, plasmids, plasmid backbones, primers, promoters, protein coding sequences, protein domains, ribosomal binding sites, terminators, translational units, riboregulators, and composite parts. It also includes devices such as protein generators, reporters, inverters, receptors, senders, and measurement devices. A key idea that motivated the development of the Registry was to develop an abstraction hierarchy implemented through the parts categorization system.The registry has previously received external funding through grants from the National Science Foundation, the Defense Advanced Research Projects Agency, and the National Institutes of Health.",1
Life Science,Respiratory inductance plethysmography,"Respiratory inductance plethysmography (RIP) is a method of evaluating pulmonary ventilation by measuring the movement of the chest and abdominal wall.
Accurate measurement of pulmonary ventilation or breathing often requires the use of devices such as masks or mouthpieces coupled to the airway opening. These devices are often both encumbering and invasive, and thus ill suited for continuous or ambulatory measurements. As an alternative RIP devices that sense respiratory excursions at the body surface can be used to measure pulmonary ventilation.
According to a paper by Konno and Mead   ""the chest can be looked upon as a system of two compartments with only one degree of freedom each"". Therefore, any volume change of the abdomen must be equal and opposite to that of the rib cage. The paper suggests that the volume change is close to being linearly related to changes in antero-posterior (front to back of body) diameter. When a known air volume is inhaled and measured with a spirometer, a volume-motion relationship can be established as the sum of the abdominal and rib cage displacements. Therefore, according to this theory, only changes in the antero-posterior diameter of the abdomen and the rib cage are needed to estimate changes in lung volume.
Several sensor methodologies based on this theory have been developed. RIP is the most frequently used, established and accurate plethysmography method to estimate lung volume from respiratory movements.
RIP has been used in many clinical and academic research studies in a variety of domains including polysomnographic (sleep), psychophysiology, psychiatric research, anxiety and stress research, anesthesia, cardiology and pulmonary research (asthma, COPD, dyspnea).

",1
Life Science,Restriction enzyme,"A restriction enzyme, restriction endonuclease, or  restrictase  is an enzyme that cleaves DNA into fragments at or near specific recognition sites within molecules known as restriction sites.   Restriction enzymes are one class of the broader endonuclease group of enzymes. Restriction enzymes are commonly classified into five types, which differ in their structure and whether they cut their DNA substrate at their recognition site, or if the recognition and cleavage sites are separate from one another. To cut DNA, all restriction enzymes make two incisions, once through each sugar-phosphate backbone (i.e. each strand) of the DNA double helix.
These enzymes are found in bacteria and archaea and provide a defense mechanism against invading viruses. Inside a prokaryote, the restriction enzymes selectively cut up foreign DNA in a process called restriction digestion; meanwhile, host DNA is protected by a modification enzyme (a methyltransferase) that modifies the prokaryotic DNA and blocks cleavage. Together, these two processes form the restriction modification system.More than 3,600 restriction endonucleases are known which represent over 250 different specificities. Over 3,000 of these have been studied in detail, and more than 800 of these are available commercially. These enzymes are routinely used for DNA modification in laboratories, and they are a vital tool in molecular cloning.",1
Life Science,Reverse transcription polymerase chain reaction,"Reverse transcription polymerase chain reaction (RT-PCR) is a laboratory technique combining reverse transcription of RNA into DNA (in this context called complementary DNA or cDNA) and amplification of specific DNA targets using polymerase chain reaction (PCR). It is primarily used to measure the amount of a specific RNA. This is achieved by monitoring the amplification reaction using fluorescence, a technique called real-time PCR or quantitative PCR (qPCR). Combined RT-PCR and qPCR are routinely used for analysis of gene expression and quantification of viral RNA in research and clinical settings.
The close association between RT-PCR and qPCR has led to metonymic use of the term qPCR to mean RT-PCR. Such use may be confusing, as RT-PCR can be used without qPCR, for example to enable molecular cloning, sequencing or simple detection of RNA. Conversely, qPCR may be used without RT-PCR, for example to quantify the copy number of a specific piece of DNA.",1
Life Science,Ribosomal intergenic spacer analysis,"Ribosomal RNA (rRNA) intergenic spacer analysis (RISA) is a method of microbial community analysis that provides a means of comparing differing environments or treatment impacts without the bias imposed by culture- dependent approaches. This type of analysis is often referred to as community fingerprinting. RISA involves PCR amplification of a region of the rRNA gene operon between the small (16S) and large (23S) subunits called the intergenic spacer region ISR.By using oligonucleotide primers targeted to conserved regions in the 16S and 23S genes, RISA fragments can be generated from most of the dominant bacteria in an environmental sample. While the majority of the rRNA operon serves a structural function, portions of the 16S-23S intergenic region can encode tRNAs depending on the bacterial species. However the taxonomic value of the ISR lies in the significant heterogeneity in both length and nucleotide sequence. In RISA, we attempt to exploit the length heterogeneity of the ISR, which has been shown to range between 150 and 1500 bp with the majority of the ISR lengths being between 150 and 500 bp.The resulting PCR product will be a mixture of fragments contributed by several dominant community members. This product is electrophoresed in a polyacrylamide gel, and the DNA is visualized following staining. The result is a complex banding pattern that provides a community-specific profile, with each DNA band corresponding to a bacterial population on the original assemblage.",1
Life Science,RNA,"Deoxyribonucleic acid ( (listen); DNA) is a polymer composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organisms and many viruses. DNA and ribonucleic acid (RNA) are nucleic acids. Alongside proteins, lipids and complex carbohydrates (polysaccharides), nucleic acids are one of the four major types of macromolecules that are essential for all known forms of life.
The two DNA strands are known as polynucleotides as they are composed of simpler monomeric units called nucleotides. Each nucleotide is composed of one of four nitrogen-containing nucleobases (cytosine [C], guanine [G], adenine [A] or thymine [T]), a sugar called deoxyribose, and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds (known as the phospho-diester linkage) between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. The nitrogenous bases of the two separate polynucleotide strands are bound together, according to base pairing rules (A with T and C with G), with hydrogen bonds to make double-stranded DNA. The complementary nitrogenous bases are divided into two groups, pyrimidines and purines. In DNA, the pyrimidines are thymine and cytosine; the purines are adenine and guanine.
Both strands of double-stranded DNA store the same biological information. This information is replicated when the two strands separate. A large part of DNA (more than 98% for humans) is non-coding, meaning that these sections do not serve as patterns for protein sequences. The two strands of DNA run in opposite directions to each other and are thus antiparallel. Attached to each sugar is one of four types of nucleobases (or bases). It is the sequence of these four nucleobases along the backbone that encodes genetic information. RNA strands are created using DNA strands as a template in a process called transcription, where DNA bases are exchanged for their corresponding bases except in the case of thymine (T), for which RNA substitutes uracil (U). Under the genetic code, these RNA strands specify the sequence of amino acids within proteins in a process called translation.
Within eukaryotic cells, DNA is organized into long structures called chromosomes. Before typical cell division, these chromosomes are duplicated in the process of DNA replication, providing a complete set of chromosomes for each daughter cell. Eukaryotic organisms (animals, plants, fungi and protists) store most of their DNA inside the cell nucleus as nuclear DNA, and some in the mitochondria as mitochondrial DNA or in chloroplasts as chloroplast DNA. In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm, in circular chromosomes. Within eukaryotic chromosomes, chromatin proteins, such as histones, compact and organize DNA. These compacting structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.",1
Life Science,RNA therapeutics,"RNA therapeutics are a new class of medications based on ribonucleic acid (RNA). Research has been working on clinical use since the 1990s, with significant success in cancer therapy in the early 2010s. In 2020 and 2021, mRNA vaccines have been developed globally for use in combating the coronavirus disease (COVID-19 pandemic). The Pfizer–BioNTech COVID-19 vaccine was the first mRNA vaccine approved by a medicines regulator, followed by the Moderna COVID-19 vaccine, and others.
The main types of RNA therapeutics are those based on messenger RNA (mRNA), antisense RNA (asRNA), RNA interference (RNAi), and RNA aptamers. Of the four types, mRNA-based therapy is the only type which is based on triggering synthesis of proteins within cells, making it particularly useful in vaccine development. Antisense RNA is complementary to coding mRNA and is used to trigger mRNA inactivation to prevent the mRNA from being used in protein translation. RNAi-based systems use a similar mechanism, and involve the use of both small interfering RNA (siRNA) and micro RNA (miRNA) to prevent mRNA translation. However, RNA aptamers are short, single stranded RNA molecules produced by directed evolution to bind to a variety of biomolecular targets with high affinity thereby affecting their normal in vivo activity.RNA is synthesized from template DNA by RNA polymerase with messenger RNA (mRNA) serving as the intermediary biomolecule between DNA expression and protein translation. Because of its unique properties (such as its typically single-stranded nature and its 2' OH group) as well as its ability to adopt many different secondary/tertiary structures, both coding and noncoding RNAs have attracted special attention in medicine. Research has begun to explore RNAs potential to be used for therapeutic benefit, and unique challenges have occurred during drug discovery and implementation of RNA therapeutics.",1
Life Science,RNase H-dependent PCR,"RNase H-dependent PCR (rhPCR) is a modification of the standard PCR technique.  In rhPCR, the primers are designed with a removable amplification block on the 3’ end.  Amplification of the blocked primer is dependent on the cleavage activity of a hyperthermophilic archaeal Type II RNase H enzyme during hybridization to the complementary target sequence.  This RNase H enzyme possesses several useful characteristics that enhance the PCR.  First, it has very little enzymatic activity at low temperature, enabling a “hot start PCR” without modifications to the DNA polymerase.    Second, the cleavage efficiency of the enzyme is reduced in the presence of mismatches near the RNA residue.  This allows for reduced primer dimer formation, detection of alternative splicing variants, ability to perform multiplex PCR with higher numbers of PCR primers, and the ability to detect single-nucleotide polymorphisms.",1
Life Science,SCTbio,"SOTIO Biotech is a Czech biotechnology company focused on clinical-stage research and development of innovative medicines for cancer with operations in Europe, North America, and Asia. The company has clinical programs which include a superagonist of the immuno-oncology target IL-15, a new generation of potent and stable antibody-drug conjugates (ADCs), proprietary technology designed to improve on the efficacy of CAR T therapies and a platform to streamline and enhance personalized cell therapies.

",1
Life Science,Seaport Centre,"Seaport Centre is a high-tech business park located in Redwood City, California, United States, and as of 2007 is one of the largest biotechnology research complexes in the San Francisco Bay Area.
The property consists of 623,000 square feet (57,900 m2) of developed building area, and is situated in proximity to the Port of Redwood City. The property is classified as Class A office space and is constructed as a series of separate buildings.  The original lands of the Seaport Centre were used as salt evaporation ponds on tidal lands of the San Francisco Bay, a land use that started sometime prior to 1940. As of 2002 leasing rates at Seaport Centre were in the range of $27 per square foot  per annum.  In 2005, Slough Estates, a United Kingdom-based REIT, purchased the entirety of Seaport Centre to develop it as a biotechnology research center to compete with the existing biotech hubs in Silicon Valley and South San Francisco.
The Seaport Centre is located on generally level ground at approximately 25 feet (8 m) above mean sea level.  Stormwater surface runoff is pumped from Seaport Centre to discharge into Redwood Creek.

",1
Life Science,Selection and amplification binding assay,Selection and amplification binding assay (SAAB) is a molecular biology technique typically used to find the DNA binding site for proteins. It was developed by T. Keith Blackwell and Harold M. Weintraub in 1990.,1
Life Science,Selective breeding,"Selective breeding (also called artificial selection) is the process by which humans use animal breeding and plant breeding to selectively develop particular phenotypic traits (characteristics) by choosing which typically animal or plant males and females will sexually reproduce and have offspring together. Domesticated animals are known as breeds, normally bred by a professional breeder, while domesticated plants are known as varieties, cultigens, cultivars, or breeds. Two purebred animals of different breeds produce a crossbreed, and crossbred plants are called hybrids. Flowers, vegetables and fruit-trees may be bred by amateurs and commercial or non-commercial professionals: major crops are usually the provenance of the professionals.
In animal breeding, techniques such as inbreeding, linebreeding, and outcrossing are utilized. In plant breeding, similar methods are used. Charles Darwin discussed how selective breeding had been successful in producing change over time in his 1859 book, On the Origin of Species. Its first chapter discusses selective breeding and domestication of such animals as pigeons, cats, cattle, and dogs. Darwin used artificial selection as a springboard to introduce and support the theory of natural selection.The deliberate exploitation of selective breeding to produce desired results has become very common in agriculture and experimental biology.
Selective breeding can be unintentional, e.g., resulting from the process of human cultivation; and it may also produce unintended – desirable or undesirable – results. For example, in some grains, an increase in seed size may have resulted from certain ploughing practices rather than from the intentional selection of larger seeds. Most likely, there has been an interdependence between natural and artificial factors that have resulted in plant domestication.",1
Life Science,SequenceBase,"SequenceBase is a privately held company, is an international patent sequence information provider with headquarters located in Edison, NJ, USA.
SequenceBase develops and markets the SequenceBase Research Portal to the biotechnology, legal, pharmaceutical, scientific, technical and academic bioinformatics communities.
Clarivate Analytics has acquired SequenceBase on 9th September 2019.USGENE provides searchable access to all available peptide and nucleotide sequences from the published applications and issued patents of the United States Patent and Trademark Office (USPTO). USGENE can be searched directly via the SequenceBase Research Portal  or via STN International by FIZ Karlsruhe. The SequenceBase Research Portal offers BLAST+ as a sequence searching method.

",1
Life Science,Single cell sequencing,"Single cell sequencing examines the sequence information from individual cells with optimized next-generation sequencing (NGS) technologies, providing a higher resolution of cellular differences and a better understanding of the function of an individual cell in the context of its microenvironment. For example, in cancer, sequencing the DNA of individual cells can give information about mutations carried by small populations of cells. In development, sequencing the RNAs expressed by individual cells can give insight into the existence and behavior of different cell types. In microbial systems, a population of the same species can appear to be genetically clonal, but single-cell sequencing of RNA or epigenetic modifications can reveal cell-to-cell variability that may help populations rapidly adapt to survive in changing environments.",1
Life Science,Single molecule fluorescent sequencing,"The Helicos Genetic Analysis System platform was the first commercial NGS (Next Generation Sequencing) implementation to use the principle of single molecule fluorescent sequencing, a method of identifying the exact sequence of a piece of DNA. It was marketed by the now defunct Helicos Biosciences.
The fragments of DNA molecules are first hybridized in place on disposable glass flow cells. Fluorescent nucleotides are then added one-by-one, with a terminating nucleotide used to pause the process until an image has been captured. From the image, one nucleotide from each DNA sequence can be determined. The fluorescent molecule is then cut away, and the process is repeated until the fragments have been completely sequenced.This sequencing method and equipment were used to sequence the genome of the M13 bacteriophage.",1
Life Science,Single-cell protein,"Single-cell proteins (SCP) or microbial proteins refer to edible unicellular microorganisms. The biomass or protein extract from pure or mixed cultures of algae, yeasts, fungi or bacteria may be used as an ingredient or a substitute for protein-rich foods, and is suitable for human consumption or as animal feeds. Industrial agriculture is marked by a high water footprint, high land use, biodiversity destruction, general environmental degradation and contributes to climate change by emission of a third of all greenhouse gases, production of SCP does not necessarily exhibit any of these serious drawbacks. As of today, SCP is commonly grown on agricultural waste products, and as such inherits the ecological footprint and water footprint of industrial agriculture. However, SCP may also be produced entirely independent of agricultural waste products through autotrophic growth. Thanks to the high diversity of microbial metabolism, autotrophic SCP provides several different modes of growth, versatile options of nutrients recycling, and a substantially increased efficiency compared to crops. A 2021 publication showed that photovoltaic-driven microbial protein production could use 10 times less land for an equivalent amount of protein compared to soybean cultivation.With the world population reaching 9 billion by 2050, there is strong evidence that agriculture will not be able to meet demand and that there is serious risk of food shortage. Autotrophic SCP represents options of fail-safe mass food-production which can produce food reliably even under harsh climate conditions.",1
Life Science,Single-cell transcriptomics,"Single-cell transcriptomics examines the gene expression level of individual cells in a given population by simultaneously measuring the messenger RNA (mRNA) concentration of hundreds to thousands of genes.
The unraveling of heterogenous cell populations, reconstruction of cellular developmental trajectories, and modeling of transcriptional dynamics — all previously masked in bulk transcriptome measurements — are made possible through analysis of this transcriptomic data.

",1
Life Science,Single-nucleotide polymorphism,"In genetics, a single-nucleotide polymorphism (SNP ; plural SNPs ) is a germline substitution of a single nucleotide at a specific position in the genome. Although certain definitions require the substitution to be present in a sufficiently large fraction of the population (e.g. 1% or more), many publications do not apply such a frequency threshold.
For example, at a specific base position in the human genome, the G nucleotide may appear in most individuals, but in a minority of individuals, the position is occupied by an A. This means that there is a SNP at this specific position, and the two possible nucleotide variations – G or A – are said to be the alleles for this specific position.SNPs pinpoint differences in our susceptibility to a wide range of diseases, for example age-related macular degeneration (a common SNP in the CFH gene is associated with increased risk of the disease) or nonalcoholic fatty liver disease (a SNP in the PNPLA3 gene is associated with increased risk of the disease). The severity of illness and the way the body responds to treatments are also manifestations of genetic variations caused by SNPs. For example, the APOE E4 allele that is determined by two common SNPs, rs429358 and rs7412, in the APOE gene is not only associated with increased risk for Alzheimer’s disease but also younger age at onset of the disease.A single-nucleotide variant (SNV) is a general term for single nucleotide change in DNA sequence. So a SNV can be a common SNP or a rare mutation, and can be germline or somatic and can be caused by cancer, but a SNP has to segregate in a species' population of organisms. SNVs also commonly arise in molecular diagnostics such as designing PCR primers to detect viruses, in which the viral RNA or DNA sample may contain SNVs.",1
Life Science,Site-specific recombination,"Site-specific recombination, also known as conservative site-specific recombination, is a type of genetic recombination in which DNA strand exchange takes place between segments possessing at least a certain degree of sequence homology. Enzymes known as site-specific recombinases (SSRs) perform rearrangements of DNA segments by recognizing and binding to short, specific DNA sequences (sites), at which they cleave the DNA backbone, exchange the two DNA helices involved, and rejoin the DNA strands. In some cases the presence of a recombinase enzyme and the recombination sites is sufficient for the reaction to proceed; in other systems a number of accessory proteins and/or accessory sites are required. Many different genome modification strategies, among these recombinase-mediated cassette exchange (RMCE), an advanced approach for the targeted introduction of transcription units into predetermined genomic loci, rely on SSRs.
Site-specific recombination systems are highly specific, fast, and efficient, even when faced with complex eukaryotic genomes. They are employed naturally in a variety of cellular processes, including bacterial genome replication, differentiation and pathogenesis, and movement of mobile genetic elements. For the same reasons, they present a potential basis for the development of genetic engineering tools.Recombination sites are typically between 30 and 200 nucleotides in length and consist of two motifs with a partial inverted-repeat symmetry, to which the recombinase binds, and which flank a central crossover sequence at which the recombination takes place. The pairs of sites between which the recombination occurs are usually identical, but there are exceptions (e.g. attP and attB of λ integrase).",1
Life Science,Smart ligand,"Smart ligands are affinity ligands selected with pre-defined equilibrium (
  
    
      
        
          K
          
            d
          
        
      
    
    {\displaystyle K_{d}}
  ), kinetic (
  
    
      
        
          k
          
            o
            f
            f
          
        
      
    
    {\displaystyle k_{off}}
  , 
  
    
      
        
          k
          
            o
            n
          
        
      
    
    {\displaystyle k_{on}}
  ) and thermodynamic (ΔH, ΔS) parameters of biomolecular interaction.
Ligands with desired parameters can be selected from large combinatorial libraries of biopolymers using instrumental separation techniques with well-described kinetic behaviour, such as kinetic capillary electrophoresis (KCE), surface plasmon resonance (SPR), microscale thermophoresis (MST), etc. Known examples of smart ligands include DNA smart aptamers; however, RNA and peptide smart aptamers can also be developed.
Smart ligands can find a set of unique applications in biomedical research, drug discovery and proteomic studies. For example, a panel of DNA smart aptamers has been recently used to develop affinity analysis of proteins with ultra-wide dynamic range of measured concentrations.",1
Life Science,SMiLE-Seq,"Selective microfluidics-based ligand enrichment followed by sequencing (SMiLE-seq) is a technique developed for the rapid identification of DNA binding specificities and affinities of full length monomeric and dimeric transcription factors in a fast and semi-high-throughput fashion. 
SMiLE-seq works by loading in vitro transcribed and translated “bait” transcription factors into a microfluidic device in combination with DNA molecules. Bound transcription factor-DNA complexes are then isolated from the device, which is followed by sequencing and sequence data analysis to characterize binding motifs. Specialized software is used to determine the DNA binding properties of monomeric or dimeric transcription factors to help predict their in vivo DNA binding activity. 
SMiLE-seq combines three critical functions that makes it unique from existing techniques: (1) the use of capillary pumps to optimize the loading of samples, (2) trapping molecular interactions on the surface of the microfluidic device through immunocapture of target transcription factors, (3) enabling the selection of DNA that is specifically bound to transcription factors from a pool of random DNA sequences.",1
Life Science,SNP genotyping,"SNP genotyping is the measurement of genetic variations of single nucleotide polymorphisms (SNPs) between members of a species. It is a form of genotyping, which is the measurement of more general genetic variation. SNPs are one of the most common types of genetic variation. A SNP is a single base pair mutation at a specific locus, usually consisting of two alleles (where the rare allele frequency is > 1%). SNPs are found to be involved in the etiology of many human diseases and are becoming of particular interest in pharmacogenetics. Because SNPs are conserved during evolution, they have been proposed as markers for use in quantitative trait loci (QTL) analysis and in association studies in place of microsatellites. The use of SNPs is being extended in the HapMap project, which aims to provide the minimal set of SNPs needed to genotype the human genome. SNPs can also provide a genetic fingerprint for use in identity testing.  The increase of interest in SNPs has been reflected by the furious development of a diverse range of SNP genotyping methods.",1
Life Science,SNPlex,"SNPlex is a platform for SNP genotyping sold by Applied Biosystems (ABI).  It is based on capillary electrophoresis to separate varying fragments of DNA, which allows the assay to be performed on ABI's 3730xl DNA analyzers.  Currently, up to 48 SNPs can be genotyped in a single reaction.",1
Life Science,Solid phase sequencing,"The principle of solid phase DNA sequencing was described in 1989 based on binding of biotinylated DNA to streptavidin-coated magnetic beads and elution of single DNA strands selectively using alkali. The method allowed robotic applications suitable for clinical sequencing, but the magnetic handling has also found frequent use in many molecular applications, including sample handling for DNA diagnostics. The use of solid phase methods for DNA handling is now frequently used as an integrated part of many of the next generation DNA sequencing methods, as well as numerous molecular diagnostics applications.",1
Life Science,Somatic embryogenesis,"Somatic embryogenesis is an artificial process in which a plant or embryo is derived from a single somatic cell. Somatic embryos are formed from plant cells that are not normally involved in the development of embryos, i.e. ordinary plant tissue. No endosperm or seed coat is formed around a somatic embryo.
Cells derived from competent source tissue are cultured to form an undifferentiated mass of cells called a callus. Plant growth regulators in the tissue culture medium can be manipulated to induce callus formation and subsequently changed to induce embryos to form the callus. The ratio of different plant growth regulators required to induce callus or embryo formation varies with the type of plant. Somatic embryos are mainly produced in vitro and for laboratory purposes, using either solid or liquid nutrient media which contain plant growth regulators (PGR’s). The main PGRs used are auxins but can contain cytokinin in a smaller amount. Shoots and roots are monopolar while somatic embryos are bipolar, allowing them to form a whole plant without culturing on multiple media types. Somatic embryogenesis has served as a model to understand the physiological and biochemical events that occur during plant developmental processes as well as a component to biotechnological advancement. The first documentation of somatic embryogenesis was by Steward et al. in 1958 and Reinert in 1959 with carrot cell suspension cultures.",1
Life Science,Somatic fusion,"Somatic fusion, also called protoplast fusion, is a type of genetic modification in plants by which two distinct species of plants are fused together to form a new hybrid plant with the characteristics of both, a somatic hybrid. Hybrids have been produced either between different varieties of the same species (e.g. between non-flowering potato plants and flowering potato plants) or between two different species (e.g. between wheat Triticum and rye Secale to produce Triticale).
Uses of somatic fusion include making potato plants resistant to potato leaf roll disease. Through somatic fusion, the crop potato plant Solanum tuberosum – the yield of which is severely reduced by a viral disease transmitted on by the aphid vector – is fused with the wild, non-tuber-bearing potato Solanum brevidens, which is resistant to the disease. The resulting hybrid has the chromosomes of both plants and is thus similar to polyploid plants.
Somatic hybridization was first introduced by Carlson et al. in Nicotiana glauca.

",1
Life Science,Sonoporation,"Sonoporation, or cellular sonication, is the use of sound (typically ultrasonic frequencies) for modifying the permeability of the cell plasma membrane. This technique is usually used in molecular biology and non-viral gene therapy in order to allow uptake of large molecules such as DNA into the cell, in a cell disruption process called transfection or transformation.  Sonoporation employs the acoustic cavitation of microbubbles to enhance delivery of these large molecules. The exact mechanism of sonoporation-mediated membrane translocation remains unclear, with a few different hypotheses currently being explored. 
Sonoporation is under active study for the introduction of foreign genes in tissue culture cells, especially mammalian cells.  Sonoporation is also being studied for use in targeted Gene therapy in vivo, in a medical treatment scenario whereby a patient is given modified DNA, and an ultrasonic transducer might target this modified DNA into specific regions of the patient's body. The bioactivity of this technique is similar to, and in some cases found superior to, electroporation. Extended exposure to low-frequency (<MHz) ultrasound has been demonstrated to result in complete cellular death (rupturing), thus cellular viability must also be accounted for when employing this technique.",1
Life Science,Sotio,"SOTIO Biotech is a Czech biotechnology company focused on clinical-stage research and development of innovative medicines for cancer with operations in Europe, North America, and Asia. The company has clinical programs which include a superagonist of the immuno-oncology target IL-15, a new generation of potent and stable antibody-drug conjugates (ADCs), proprietary technology designed to improve on the efficacy of CAR T therapies and a platform to streamline and enhance personalized cell therapies.

",1
Life Science,Spatial transcriptomics,"Spatial transcriptomics is an overarching term for a range of methods designed for assigning cell types (identified by the mRNA readouts) to their locations in the histological sections. This method can also be used to determine subcellular localization of mRNA molecules. The term is a variation of Spatial Genomics, first described by Doyle, et al., in 2000  and then expanded upon by Ståhl et al. in a technique developed in 2016, which has since undergone a variety of improvements and modifications.The Ståhl method implies positioning individual tissue samples on the arrays of spatially barcoded reverse transcription primers able to capture mRNA with the oligo(dT) tails. Besides oligo(dT) tail and spatial barcode, which indicates the x and y position on the arrayed slide, the probe contains a cleavage site, amplification and sequencing handle, and unique molecular identifier. Commonly, histological samples are cut using cryotome, then fixed, stained, and put on the microarrays. After that, it undergoes enzymatic permeabilization, so that molecules can diffuse down to the slide, with further mRNA release and binding to the probes. Reverse transcription is then carried out in situ. As a result, spatially marked complementary DNA is synthesized, providing information about gene expression in the exact location of the sample. Thus, described protocol combines paralleled sequencing and staining of the same sample. It is important to mention that the first generation of the arrayed slides comprised about 1,000 spots of the 100-μm diameter, limiting resolution to ~10-40 cells per spot.In the broader meaning of this term, spatial transcriptomics includes methods that can be divided into five principal approaches to resolving spatial distribution of transcripts. They are microdissection techniques, Fluorescent in situ hybridization methods, in situ sequencing, in situ capture protocols and in silico approaches.Application
Defining the spatial distribution of mRNA molecules allows for the experimentalist to uncover cellular heterogeneity in tissues, tumours, immune cells as well as determine the subcellular distribution of transcripts in various conditions. This information provides a unique opportunity to decipher both the cellular and subcellular architecture in both tissues and individual cells. These methodologies provide crucial insights in the fields of embryology, oncology, immunology and histology. The functioning of the individual cells in multicellular organisms can only be completely explained in the context of identifying their exact location in the body. Spatial transcriptomics techniques sought to elucidate cells’ properties this way. Below, we look into the methods that connect gene expression to the spatial organization of  cells.",1
Life Science,Specialty drugs in the United States,"Specialty drugs or specialty pharmaceuticals are a recent designation of pharmaceuticals that are classified as high-cost, high complexity and/or high touch. Specialty drugs are often biologics—""drugs derived from living cells"" that are injectable or infused (although some are oral medications). They are used to treat complex or rare chronic conditions such as cancer, rheumatoid arthritis, hemophilia, H.I.V. psoriasis, inflammatory bowel disease and hepatitis C. In 1990 there were 10 specialty drugs on the market, in the mid-1990s there were fewer than 30, by 2008 there were 200, and by 2015 there were 300. Drugs are often defined as specialty because their price is much higher than that of non-specialty drugs. Medicare defines any drug for which the negotiated price is $670 per month or more, as a specialty drug which is placed in a specialty tier that requires a higher patient cost sharing. Drugs are also identified as specialty when there is a special handling requirement or the drug is only available via a limited distributions network. By 2015 ""specialty medications accounted for one-third of all spending on drugs in the United States, up from 19 percent in 2004 and heading toward 50 percent in the next 10 years"", according to IMS Health, which tracks prescriptions. According to a 2010 article in Forbes, specialty drugs for rare diseases became more expensive ""than anyone imagined"" and their success came ""at a time when the traditional drug business of selling medicines to the masses"" was ""in decline"". In 2015 analysis by The Wall Street Journal suggested the large premium was due to the perceived value of rare disease treatments which usually are very expensive when compared to treatments for more common diseases.",1
Life Science,Split-intein circular ligation of peptides and proteins,"Split-intein circular ligation of peptides and proteins (SICLOPPS) is a biotechnology technique that permits the creation of cyclic peptides.  These peptides are produced by ribosomal protein synthesis, followed by an intein-like event that splices the protein into a loop.  By contrast with the nonribosomal peptide synthetases that produces some cyclic peptides like gramicidin S, SICLOPPS offers the advantage that the peptides' structure can be encoded by DNA in a simple manner according to the genetic code, but for this reason it imposes limitations on the types of amino acids incorporated that are comparable to those that apply to ordinary proteins.  As implemented there is also some constraint on the peptide sequence of the cyclic sequence; for example, libraries may use the sequence SGXX..XXPL to increase the efficiency of circularization of the peptide.  SICLOPPS is frequently used with a library of randomized DNA sequence that permits the simultaneous production and screening of large numbers of constructs at once, followed by the recovery of the DNA sequences responsible for the activity of the clone of interest.
A number of natural antimicrobial peptides are cyclic, and the products of SICLOPPS are ""increasingly viewed as ideal backbones for modulation of protein-protein interactions.""  Circular peptides tend to be resistant to protease activity, and may be suitable for use as orally administered drugs.  Once a cyclic peptide is identified with a biological activity of interest, it may also be possible to identify the target of the peptide (a gene that encodes a protein with which it interacts) by functional complementation, facilitating a better understanding of its mechanism of action.",1
Life Science,Squalene,"Squalene is an organic compound. With the formula (C5H8)6, it is a triterpene. It is a colourless oil although impure samples appear yellow. It was originally obtained from shark liver oil (hence its name, as Squalus is a genus of sharks). All plants and animals produce squalene as a biochemical intermediate. An estimated 12% of bodily squalene in humans comes from the sebum. Squalene has a role in topical skin lubrication and protection.Squalene is a precursor for synthesis of all plant and animal sterols, including cholesterol and steroid hormones in the human body.Squalene is an important ingredient in some vaccine adjuvants: Novartis produces a substance they call MF59, while GlaxoSmithKline produces AS03.",1
Life Science,Stable isotope labeling by amino acids in cell culture,"Stable Isotope Labeling by/with Amino acids in Cell culture (SILAC) is a technique based on mass spectrometry that detects differences in protein abundance among samples using non-radioactive isotopic labeling. It is a popular method for quantitative proteomics.

",1
Life Science,Stem cell,"In multicellular organisms, stem cells are undifferentiated or partially differentiated cells that can differentiate into various types of cells and proliferate indefinitely to produce more of the same stem cell. They are the earliest type of cell in a cell lineage. They are found in both embryonic and adult organisms, but they have slightly different properties in each. They are usually distinguished from progenitor cells, which cannot divide indefinitely, and precursor or blast cells, which are usually committed to differentiating into one cell type.
In mammals, roughly 50–150 cells make up the inner cell mass during the blastocyst stage of embryonic development, around days 5–14. These have stem-cell capability. In vivo, they eventually differentiate into all of the body's cell types (making them pluripotent). This process starts with the differentiation into the three germ layers – the ectoderm, mesoderm and endoderm – at the gastrulation stage. However, when they are isolated and cultured in vitro, they can be kept in the stem-cell stage and are known as embryonic stem cells (ESCs).
Adult stem cells are found in a few select locations in the body, known as niches, such as those in the bone marrow or gonads. They exist to replenish rapidly lost cell types and are multipotent or unipotent, meaning they only differentiate into a few cell types or one type of cell. In mammals, they include, among others, hematopoietic stem cells, which replenish blood and immune cells, basal cells, which maintain the skin epithelium, and mesenchymal stem cells, which maintain bone, cartilage, muscle and fat cells. Adult stem cells are a small minority of cells; they are vastly outnumbered by the progenitor cells and terminally differentiated cells that they differentiate into.Research into stem cells grew out of findings by Canadian biologists Ernest McCulloch, James Till and Andrew J. Becker at the University of Toronto and the Ontario Cancer Institute in the 1960s. As of 2016, the only established medical therapy using stem cells is hematopoietic stem cell transplantation, first performed in 1958 by French oncologist Georges Mathé. Since 1998 however, it has been possible to culture and differentiate human embryonic stem cells (in stem-cell lines). The process of isolating these cells has been controversial, because it typically results in the destruction of the embryo. Sources for isolating ESCs have been restricted in some European countries and Canada, but others such as the UK and China have promoted the research. Somatic cell nuclear transfer is a cloning method that can be used to create a cloned embryo for the use of its embryonic stem cells in stem cell therapy. In 2006, a Japanese team led by Shinya Yamanaka discovered a method to convert mature body cells back into stem cells. These were termed induced pluripotent stem cells (iPSCs).",1
Life Science,Strep-tag,"The Strep-tag system is a method which allows the purification and detection of proteins by affinity chromatography. The Strep-tag II is a synthetic peptide consisting of eight amino acids (Trp-Ser-His-Pro-Gln-Phe-Glu-Lys). This peptide sequence exhibits intrinsic affinity towards Strep-Tactin, a specifically engineered streptavidin, and can be N- or C- terminally fused to recombinant proteins. By exploiting the highly specific interaction, Strep-tagged proteins can be isolated in one step from crude cell lysates. Because the Strep-tag elutes under gentle, physiological conditions it is especially suited for generation of functional proteins.",1
Life Science,Streptamer,"The Streptamer technology allows the reversible isolation and staining of antigen-specific T cells. This technology combines a current T cell isolation method with the Strep-tag technology. In principle, the T cells are separated by establishing a specific interaction between the T cell of interest and a molecule that is conjugated to a marker, which enables the isolation. The reversibility of this interaction and the low temperatures at which it is performed allows for the isolation and characterization of functional T cells.
Because T cells remain phenotypically and functionally indistinguishable from untreated cells, this method offers modern strategies in clinical and basic T cell research.",1
Life Science,Strimvelis,"Autologous CD34+ enriched cell fraction that contains CD34+ cells transduced with retroviral vector that encodes for the human ADA cDNA sequence, sold under the brand name Strimvelis, is a medication used to treat severe combined immunodeficiency due to adenosine deaminase deficiency (ADA-SCID).The most common side effect is pyrexia (fever).ADA-SCID is a rare inherited condition in which there is a change (mutation) in the gene needed to make an enzyme called adenosine deaminase (ADA). As a result, people lack the ADA enzyme. Because ADA is essential for maintaining healthy lymphocytes (white blood cells that fight off infections), the immune system of people with ADA-SCID does not work properly and without effective treatment they rarely survive more than two years.Strimvelis is the first ex vivo autologous gene therapy approved by the European Medicines Agency (EMA).",1
Life Science,Stromagen,"Stromagen is a product that is made of stem cells taken from a patient's bone marrow and grown in the laboratory. After a patient's bone marrow is destroyed by treatment with whole body irradiation or chemotherapy, these cells are injected back into the patient to help rebuild bone marrow. Stromagen has been studied in the prevention of graft-versus-host disease during stem cell transplant in patients receiving treatment for cancer. Stromagen is used in cellular therapy. Also called autologous expanded mesenchymal stem cells OTI-010. Peripheral stem cell transplantation may allow doctors to give higher doses of chemotherapy and kill more tumor cells. It is not yet known whether Stromagen improves the success of stem cell transplantation in women with breast cancer.",1
Life Science,Subcloning,"In molecular biology, subcloning is a technique used to move a particular DNA sequence from a parent vector to a destination vector.
Subcloning is not to be confused with molecular cloning, a related technique.",1
Life Science,Substantial equivalence,"In food safety, the concept of substantial equivalence holds that the safety of a new food, particularly one that has been genetically modified (GM), may be assessed by comparing it with a similar traditional food that has proven safe in normal use over time. It was first formulated as a food safety policy in 1993, by the Organisation for Economic Co-operation and Development (OECD).As part of a food safety testing process, substantial equivalence is the initial step, establishing toxicological and nutritional differences in the new food compared to a conventional counterpart—differences are analyzed and evaluated, and further testing may be conducted, leading to a final safety assessment.Substantial equivalence is the underlying principle in GM food safety assessment for a number of national and international agencies, including the Canadian Food Inspection Agency (CFIA), Japan's Ministry of Health, Labour and Welfare (MHLW), the US Food and Drug Administration (FDA), and the United Nations' Food and Agriculture Organization (FAO) and  World Health Organization.",1
Life Science,Subtelomere,Subtelomeres are segments of DNA between telomeric caps and chromatin.,1
Life Science,Suppression subtractive hybridization,"Subtractive hybridization is a technology that allows for PCR-based amplification of only cDNA fragments that differ between a control (driver) and experimental transcriptome. cDNA is produced from mRNA.  Differences in relative abundance of transcripts are highlighted, as are genetic differences between species.  The technique relies on the removal of dsDNA formed by hybridization between a control and test sample, thus eliminating cDNAs or gDNAs of similar abundance, and retaining differentially expressed, or variable in sequence, transcripts or genomic sequences.
Suppression subtractive hybridization has also been successfully used to identify strain- or species-specific DNA sequences in a variety of bacteria including Vibrio species (Metagenomics).",1
Life Science,Surface tension biomimetics,"Surface tension is one of the areas of interest in biomimetics research. Surface tension forces will only begin to dominate gravitational forces below length scales on the order of the fluid's capillary length, which for water is about 2 millimeters. Because of this scaling, biomimetic devices that utilize surface tension will generally be very small, however there are many ways in which such devices could be used.",1
Life Science,Surfactant,"Surfactants are compounds that lower the surface tension (or interfacial tension) between two liquids, between a gas and a liquid, or between a liquid and a solid. Surfactants may act as detergents, wetting agents, emulsifiers, foaming agents, or dispersants.
The word ""surfactant"" is a blend of surface-active agent,
coined c.  1950.Agents that increase surface tension are ""surface active"" in the literal sense but are not called surfactants as their effect is opposite to the common meaning. A common example of surface tension increase is salting out: by adding an inorganic salt to an aqueous solution of a weakly polar substance, the substance will precipitate. The substance may itself be a surfactant – this is one  of the reasons why many surfactants are ineffective in sea water.",1
Life Science,SynBio,"SynBio is a long-term project started in 2011 with the goal of creating innovative medicines, including what are known as Biobetters. This project is a collaborative effort of several Russian and international pharmaceutical companies. The largest private participant of SynBio is the Human Stem Cells Institute (HSCI), a leading Russian biotech company, and Rusnano is a key investor.   The project is a significant example of international cooperation between researchers in Russia, England, and Germany. Special project company SynBio LLC is headquartered in Moscow.Currently, SynBio LLC is developing nine drugs based on three biotechnology platforms (Histone, PolyXen and Gemacell) for the treatment of liver disease, cardiovascular disease, acute leukemia, growth hormone deficiency and diabetes mellitus.The SynBio project also entails the creation of modern production facilities. These facilities will be dedicated to the manufacturing of the company's pharmaceutical substances and market-ready medicines once they have successfully undergone clinical testing.

",1
Life Science,Synchronous coefficient of drag alteration,"Synchronous coefficient of drag alteration (SCODA) is a biotechnology method for purifying, separating and/or concentrating bio-molecules. SCODA has the ability to separate molecules whose mobility (or drag) can be altered in sync with a driving field. This technique has been primarily used for concentrating and purifying DNA, where DNA mobility changes with an applied electrophoretic field. Electrophoretic SCODA has also been demonstrated with RNA and proteins.",1
Life Science,Synthetic biology,"Synthetic biology (SynBio) is a multidisciplinary area of research that seeks to create new biological parts, devices, and systems, or to redesign systems that are already found in nature.
It is a branch of science that encompasses a broad range of methodologies from various disciplines, such as biotechnology, genetic engineering, molecular biology, molecular engineering, systems biology, membrane science, biophysics, chemical and biological engineering, electrical and computer engineering, control engineering and evolutionary biology.
Due to more powerful genetic engineering capabilities and decreased DNA synthesis and sequencing costs, the field of synthetic biology is rapidly growing. In 2016, more than 350 companies across 40 countries were actively engaged in synthetic biology applications; all these companies had an estimated net worth of $3.9 billion in the global market.",1
Life Science,Transfer DNA,"The transfer DNA (abbreviated T-DNA) is the transferred DNA of the tumor-inducing (Ti) plasmid of some species of bacteria such as Agrobacterium tumefaciens and Agrobacterium rhizogenes(actually an Ri plasmid). The T-DNA is transferred from bacterium into the host plant's nuclear DNA genome. The capability of this specialized tumor-inducing (Ti) plasmid is attributed to two essential regions required for DNA transfer to the host cell. The T-DNA is bordered by 25-base-pair repeats on each end. Transfer is initiated at the right border and terminated at the left border and requires the vir genes of the Ti plasmid.
The bacterial T-DNA is about 24,000 base pairs long and contains plant-expressed genes that code for enzymes synthesizing opines and phytohormones. By transferring the T-DNA into the plant genome, the bacterium essentially reprograms the plant cells to grow into a tumor and produce a unique food source for the bacteria. The synthesis of the plant hormones auxin and cytokinin by enzymes encoded in the T-DNA  enables the plant cell to overgrow, thus forming the crown gall tumors typically induced by Agrobacterium tumefaciens infection. Agrobacterium rhizogenes causes a similar infection known as hairy root disease. The opines are amino acid derivatives used by the bacterium as a source of carbon and energy. This natural process of horizontal gene transfer in plants is being utilized as a tool for fundamental and applied research in plant biology through Agrobacterium tumefaciens mediated foreign gene transformation and insertional mutagenesis. Plant genomes can be engineered by use of Agrobacterium for the delivery of sequences hosted in T-DNA binary vectors.",1
Life Science,TA cloning,"TA cloning (also known as rapid cloning or T cloning) is a subcloning technique that avoids the use of restriction enzymes and is easier and quicker than traditional subcloning. The technique relies on the ability of adenine (A) and thymine (T) (complementary basepairs) on different DNA fragments to hybridize and, in the presence of ligase, become ligated together. PCR products are usually amplified using Taq DNA polymerase which preferentially adds an adenine to the 3' end of the product. Such PCR amplified inserts are cloned into linearized vectors that have complementary 3' thymine overhangs.",1
Life Science,Tac-Promoter,"The Tac-Promoter (abbreviated as Ptac), or tac vector is a synthetically produced DNA promoter, produced from the combination of promoters from the trp and lac operons. It is commonly used for protein production in Escherichia coli.Two hybrid promoters functional in Escherichia coli were constructed. These hybrid promoters, tacI and tacII, were derived from sequences of the trp and the lac UV5 promoters. In the first hybrid promoter (tacI), the DNA upstream of position -20 with respect to the transcriptional start site was derived from the trp promoter. The DNA downstream of position -20 was derived from the lac UV5 promoter. In the second hybrid promoter (tacII), the DNA upstream of position -11 at the Hpa I site within the Pribnow box was derived from the trp promoter. The DNA downstream of position -11 is a 46-base-pair synthetic DNA fragment that specifies part of the hybrid Pribnow box and the entire lac operator. It also specifies a Shine-Dalgarno sequence flanked by two unique restriction sites (portable Shine-Dalgarno sequence).
The tacI and the tacII promoters respectively direct transcription approximately 11 and 7 times more efficiently than the derepressed parental lac UV5 promoter and approximately 3 and 2 times more efficiently than the trp promoter in the absence of the trp repressor. Both hybrid promoters can be repressed by the lac repressor and both can be derepressed with isopropyl-beta-D-thiogalactoside. Consequently, these hybrid promoters are useful for the controlled expression of foreign genes at high levels in E. coli. In contrast to the trp and the lac UV5 promoters, the tacI promoter has not only a consensus -35 sequence but also a consensus Pribnow box sequence. This may explain the higher efficiency of this hybrid promoter with respect to either one of the parental promoters.",1
Life Science,Talimogene laherparepvec,"Talimogene laherparepvec, sold under the brand name Imlygic, is a biopharmaceutical medication used to treat melanoma that cannot be operated on; it is injected directly into a subset of lesions which generates a systemic immune response against the recipient's cancer.  The final four year analysis from the pivotal phase 3 study upon which TVEC was approved by the FDA showed a 31.5% response rate with a 16.9% complete response (CR) rate. There was also a substantial and statistically significant survival benefit in patients with earlier metastatic disease (stages IIIb-IVM1a) and in patients who hadn't received prior systemic treatment for melanoma. The earlier stage group had a reduction in the risk of death of approximately 50% with one in four patients appearing to have met, or be close to be reaching, the medical definition of cure. Real world use of talimogene laherparepvec have shown response rates of up to 88.5% with CR rates of up to 61.5%.Around half of people treated with talimogene laherparepvec in clinical trials experienced fatigue and chills; around 40% had fever, around 35% had nausea, and around 30% had flu-like symptoms as well as pain at the injection site.  The reactions were mild to moderate in severity; 2% of people had severe reactions and these were generally cellulitis.Talimogene laherparepvec is a genetically engineered herpes virus (an oncolytic herpes virus). Two genes were removed – one that shuts down an individual cell's defenses, and another that helps the virus evade the immune system – and a gene for human GM-CSF was added.  The drug works by replicating in cancer cells, causing them to burst; it was also designed to stimulate an immune response against the patient's cancer, which has been demonstrated by multiple pieces of data, including regression of tumors which have not been injected with talimogene laherparepvec.The drug was created and initially developed by BioVex, Inc. and was continued by Amgen, which acquired BioVex in 2011. It was one of the first oncolytic immunotherapy approved globally; it was approved in the US in October 2015 and approved in Europe in December 2015.

",1
Life Science,Tandem mass tag,"A tandem mass tag (TMT) is a chemical label that facilitates sample multiplexing in mass spectrometry (MS)-based quantification and identification of biological macromolecules such as proteins, peptides and nucleic acids. TMT belongs to a family of reagents referred to as isobaric mass tags which are a set of molecules with the same mass, but yield reporter ions of differing mass after fragmentation. The relative ratio of the measured reporter ions represents the relative abundance of the tagged molecule, although ion suppression has a detrimental effect on accuracy. Despite these complications, TMT-based proteomics has been shown to afford higher precision than Label-free quantification. In addition to aiding in protein quantification, TMT tags can also increase the detection sensitivity of certain highly hydrophilic analytes, such as phosphopeptides, in RPLC-MS analyses.

",1
Life Science,Ti plasmid,"A tumour inducing (Ti) plasmid is a plasmid found in pathogenic species of Agrobacterium, including A. tumefaciens, A. rhizogenes, A. rubi and A. vitis.
Evolutionarily, the Ti plasmid is part of a family of plasmids carried by many species of Alphaproteobacteria. Members of this plasmid family are defined by the presence of a conserved DNA region known as the repABC gene cassette, which mediates the replication of the plasmid, the partitioning of the plasmid into daughter cells during cell division as well as the maintenance of the plasmid at low copy numbers in a cell. The Ti plasmids themselves are sorted into different categories based on the type of molecule, or opine, they allow the bacteria to break down as an energy source.The presence of this Ti plasmid is essential for the bacteria to cause crown gall disease in plants. This is facilitated via certain crucial regions in the Ti plasmid, including the vir region, which encodes for virulence genes, and the transfer DNA (T-DNA) region, which is a section of the Ti plasmid that is transferred via conjugation into host plant cells after an injury site is sensed by the bacteria. These regions have features that allow the delivery of T-DNA into host plant cells, and can modify the host plant cell to cause the synthesis of molecules like plant hormones (e.g. auxins, cytokinins) and opines and the formation of crown gall tumours.Because the T-DNA region of the Ti plasmid can be transferred from bacteria to plant cells, it represented an exciting avenue for the transfer of DNA between kingdoms and spurred large amounts of research on the Ti plasmid and its possible uses in bioengineering.

",1
Life Science,TK cell therapy,"TK is an experimental cell therapy which may be used to treat high-risk leukemia.  It is currently undergoing a Phase III clinical trial to determine efficacy and clinical usefulness.TK is currently being investigated in patients suffering from acute leukemia in first or subsequent complete remission and at high risk of relapse or in patients with relapsed disease who are candidates for haploidentical transplantation of hemopoietic stem cells (taken from a partially HLA-compatible family donor).

",1
Life Science,Trajectory inference,"Trajectory inference or pseudotemporal ordering is a computational technique used in single-cell transcriptomics to determine the pattern of a dynamic process experienced by cells and then arrange cells based on their progression through the process. Single-cell protocols have much higher levels of noise than bulk RNA-seq, so a common step in a single-cell transcriptomics workflow is the clustering of cells into subgroups.  Clustering can contend with this inherent variation by combining the signal from many cells, while allowing for the identification of cell types. However, some differences in gene expression between cells are the result of dynamic processes such as the cell cycle, cell differentiation, or response to an external stimuli. Trajectory inference seeks to characterize such differences by placing cells along a continuous path that represents the evolution of the process rather than dividing cells into discrete clusters. In some methods this is done by projecting cells onto an axis called pseudotime which represents the progression through the process.

",1
Life Science,Transcription activator-like effector nuclease,"Transcription activator-like effector nucleases (TALEN) are restriction enzymes that can be engineered to cut specific sequences of DNA. They are made by fusing a TAL effector DNA-binding domain to a DNA cleavage domain (a nuclease which cuts DNA strands). Transcription activator-like effectors (TALEs) can be engineered to bind to practically any desired DNA sequence, so when combined with a nuclease, DNA can be cut at specific locations. The restriction enzymes can be introduced into cells, for use in gene editing or for genome editing in situ, a technique known as genome editing with engineered nucleases. Alongside zinc finger nucleases and CRISPR/Cas9, TALEN is a prominent tool in the field of genome editing.

",1
Life Science,Transfer DNA binary system,"A transfer DNA (T-DNA) binary system is a pair of plasmids consisting of a T-DNA binary vector and a vir helper plasmid. The two plasmids are used together (thus binary) to produce genetically modified plants. They are artificial vectors that have been derived from the naturally occurring Ti plasmid found in bacterial species of the genus Agrobacterium, such as A. tumefaciens. The binary vector is a shuttle vector, so-called because it is able to replicate in multiple hosts (e.g. Escherichia coli and Agrobacterium).
Systems in which T-DNA and vir genes are located on separate replicons are called T-DNA binary systems. T-DNA is located on the binary vector (the non-T-DNA region of this vector containing origin(s) of replication that could function both in E. coli and Agrobacterium, and antibiotic resistance genes used to select for the presence of the binary vector in bacteria, became known as vector backbone sequences). The replicon containing the vir genes became known as the vir helper plasmid. The vir helper plasmid is considered disarmed if it does not contain oncogenes that could be transferred to a plant.",1
Life Science,Transposon silencing,"Transposon silencing is a form of transcriptional gene silencing targeting transposons. Transcriptional gene silencing is a product of histone modifications that prevent the transcription of a particular area of DNA. Transcriptional silencing of transposons is crucial to the maintenance of a genome. The “jumping” of transposons generates genomic instability and can cause extremely deleterious mutations. Transposable element insertions have been linked to many diseases including hemophilia, severe combined immunodeficiency, and predisposition to cancer. The silencing of transposons is therefore extremely critical in the germline in order to stop transposon mutations from developing and being passed on to the next generation. Additionally, these epigenetic defenses against transposons can be heritable. Studies in Drosophila, Arabidopsis thaliana, and mice all indicate that small interfering RNAs are responsible for transposon silencing. In animals, these siRNAS and piRNAs are most active in the gonads.",1
Life Science,Trichoderma asperellum,"Trichoderma asperellum Samuels, Lieckf & Nirenberg  is a species of fungus in the family Hypocreaceae. It can be distinguished from T. viride by molecular and phenotypic characteristics. The most important molecular characteristics are divergent ITS-1 and 28S sequences and RFLP's of the endochitinase gene. Main phenotypic characters are conidial ornamentation and arrangement and branching of the conidiophores.

",1
Life Science,Trichoderma hamatum,"Trichoderma hamatum is a species of fungus in the family Hypocreaceae. It has been used a biological control of certain plant diseases.

",1
Life Science,Trichoderma stromaticum,Trichoderma stromaticum is a species of fungus in the family Hypocreaceae. It is a parasite of the cacao witches broom pathogen and has been used in its biological control.,1
Life Science,Tumor-homing bacteria,"Tumor homing bacteria is a group of facultative or obligate anaerobic bacteria (capable of producing ATP when oxygen is absent or is destroyed in normal oxygen levels) that are able to target cancerous cells in the body, suppress tumor growth and survive in the body for a long time even after the infection. When this type of bacteria is administered into the body it migrates to the cancerous tissues and starts to grow, then deploys distinct mechanisms to destroy solid tumors. Each bacteria species uses a different process to eliminate the tumor. Some common tumor homing bacteria include Salmonella, Clostridium, Bifidobacterium, Listeria, and Streptococcus. The earliest research of this type of bacteria was highlighted in 1813 when scientists began observing that patients that had gas gangrene, an infection caused by the bacteria Clostridium, were able to have tumor regressions.",1
Life Science,UKM Medical Molecular Biology Institute,"The UKM Medical Molecular Biology Institute, usually referred to as UMBI, is a biomedicine and cancer research institute located in Bandar Tun Razak, Kuala Lumpur, Malaysia. The institute is one of research institute in National University of Malaysia. UMBI was established in 2003. The institute has been recognized as a Center for Excellence in Higher Education (HICoE) in 2009 by the Prime Minister of Malaysia.",1
Life Science,Biotechnology in pharmaceutical manufacturing,Modern pharmaceutical manufacturing techniques frequently rely upon biotechnology.,1
Life Science,Vaccination,"Vaccination is the administration of a vaccine to help the immune system develop immunity from a disease. Vaccines contain a microorganism or virus in a weakened, live or killed state, or proteins or toxins from the organism. In stimulating the body's adaptive immunity, they help prevent sickness from an infectious disease. When a sufficiently large percentage of a population has been vaccinated, herd immunity results. Herd immunity protects those who may be immunocompromised and cannot get a vaccine because even a weakened version would harm them. The effectiveness of vaccination has been widely studied and verified. Vaccination is the most effective method of preventing infectious diseases; widespread immunity due to vaccination is largely responsible for the worldwide eradication of smallpox and the elimination of diseases such as polio and tetanus from much of the world. However, some diseases, such as measles outbreaks in America, have seen rising cases due to relatively low vaccination rates in the 2010s – attributed, in part, to vaccine hesitancy.The first disease people tried to prevent by inoculation was most likely smallpox, with the first recorded use of variolation occurring in the 16th century in China. It was also the first disease for which a vaccine was produced. Although at least six people had used the same principles years earlier, the smallpox vaccine was invented in 1796 by English physician Edward Jenner. He was the first to publish evidence that it was effective and to provide advice on its production. Louis Pasteur furthered the concept through his work in microbiology. The immunization was called vaccination because it was derived from a virus affecting cows (Latin: vacca 'cow'). Smallpox was a contagious and deadly disease, causing the deaths of 20–60% of infected adults and over 80% of infected children. When smallpox was finally eradicated in 1979, it had already killed an estimated 300–500 million people in the 20th century.Vaccination and immunization have a similar meaning in everyday language. This is distinct from inoculation, which uses unweakened live pathogens. Vaccination efforts have been met with some reluctance on scientific, ethical, political, medical safety, and religious grounds, although no major religions oppose vaccination, and some consider it an obligation due to the potential to save lives. In the United States, people may receive compensation for alleged injuries under the National Vaccine Injury Compensation Program. Early success brought widespread acceptance, and mass vaccination campaigns have greatly reduced the incidence of many diseases in numerous geographic regions. The Centers for Disease Control and Prevention lists vaccination as one of the ten great public health achievements of the 20th century in the U.S.",1
Life Science,Vectors in gene therapy,"Gene therapy utilizes the delivery of DNA into cells, which can be accomplished by several methods, summarized below. The two major classes of methods are those that use recombinant viruses (sometimes called biological nanoparticles or viral vectors) and those that use naked DNA or DNA complexes (non-viral methods).",1
Life Science,ViroCap,"ViroCap is a test announced in 2015 by researchers at Washington University in St. Louis which can detect most of the infectious viruses which affect both humans and animals. It was demonstrated to be as sensitive as the various Polymerase chain reaction assays for the viruses. It will not be available for clinical use until validation studies are done, which may take years. The test examines two million sequences of genetic data from viruses. The research was published in September 2015 in the online journal Genome Research.",1
Life Science,Virotherapy,"Virotherapy is a treatment using biotechnology to convert viruses into therapeutic agents by reprogramming viruses to treat diseases. There are three main branches of virotherapy: anti-cancer oncolytic viruses, viral vectors for gene therapy and viral immunotherapy. These branches use three different types of treatment methods: gene overexpression, gene knockout, and suicide gene delivery. Gene overexpression adds genetic sequences that compensate for low to zero levels of needed gene expression. Gene knockout uses RNA methods to silence or reduce expression of disease-causing genes. Suicide gene delivery introduces genetic sequences that induce an apoptotic response in cells, usually to kill cancerous growths. In a slightly different context, virotherapy can also refer more broadly to the use of viruses to treat certain medical conditions by killing pathogens.",1
Life Science,Whole genome sequencing,"Whole genome sequencing (WGS), also known as full genome sequencing, complete genome sequencing, or entire genome sequencing, is the process of determining the entirety, or nearly the entirety, of the DNA sequence of an organism's genome at a single time. This entails sequencing all of an organism's chromosomal DNA as well as DNA contained in the mitochondria and, for plants, in the chloroplast.
Whole genome sequencing has largely been used as a research tool, but was being introduced to clinics in 2014. In the future of personalized medicine, whole genome sequence data may be an important tool to guide therapeutic intervention. The tool of gene sequencing at SNP level is also used to pinpoint functional variants from association studies and improve the knowledge available to researchers interested in evolutionary biology, and hence may lay the foundation for predicting disease susceptibility and drug response.
Whole genome sequencing should not be confused with DNA profiling, which only determines the likelihood that genetic material came from a particular individual or group, and does not contain additional information on genetic relationships, origin or susceptibility to specific diseases. In addition, whole genome sequencing should not be confused with methods that sequence specific subsets of the genome – such methods include whole exome sequencing (1–2% of the genome) or SNP genotyping (< 0.1% of the genome).",1
Life Science,WiCell,"WiCell Research Institute is a scientific research institute in Madison, Wisconsin that focuses on stem cell research. Independently governed and supported as a 501(c)(3) organization, WiCell operates as an affiliate of the Wisconsin Alumni Research Foundation and works to advance stem cell research at the University of Wisconsin–Madison and beyond.",1
Life Science,Xenobiology,"Xenobiology (XB) is a subfield of synthetic biology, the study of synthesizing and manipulating biological devices and systems. The name ""xenobiology"" derives from the Greek word xenos, which means ""stranger, alien"". Xenobiology is a form of biology that is not (yet) familiar to science and is not found in nature. In practice, it describes novel biological systems and biochemistries that differ from the canonical DNA–RNA-20 amino acid system (see central dogma of molecular biology). For example, instead of DNA or RNA, XB explores nucleic acid analogues, termed xeno nucleic acid (XNA) as information carriers. It also focuses on an expanded genetic code and the incorporation of non-proteinogenic amino acids into proteins.",1
Life Science,Zinc finger nuclease,"Zinc-finger nucleases (ZFNs) are artificial restriction enzymes generated by fusing a zinc finger DNA-binding domain to a DNA-cleavage domain.   Zinc finger domains can be engineered to target specific desired DNA sequences and this enables zinc-finger nucleases to target unique sequences within complex genomes.  By taking advantage of endogenous DNA repair machinery, these reagents can be used to precisely alter the genomes of higher organisms. Alongside CRISPR/Cas9 and TALEN, ZFN is a prominent tool in the field of genome editing.",1
Life Science,ZMapp,"ZMapp is an experimental biopharmaceutical drug comprising three chimeric monoclonal antibodies under development as a treatment for Ebola virus disease. Two of the three components were originally developed at the Public Health Agency of Canada's National Microbiology Laboratory (NML), and the third at the U.S. Army Medical Research Institute of Infectious Diseases; the cocktail was optimized by Gary Kobinger, a research scientist at the NML and underwent further development under license by Mapp Biopharmaceutical. ZMapp was first used on humans during the 2014 West Africa Ebola virus outbreak, having only been previously tested on animals and not yet subjected to a randomized controlled trial. The NIH ran a clinical trial starting in January 2015 with subjects from Sierra Leone, Guinea, and Liberia aiming to enroll 200 people, but the epidemic waned and the trial closed early, leaving it too statistically underpowered to give a meaningful result about whether ZMapp worked.In 2016, a clinical study comparing ZMapp to the current standard of care for Ebola was inconclusive.

",1
Life Science,Outline of health sciences,"The following outline is provided as an overview of and topical guide to health sciences:
Health sciences are those sciences which focus on health, or health care, as core parts of their subject matter. Health sciences relate to multiple academic disciplines, including STEM disciplines and emerging patient safety disciplines (such as social care research).",1
Life Science,Biomedical sciences,"Biomedical sciences are a set of sciences applying portions of natural science or formal science, or both, to develop knowledge, interventions, or technology that are of use in healthcare or public health.  Such disciplines as medical microbiology, clinical virology, clinical epidemiology, genetic epidemiology, and biomedical engineering are medical sciences.  In explaining physiological mechanisms operating in pathological processes, however, pathophysiology can be regarded as basic science.
Biomedical Sciences, as defined by the UK Quality Assurance Agency for Higher Education Benchmark Statement in 2015, includes those science disciplines whose primary focus is the biology of human health and disease and ranges from the generic study of biomedical sciences and human biology to more specialised subject areas such as pharmacology, human physiology and human nutrition. It is underpinned by relevant basic sciences including anatomy and physiology, cell biology, biochemistry, microbiology, genetics and molecular biology, immunology, mathematics and statistics, and bioinformatics. As such the biomedical sciences have a much wider range of academic and research activities and economic significance than that defined by hospital laboratory sciences. Biomedical Sciences are the major focus of bioscience research and funding in the 21st century.https://www.hitech-ly.com/autoclave/autoclave-common-problems/==Roles within biomedical science==
A sub-set of biomedical sciences is the science of clinical laboratory diagnosis. This is commonly referred to in the UK as 'biomedical science' or 'healthcare science'. There are at least 45 different specialisms within healthcare science, which are traditionally grouped into three main divisions:
specialisms involving life sciences
specialisms involving physiological science
specialisms involving medical physics or bioengineering

",1
Life Science,Anthropomaximology,"According to the International Federation of Kinesiology, anthropomaximology is the study of the anatomy, physiology, and mechanics of body movement, especially in humans, and its application to the evaluation and treatment of muscular imbalance or derangement. The concept was developed in the USSR during the 1970s–1980s as a result of numerous Olympic victories. The Soviets utilized anthropomaximology in their athletic training, combining rigorous physical exercise with mental training techniques which allowed the competitors to tap into ""hidden reserves"" and surpass other athletes' endurance.",1
Life Science,Health Sciences Descriptors,"DeCS – Health Sciences Descriptors is a structured and trilingual thesaurus created by BIREME – Latin American and Caribbean Center on Health Sciences Information – in 1986 for indexing scientific journal articles, books, proceedings of congresses, technical reports and other types of materials, as well as for searching and recovering scientific information in LILACS, MEDLINE and other databases. In the VHL, Virtual Health Library, DeCS is the tool that permits the navigation between records and sources of information through controlled concepts and organized in Portuguese, Spanish and English.
It was developed from MeSH – Medical Subject Headings from the NLM – U.S. National Library of Medicine – in order to permit the use of common terminology for searching in three languages, providing a consistent and unique environment for information retrieval regardless of the language. In addition to the original MeSH terms, four specific areas were developed: Public Health (1986), Homeopathy (1991), Health Surveillance (2005), and Science and Health (2005).
The concepts that compose the DeCS vocabulary are organized in a hierarchical structure permitting searches in broader or more specific terms or all the terms that belong to a single hierarchy.  
Its main purpose is to serve as a unique language for indexing and recovery of information among the components of the Latin American and Caribbean Health Sciences Information System, coordinated by BIREME and that encompasses 37 countries in Latin America and the Caribbean, permitting a uniform dialog between nearly 600 libraries.  
DeCS participates in the unified terminology development project, UMLS – Unified Medical Language System of the NLM, with the responsibility of contributing with the terms in Portuguese and Spanish.",1
Business Administration,Business,"Business is the activity of making one's living or making money by producing or buying and selling products (such as goods and services). It is also ""any activity or enterprise entered into for profit.""Having a business name does not separate the business entity from the owner, which means that the owner of the business is responsible and liable for debts incurred by the business.  If the business acquires debts, the creditors can go after the owner's personal possessions.  A business structure does not allow for corporate tax rates. The proprietor is personally taxed on all income from the business.
The term is also often used colloquially (but not by lawyers or by public officials) to refer to a company. A company, on the other hand, is a separate legal entity and provides for limited liability, as well as corporate tax rates.  A company structure is more complicated and expensive to set up, but offers more protection and benefits for the owner.",0
Business Administration,Real estate business,"Real estate business is the profession of buying, selling, or renting real estate (land, buildings, or housing).

",0
Business Administration,11 Honoré,"11 Honoré is a Los Angeles-based luxury plus-sized contemporary fashion e-retailer that works directly with high-end designers and brands, including Zac Posen, Prabal Gurung, Jason Wu, Christian Siriano and more. Founded in 2017 by Patrick Herning, the company began with investments from Forerunner, Nordstrom, Upfront, Greycroft and Canvas. Over time, the company would expand from its initial 15 brands at launch to almost 90 within two years of its launch.In 2019, 11 Honoré received significant press coverage for its event at New York Fashion Week featuring a size-inclusive runway featuring multiple designers, headlined by actress Laverne Cox and plus-sized models including Candice Huffine, Marquita Pring, Precious Lee, Stella Duval and Tara Lynn.In 2020 the company launched its own private label, initially featuring 24 pieces.In October 2021, 11 Honoré announced a partnership with Nordstrom where the company's clothing will be accessible in Nordstrom stores as well as on the retailer's website.",0
Business Administration,Agribusiness,"Agribusiness refers to the enterprises, the industry, and the field of study of  value chains in agriculture and in the bio-economy,
in which case it is also called bio-business or bio-enterprise. 
The primary goal of agribusiness is to maximize profit while sustainably satisfying the needs of consumers for products related to natural resources such as biotechnology, farms, food, forestry, fisheries, fuel, and fiber — usually with the exclusion of non-renewable resources such as mining.Studies of business growth and performance in farming have found successful agricultural businesses are cost-efficient internally and operate in favorable economic, political, and physical-organic environments.  They are able to expand and make profits, improve the productivity of land, labor, and capital, and keep their costs down to ensure market price competitiveness.Agribusiness is not limited to farming. It encompasses a broader spectrum through the agribusiness system which includes input supplies, value-addition, marketing, entrepreneurship, microfinancing, agricultural extension, among others.
In some countries like the Philippines, creation and management of agribusiness enterprises require consultation with registered agriculturists if reached a certain level of operations, capitalization, land area, or number of animals in the farm.",0
Business Administration,Business interaction networks,"Business interaction networks are networks that allow businesses and their communities of interest to collaborate and do business online securely via the Internet.Mary Johnston Turner first discussed the concept in a Network World opinion piece in August 1995 and attributed the first advocacy for the concept to the now-defunct BBN Planet, the ISP division of BBN Technologies.",0
Business Administration,Business sector,"In economics, the business sector or corporate sector - sometimes popularly called simply ""business"" - is ""the part of the economy made up by companies"". It is a subset of the domestic economy, excluding the economic activities of general government, of private households, and of non-profit organizations serving individuals. The business sector is part of the private sector, but it differs in that the private sector includes all non-government activity, including non-profit organizations, while the business sector only includes business that operate for profit.
In the United States the business sector accounted for about 78 percent of the value of gross domestic product (GDP) as of 2000. Kuwait and Tuvalu each had business sectors accounting for less than 40% of GDP as of 2015.In systems of state capitalism, much of the business sector forms part of the public sector.
In  mixed economies, state-owned enterprises may straddle any divide between public and business sectors, allowing analysts to use the concept of a ""state-owned enterprise sector"".The Oxford English Dictionary records the phrase ""business sector"" in the general sense from 1934.
Word usage suggests that the concept of a ""business sector"" came into wider use after 1940.
Related terms in previous times included ""merchant class"" and ""merchant caste"".",0
Business Administration,Client (business),"In business, commerce, and economics, a client is a person who receives advice or services from a professional, such as a lawyer or a health care provider. Clients differ from customers in that customers are thought of as ""one-time buyers"" while clients can be summarized as ""long-term recipients.""",0
Business Administration,Closure (business),"Closure is the term used to refer to the actions necessary when it is no longer necessary or possible for a business or other organization to continue to operate.  Closure may be the result of a bankruptcy, where the organization lacks sufficient funds to continue operations, as a result of the proprietor of the business dying, as a result of a business being purchased by another organization (or a competitor) and shut down as superfluous, or because it is the non-surviving entity in a corporate merger.  A closure may occur because the purpose for which the organization was created is no longer necessary.
While a closure is typically of a business or a non-profit organization, any entity which is created by human beings can be subject to a closure, from a single church to a whole religion, up to and including an entire country if, for some reason, it ceases to exist.
Closures are of two types, voluntary or involuntary. Voluntary closures of organizations are much rarer than involuntary ones, as, in the absence of some change making operations impossible or unnecessary, most operations will continue until something happens that causes a change requiring this situation.
The most common form of voluntary closure would be when those involved in an organization such as a social club, a band, or other non-profit organization decide to cease operating. Once the organization has paid any outstanding debts and completed any pending operations, closure may simply mean that the organization ceases to exist.
If an organization has debts that cannot be paid, it may be necessary to perform a liquidation of its assets. If there is anything left after the assets are converted to cash, in the case of a for-profit organization, the remainder is distributed to the stockholders; in the case of a non-profit, by law any remaining assets must be distributed to another non-profit.
If an organization has more debts than assets, it may have to declare bankruptcy. If the organization is viable, it may reorganizes itself as a result of the bankruptcy and continue operations. If it is not viable for the business to continue operating, then a closure occurs through a bankruptcy liquidation: its assets are liquidated, the creditors are paid from whatever assets could be liquidated, and the business ceases operations.
Possibly the largest ""closure"" in history (but more closely analogous to a demerger) was the split of the Soviet Union into its constituent countries. In comparison, the end of East Germany can be considered a merger rather than a closure as West Germany assumed all of the assets and liabilities of East Germany. The end of the Soviet Union was the equivalent of a closure through a bankruptcy liquidation, because while Russia assumed most of the assets and responsibilities of the former Soviet Union, it did not assume all of them. There have been issues over who is responsible for unpaid parking tickets accumulated by motor vehicles operated on behalf of diplomatic missions operated by the former Soviet Union in other countries, as Russia claims it is not responsible for them.
Several major business closures include the bankruptcy of the Penn Central railroad, the Enron scandals, and MCI Worldcom's bankruptcy and eventual merger into Verizon.",0
Business Administration,Company code of conduct,"A code of conduct is a set of rules outlining the norms, rules, and responsibilities or proper practices of an individual party or an organization.",0
Business Administration,Core business,"The core business of an organization is an idealized construct intended to express that organization's ""main"" or ""essential"" activity.
Core business process means that a business's success depends not only on how well each department performs its work, but also on how well the company manages to coordinate departmental activities to conduct the core business process, which is;
1. The market-sensing process
Meaning all activities in gathering marketing intelligence and acting on the information.
2. The new-offering realization process
Covering all activities in research, development and launching new quality offerings quickly and within budget.
3. The customer acquisition process
all the activities defining the target market and prospecting for new customers
4. The customer relationship management process
all the activities covering building deeper understanding, relationships and offerings to individual customers.
5. The fulfillment management process
all the activities in receiving and approving orders, shipping out on time and collecting payment.
In business, a core item is defined as an item that is immediately responsible for the revenues and cash flows of that particular business, whereas a non-core item is of a more strategic view, intended to benefit the revenue model and cash flows of the core items.Therefor, the easiest way to identify a core business function is to look at whether the primary cash flows of the business revenue model runs directly through it, or not. Core business functions are always directly involved in the primary cash flows of the revenue model or models of the business, whereas non-core business functions usually are not, meaning that the business can technically operate without non-core business functions without impacting the primary cash flows, whereas the core business functions are essential to the continuance of its primary cash flows.
To be successful, a business needs to look for competitive advantages beyond its own operations. The business needs to look at the competitiveness value chain of suppliers, distributors and customers. Many companies today have partnered with specific suppliers and distributors to create a superior value delivery network.

",0
Business Administration,Developer relations,"Developer Relations, also known as DevRel, is an umbrella term covering the strategies and tactics for building and nurturing a community of mutually beneficial relationships between organizations and developers (e.g., software developers) as the primary users, and often influencers on purchases, of a product.Developer Relations is a form of Platform Evangelism and the activities involved are sometimes referred to as a Developer Program or DevRel Program. A DevRel program may comprise a framework built around some or all of the following aspects:
Developer Marketing: Outreach and engagement activities to create awareness and convert developers to use a product.
Developer Education: Product documentation and education resources to aid learning and build affinity with a product and community.
Developer Experience (DX): Resources like a developer portal, product, and documentation, to activate the developer with the least friction.
Developer Success: Activities to nurture and retain developers as they build and scale with a product.
Community: Nourishes a community to maintain a sustainable program.The impacts and goals of DevRel programs include:
Increased revenue and funding
User growth and retention
product innovation and improvements
Customer satisfaction and support deflection
Strong technical recruiting pipeline
Brand recognition and awarenessOther goals of DevRel initiatives can include:
Product Building: An organization relies on a community of developers to build their technology (e.g., open source).
Product-market Fit: The product's success depends on understanding developers' needs and desires.
Developer Enablement: Supporting developers' use of the product (e.g., by providing education, tools, and infrastructure).
Developer Perception: To overcome developer perceptions that may be preventing success of a product.
Hiring/Recruiting: To attract potential developers for recruitment.

",0
Business Administration,Edmund Hillary Fellowship,"The Edmund Hillary Fellowship is a Fellowship programme in New Zealand and community that provides exceptional entrepreneurs, investors and startup teams with a platform to incubate global impact ventures. The idea behind the Fellowship was to bring foreign entrepreneurs and investors to New Zealand to incubate new businesses. The Immigration New Zealand partnered with the Edmund Hillary Fellowship to deliver Global Impact Visa to 400 international applications over four years. It was setup in 2016 and has 532 fellows (400 international and 132 kiwis) as of March 2022.Immigration New Zealand will partner with the Edmund Hillary Fellowship to bring innovation-based ventures to New Zealand, announced Immigration Minister Michael Woodhouse.
Yoseph Ayele is one of the co-founders of the Edmund Hillary Fellowship and was the first CEO as well. Anna Kominik serves as their Board Chair.
Famous fellows of this Fellowship include Naval Ravikant, Deepa Malik, Amitabh Kant, Shay Wright, Tashi and Nungshi Malik, Kunal Kapur  and many more.",0
Business Administration,Portfolio career,"A portfolio career comprises a variety of roles rather than one job at a single organisation. It can be a career that combines multiple paid and/or voluntary roles.
The philosopher and organisational behaviourist Charles Handy popularised the ""portfolio"" concept
in works like his 1994 book The Empty Raincoat.
Handy's recognition of the portfolio career-path came about when he realised that individuals would be required to develop portable skillsets to meet the needs of a fast-moving future workplace. His prediction foresaw what is now known as the gig economy.Portfolio careers are often found in the creative industries where freelancing is the norm.
Economic conditions mean many are now actively choosing to pursue portfolio careers to make the most of their earning potential.",0
Business Administration,Ramapati Singhania,"Ramapati Singhania was born on May 8, 1956 to Gopal Krishna Singhania and Sulochana Devi of the Singhania family. He is the grandson of Sir Padampat Singhania who was knighted in the 1943 Honours list. He is an entrepreneur and author.Ramapati Singhania acquired his MBA from the University of Virginia in 1981. He is an alumnus of the Darden School of Business. He completed his BS in Electrical Engineering from Carnegie Mellon, Pennsylvania (USA).
He held the position of full time Director of JK Synthetics Ltd and Director of JK Bombay Ltd.Other companies he held crucial positions include:

JayKay Tech Ltd
Keventer Agro Ltd
JK Jute Mills.He contributed to promoting the Quality Circle Forum of India. He was president of the Employers Association, Rajasthan and the National Institute of Quality and Reliability. He was an instructor at the Search Engine Academy, UAE.Presently, Ramapati Singhania is the Managing Director of Varal Consultancy DMCC, Dubai (UAE).",0
Business Administration,Registered office,"A registered office is the official address of an incorporated company, association or any other legal entity. Generally it will form part of the public record and is required in most countries where the registered organization or legal entity is incorporated. A registered physical office address is required for incorporated organizations to receive official correspondence and formal notices from government departments, investors, banks, shareholders and the general public.: 209 

",0
Business Administration,Religion and business,"Religion and business have throughout history interacted in ways that relate to and affected one another, as well as influenced sociocultural evolution, political geographies, and labour laws. As businesses expand globally they seek new markets which leads to expanding their corporation's norms and rules to encompass the new locations norms which most often involve religious rules and terms.",0
Business Administration,Management,"Management (or managing) is the administration of an organization, whether it is a business, a non-profit organization, or a government body. It is the art and science of managing resources of the business. 
Management includes the activities of setting the strategy of an organization and coordinating the efforts of its employees (or of volunteers) to accomplish its objectives through the application of available resources, such as financial, natural, technological, and human resources. ""Run the business"" and ""Change the business"" are two concepts that are used in management to differentiate between the continued delivery of goods or services and adapting of goods or services to meet the changing needs of customers - see trend. The term ""management"" may also refer to those people who manage an organization—managers. 
Some people study management at colleges or universities; major degrees in management includes the Bachelor of Commerce (B.Com.), Bachelor of Business Administration (BBA.), Master of Business Administration (MBA.), Master in Management (MSM or MIM) and, for the public sector, the Master of Public Administration (MPA) degree. Individuals who aim to become management specialists or experts, management researchers, or professors may complete the Doctor of Management (DM), the Doctor of Business Administration (DBA), or the Ph.D. in Business Administration or Management. In the past few decades, there has been a movement for evidence-based management.Larger organizations generally have three hierarchical levels of managers, in a pyramid structure:

Senior managers such as members of a board of directors and a chief executive officer (CEO) or a president of an organization sets the strategic goals of the organization and make decisions on how the overall organization will operate. Senior managers are generally executive-level professionals who provide direction to middle management, and directly or indirectly report to them.
Middle managers such as branch managers, regional managers, department managers, and section managers, who provide direction to the front-line managers. They communicate the strategic goals of senior management to the front-line managers.
Line managers such as supervisors and front-line team leaders, oversee the work of regular employees (or volunteers, in some voluntary organizations) and provide direction on their work. Line managers often perform the traditional functions of management. They are usually considered part of the workforce and not a proper part of the organization's management.In smaller organizations, a manager may have a much wider scope and may perform several roles or even all of the roles commonly observed in a large organization.
Social scientists study management as an academic discipline, investigating areas such as social organization, organizational adaptation, and organizational leadership.",0
Business Administration,Index of management articles,"This is a list of articles on general management and strategic management topics. For articles on specific areas of management, such as marketing management, production management, human resource management, information technology management, and international trade, see the list of related topics at the bottom of this page.

Management an overview
Balanced scorecard
Benchmarking
Business intelligence
Industrial espionage
Environmental scanning
Marketing research
Competitor analysis
Reverse engineering
Operations
Popular management theories : a critique
Centralisation
Change management
Communications management
Conjoint analysis
Constraint Management
Focused improvement
Corporate governance
Corporation
Board of directors
Middle management
Senior management
Corporate titles
Cross ownership
Community management
Corporate image
Cost management
Spend management
Procurement
Crisis management
Critical management studies
Cultural intelligence
Decentralisation
Design management
Diagnostic Enterprise Method
Engineering Management
Enterprise content management
Content management system
Web content management system
Document management system
Contract management
Fixed assets management
Records Management
Extended Enterprise
Facility management
Force field analysis
Fraud deterrence
Human Interaction Management
Information technology management (MIS)
Knowledge management
Organizational development
Overall Equipment Effectiveness
Management effectiveness
Management fad
Management information systems
Management of Technology (MOT)
Midsourcing
Peter Drucker's Management by objectives (MBO)
Management consulting
Management science and operations research
Manufacturing
Just In Time manufacturing
Lean manufacturing
News management
Planning
Planning fallacy
Professional institutions in management
Quality management
Value-based management
Volatility, uncertainty, complexity and ambiguity
Project management
Risk management

",0
Business Administration,Outline of business management,"The following outline is provided as an overview of and topical guide to management:
Business management – management of a business. Business management rule #1 is delegation, assign the best qualified people to each position and trust your staff to do the work instead of trying to do everything yourself. It includes all aspects of overseeing and supervising business operations. Management is the act of allocating resources to accomplish desired goals and objectives efficiently and effectively; it comprises planning, organizing, staffing, leading or directing, and controlling an organization (a group of one or more people or entities) or effort for the purpose of accomplishing a goal.",0
Business Administration,Abilene paradox,"In the Abilene paradox, a group of people collectively decide on a course of action that is counter to the preferences of many or all of the individuals in the group. It involves a common breakdown of group communication in which each member mistakenly believes that their own preferences are counter to the group's and, therefore, does not raise objections, or even states support for an outcome they do not want. A common phrase relating to the Abilene paradox is a desire to not ""rock the boat"". This differs from groupthink in that the Abilene paradox is characterized by an inability to manage agreement.

",0
Business Administration,Abusive supervision,"Abusive supervision is most commonly studied in the context of the workplace, although it can arise in other areas such as in the household and at school. ""Abusive supervision has been investigated as an antecedent to negative subordinate workplace outcome.""  ""Workplace violence has combination of situational and personal factors"". The study that was conducted looked at the link between abusive supervision and different workplace events.Researchers have previously argued that abusive supervision is a one dimensional construct, however, recently it is found to be a four dimensional construct. The study of Ghayas and Jabeen is a paramount study that suggests abusive supervision to be a four dimensional construct where yelling, belittling behavior, scapegoating and credit stealing are described as the dimensions of abusive supervision. Researchers such as Tepper and Martinko had previously asserted that there was a need to study dimensions of abusive supervision.

",0
Business Administration,Action item,"In management, an action item is a documented event, task, activity, or action that needs to take place. Action items are discrete units that can be handled by a single person.

",0
Business Administration,Association management company,"An association management company, or AMC, provides management and specialized administrative services to non-profit trade associations and professional associations using a for-profit approach. Many AMCs serve as an organization's headquarters, managing day-to-day operations and becoming the public face of the organization.Services may include executive, administrative and financial management; strategic planning; membership development; public affairs and lobbying; education and professional development; statistical research; meetings management; and marketing and communication services. Orienting board members is common; AMCs lay out expectations for fiduciary oversight and point out conflicts of interest.Fernley & Fernley, Inc., based in Philadelphia and founded in 1886, was the first association management company in the United States. More than 600 AMCs worldwide now collectively manage associations ranging in budget size from $50,000 to $16 million and representing more than 3 million members. AMCs can be found in most major U.S. cities.The Alexandria, Va.-based AMC Institute accredits AMCs under the guidance of the American National Standards Institute. Current employees of AMCs are eligible to apply to become a Certified Association Executive.Chicago-based SmithBucklin is the world's largest AMC, although Geneva, Switzerland-based MCI Group, a professional conference organiser that offers AMC services, has more employees: 1,900 as of 2016.

",0
Business Administration,Business acumen,"Business acumen, also known as business savviness, business sense and business understanding, is keenness and quickness in understanding and dealing with a business situation (risks and opportunities) in a manner that is likely to lead to a good outcome. Additionally, business acumen has emerged as a vehicle for improving financial performance and leadership development. Consequently, several different types of strategies have developed around improving business acumen.

",0
Business Administration,CEO succession,"CEO succession refers to the process by which boards of directors ensure that their organization has the ability to sustain excellence in CEO leadership over time, with transitions from one leader to the next.",0
Business Administration,Chartered Administrator,"A Chartered Administrator (French: Administrateur agréé) is a member of the Ordre des Administrateurs Agréés du Québec who may use the abbreviation ""Adm.A."" in French or ""C.Adm."" in English.
The title Adm.A. or C.Adm. turns out to be a reserved regulated profession; it has no reserved activities. These management professionals are supervised by the Quebec professional system.
In Canada (outside Quebec), there is no equivalent professional title in the management sciences.",0
Business Administration,Cognitive inertia,"Cognitive inertia is the tendency for a particular orientation in how an individual thinks about an issue, belief or strategy to resist change. In clinical and neuroscientific literature it is often defined as a lack of motivation to generate distinct cognitive processes needed to attend to a problem or issue. The physics term inertia is to emphasize the rigidity and resistance to change in the method of cognitive processing that has been in use for a significant amount of time. Commonly confused with belief perseverance, cognitive inertia is the perseverance of how one interprets information, not the perseverance of the belief itself.
Cognitive inertia has been causally implicated in disregard of impending threat to one's health or environment, enduring political values and deficits in task switching. Interest in the phenomenon was largely taken up by economic and industrial psychologists to explain resistance to change in brand loyalty, group brainstorming and business strategies. In the clinical setting cognitive inertia has been used as a diagnostic tool for neurodegenerative diseases, depression and anxiety. Critics have stated that the term oversimplifies resistant thought processes and suggest a more integrative approach that involves motivation, emotion and developmental factors.",0
Business Administration,Communities of innovation,"Communities that support innovation have been referred to as communities of innovation (CoI), communities for innovation, innovation communities, open innovation communities, and communities of creation.

",0
Business Administration,Community of practice,"A community of practice (CoP) is a group of people who ""share a concern or a passion for something they do and learn how to do it better as they interact regularly"". The concept was first proposed by cognitive anthropologist Jean Lave and educational theorist Etienne Wenger in their 1991 book Situated Learning (Lave & Wenger 1991). Wenger then significantly expanded on the concept in his 1998 book Communities of Practice (Wenger 1998).
A CoP can evolve naturally because of the members' common interest in a particular domain or area, or it can be created deliberately with the goal of gaining knowledge related to a specific field. It is through the process of sharing information and experiences with the group that members learn from each other, and have an opportunity to develop personally and professionally (Lave & Wenger 1991).
CoPs can exist in physical settings, for example, a lunchroom at work, a field setting, a factory floor, or elsewhere in the environment, but members of CoPs do not have to be co-located. They form a ""virtual community of practice"" (VCoP) (Dubé, Bourhis & Jacob 2005) when they collaborate online, such as within discussion boards, newsgroups, or the various chats on social media, such as #musochat centered on contemporary classical music performance (Sheridan 2015). A ""mobile community of practice"" (MCoP) (Kietzmann et al. 2013) is when members communicate with one another via mobile phones and participate in community work on the go.
Communities of practice are not new phenomena: this type of learning has existed for as long as people have been learning and sharing their experiences through storytelling. The idea is rooted in American pragmatism, especially C. S. Peirce's concept of the ""community of inquiry"" (Shields 2003), but also John Dewey's principle of learning through occupation (Wallace 2007).

",0
Business Administration,Completed staff work,"Completed staff work is a principle of management which states that subordinates are responsible for submitting written recommendations to superiors in such a manner that the superior need do nothing further in the process than review the submitted document and indicate approval or disapproval.
In Completed Staff Work, the subordinate is responsible for identifying the problem or issue requiring decision by some higher authority. In written form such as a memorandum, the subordinate documents the research done, the facts gathered, and analysis made of alternative courses of action. The memo concludes with a specific recommendation for action by the superior.
The earliest description of the concept of Completed Staff Work appears in U.S. Army publications. Since its early military origin, it has subsequently found favor in police management texts in the U.S.James Webb, Director of the Bureau of the Budget (1946-1949), attributes the Doctrine of Completed Staff Work to President Harry S. Truman. 
However, a memo written and circulated by Briagadier General George A. Rehm, executive officer for the G-3, Operations section, attributes the policy to General MacArthur's headquarters in the Southwest Pacific Areas during World War II.",0
Business Administration,Control (management),"Control is a function of management which helps to check errors in order to take corrective actions. This is done to minimize deviation from standards and ensure that the stated goals of the organization are achieved in a desired manner.
According to modern concepts, control is a foreseeing action; earlier concepts of control were only used when errors were detected. Control in management includes setting standards, measuring actual performance and taking corrective action in decision making.",0
Business Administration,Corporate governance,"Corporate governance is defined, described or delineated in diverse ways, depending on the writer's purpose. Writers focussed on a disciplinary interest or context (such as accounting, finance, law, or management) often adopt narrow definitions that appear purpose-specific. Writers concerned with regulatory policy in relation to corporate governance practices often use broader structural descriptions. A broad (meta) definition that encompasses many adopted definitions is '“Corporate governance” describes the processes, structures, and mechanisms that influence the control and direction of corporations'.
This meta definition accommodates both the narrow definitions used in specific contexts and the broader descriptions that are often presented as authoritative. The latter include: the structural definition from the Cadbury Report, which identifies corporate governance as 'the system by which companies are directed and controlled' (Cadbury 1992, p. 15); and the relational-structural view adopted by the Organization for Economic Cooperation and Development (OECD) of 'Corporate governance involves a set of relationships between a company's management, its board, its shareholders and other stakeholders. Corporate governance also provides the structure through which the objectives of the company are set, and the means of attaining those objectives and monitoring performance are determined' (OECD 2015, p.9). .",0
Business Administration,Court of assistants,"A court of assistants is a council of members belonging to professional, trade, craft or livery companies.
The term originated among the London livery companies, as 'certain senior members who manage the affairs of the City of London Companies', but may also be used by other trade associations. A court of assistants usually comprises the governing body of such organisations and may include the officials, as in the case of the Worshipful Company of Clockmakers founded in 1631: ""The governing body of the Company is the Court of Assistants, comprising the Master, three Wardens and not less than ten Assistants.""Another example is the Honourable Artillery Company, which has an annual General Court open to all members: it meets in March to elect 20 Assistants. The company is governed in its civil and financial affairs by the Court of Assistants, which was first established in 1633.",0
Business Administration,Critical management studies,"Critical management studies (CMS) is a loose but extensive grouping of theoretically informed critiques of management, business and organisation, grounded originally in a critical theory perspective. Today it encompasses a wide range of perspectives that are critical of traditional theories of management and the business schools that generate these theories.

",0
Business Administration,Cultural intelligence,"Cultural intelligence or cultural quotient (CQ) is the capability to relate and work effectively across cultures, bearing similarity to the term cultural agility. The term has been used in business, education, government and academic research contexts. Originally, the term cultural intelligence and the abbreviation ""CQ"" was developed by the research done by Christopher Earley (2002) and Earley and Soon Ang (2003). During the same period, researchers David Thomas and Kerr Inkson worked on a complementary framework of CQ as well. A few years later, Ang Soon and Linn Van Dyne worked on a scale development of the CQ construct as a researched-based way of measuring and predicting intercultural performance.
The term is relatively recent: early definitions and studies of the concepts were given by P. Christopher Earley and Soon Ang in the book Cultural Intelligence: Individual Interactions Across Cultures (2003) and more fully developed later by David Livermore in the book, Leading with Cultural Intelligence. The concept is related to that of cross-cultural competence. but goes beyond that to actually look at intercultural capabilities as a form of intelligence that can be measured and developed. According to Earley, Ang, and Van Dyne, cultural intelligence can be defined as ""a person's capability to adapt as s/he interacts with others from different cultural
regions"", and has behavioral, motivational, and metacognitive aspects. Without cultural intelligence, both business and military actors seeking to engage foreigners are susceptible to mirror imaging.Cultural intelligence or CQ is measured on a scale, similar to that used to measure an individual's intelligence quotient. People with higher CQs are regarded as better able to successfully blend into any environment, using more effective business practices, than those with a lower CQ. CQ is assessed using the academically validated assessment created by Linn Van Dyne and Soon Ang. Both self-assessments and multi-rater assessments are available through the Cultural Intelligence Center in East Lansing, Michigan and the Center makes the CQ Scale available to other academic researchers at no charge. Research demonstrates that CQ is a consistent predictor of performance in multicultural settings.  Cultural intelligence research has been cited and peer-reviewed in more than seventy academic journals. The research and application of cultural intelligence is being driven by the Cultural Intelligence Center in the U.S. and Nanyang Business School in Singapore. Additional research and application of cultural intelligence has been conducted by Liliana Gil Valletta, who holds the trademark for the term since 2013. Defined as the ability to be aware of, understand and apply cultural competence into everyday business decisions, Gil Valletta has expanded the definition of cultural intelligence into a capability that yields a commercial advantage by turning cultural trends into profits and P&L impact. Since 2010, the firm CIEN+ and data science platform Culturintel is the first using artificial intelligence and big data tools to report measures of cultural intelligence and enable corporations to embed inclusion for business growth.

",0
Business Administration,Customer benefit package,"A customer benefit package (CBP) forms part of the operations management (OM) toolkit. It involves a clearly defined set of tangible (goods) and intangible (services) features that the customer recognizes, purchase or use. This can be the real or perceived value that a customer experiences or believes they are receiving through dealing with a company.",0
Business Administration,Decentralized decision-making,"Decentralized decision-making is any process where the decision-making authority is distributed throughout a larger group. It also connotes a higher authority given to lower level functionaries, executives, and workers. This can be in any organization of any size, from a governmental authority to a corporation. However, the context in which the term is used is generally that of larger organizations. This distribution of power, in effect, has far-reaching implications for the fields of management, organizational behavior, and government.
The decisions arising from a process of decentralized decision-making are the functional result of group intelligence and crowd wisdom. Decentralized decision-making also contributes to the core knowledge of group intelligence and crowd wisdom, often in a subconscious way a la Carl Jung's collective unconscious.
Decision theory is a method of deductive reasoning based on formal probability and deductive reasoning models. It is also studied in a specialized field of mathematics wherein models are used to help make decisions in all human activities including the sciences and engineering. (See also Game theory, Uncertainty, Expectation maximization principle.)",0
Business Administration,Delaying tactic,"A delaying tactic or delay tactic is a strategic device sometimes used during business, diplomatic or interpersonal negotiations, in which one party to the negotiation seeks to gain an advantage by postponing a decision.  Someone uses a delaying tactic when they expect to have a stronger negotiating position at a later time. They may also use a delaying tactic when they prefer the status quo to any of the potential resolutions, or to impose costs on the other party to compel them to accept a settlement or compromise. Delay tactics are also sometimes used as a form of indirect refusal wherein one party postpones a decision indefinitely rather than refusing a negotiation outright. To use a delaying tactic, the delaying party must have some form of control over the decision-making process.",0
Business Administration,Design leadership,"Design leadership is a concept complementary to design management. In practice, design managers within companies often operate in the field of design leadership and design leaders in the field of design management. However, the two terms are not interchangeable; they are interdependent. In essence, design leadership aims to define future strategies, and design management is responsible for implementation. Both are critically important to business, government, and society, and both are necessary in order to maximize value from design activity and investment.
Design leadership can be described as leadership that generates innovative design solutions. Turner defines design leadership by adding three additional aspects for design leadership:
the difference in leading through design,
the sustaining design leadership over time
the gaining of acknowledgment for achievements through design.Turner separates the core responsibilities of design leadership into the following six activities:
envisioning of the future
manifesting strategic intent
directing design investment
managing corporate reputation
creating and nurturing an environment of innovation
training for design leadership

",0
Business Administration,Diagnostic Enterprise Method,"The diagnostic enterprise method is a management theory whose methods were created based on Frederick Winslow Taylor's principles to develop new ways in which companies can change their internal structure without outside help. Some of the techniques bring the managers the advantage to find opportunity areas in the company to correct them without affecting the enterprise development. This methods can be applied without big costs in money, time and training. This method can be applied to almost any enterprise.",0
Business Administration,Disagree and commit,"Disagree and commit is a management principle which states that individuals are allowed to disagree while a decision is being made, but that once a decision has been made, everybody must commit to it. The principle can also be understood as a statement about when it is useful to have conflict and disagreement, with the principle saying disagreement is useful in early states of decision-making while harmful after a decision has been made. Disagree and commit is a method of avoiding the consensus trap, in which the lack of consensus leads to inaction.",0
Business Administration,Dynamic capabilities,"In organizational theory, dynamic capability is the capability of an organization to purposefully adapt an organization's resource base. The concept was defined by David Teece, Gary Pisano and Amy Shuen, in their 1997 paper Dynamic Capabilities and Strategic Management, as ""the firm’s ability to integrate, build, and reconfigure internal and external competences to address rapidly changing environments"".The term is often used in the plural form, dynamic capabilities, emphasizing that the ability to react adequately and timely to external changes requires a combination of multiple capabilities.

",0
Business Administration,Dynamic enterprise modeling,"Dynamic enterprise modeling (DEM) is an enterprise modeling approach developed by the Baan company, and used for the Baan enterprise resource planning system which aims ""to align and implement it in the organizational architecture of the end-using company"".According to Koning (2008), Baan introduced dynamic enterprise modelling in 1996 as a ""means for implementing the Baan ERP product. The modelling focused on a Petri net–based technique for business process modelling to which the Baan application units were to be linked. DEM also contains a supply-chain diagram tool for the logistic network of the company and of an enterprise function modelling diagram"".",0
Business Administration,Dynaxity,"Dynaxity is a compound word of dynamics and complexity. The term describes the combination of dynamics and complexity. It was invented in the late 1980s and was initially published and used by Rieckmann. The term was used by many authors, i.a. by Henning and Tiltmann.The term was developed from practical experiences in managing complex systems in companies and organizations and describes the simultaneous increase of complexity and dynamics as well as the implications for the perception, diagnosis and management of such.
In general, four different zones can be distinguished: static, dynamic, turbulent and chaotic. These four zones correspond to the different degrees of Dynaxity.",0
Business Administration,Empowerment,"Empowerment is the degree of autonomy and self-determination in people and in communities. This enables them to represent their interests in a responsible and self-determined way, acting on their own authority. It is the process of becoming stronger and more confident, especially in controlling one's life and claiming one's rights. Empowerment as action refers both to the process of self-empowerment and to professional support of people, which enables them to overcome their sense of powerlessness and lack of influence, and to recognize and use their resources.
As a term, empowerment originates from American community psychology and is associated with the social scientist Julian Rappaport (1981). However, the roots of empowerment theory extend further into history and are linked to Marxist sociological theory. These sociological ideas have continued to be developed and refined through Neo-Marxist Theory (also known as Critical Theory).In social work, empowerment forms a practical approach of resource-oriented intervention. In the field of citizenship education and democratic education, empowerment is seen as a tool to increase the responsibility of the citizen. Empowerment is a key concept in the discourse on promoting civic engagement. Empowerment as a concept, which is characterized by a move away from a deficit-oriented towards a more strength-oriented perception, can increasingly be found in management concepts, as well as in the areas of continuing education and self-help.",0
Business Administration,Energy monitoring and targeting,"Energy monitoring and targeting (M&T) is an energy efficiency technique based on the standard management axiom stating that “you cannot manage what you cannot measure”. M&T techniques provide energy managers with feedback on operating practices, results of energy management projects, and guidance on the level of energy use that is expected in a certain period. Importantly, they also give early warning of unexpected excess consumption caused by equipment malfunctions, operator error, unwanted user behaviours, lack of effective maintenance and the like.
The foundation of M&T lies in determining the normal relationships of energy consumptions to relevant driving factors (HVAC equipment, production though puts, weather, occupancy available daylight, etc.) and the goal is to help business managers:

Identify and explain excessive energy use
Detect instances when consumption is unexpectedly higher or lower than would usually have been the case
Visualize energy consumption trends (daily, weekly, seasonal, operational...)
Determine future energy use and costs when planning changes in the business
Diagnose specific areas of wasted energy
Observe how changes to relevant driving factors impact energy efficiency
Develop performance targets for energy management programs
Manage energy consumption, rather than accept it as a fixed costThe ultimate goal is to reduce energy costs through improved energy efficiency and energy management control. Other benefits generally include increased resource efficiency, improved production budgeting and reduction of  greenhouse gas (GHG) emissions.",0
Business Administration,Exit criteria,"Exit criteria are the criteria or requirements which must be met to complete a specific task or process as used in some fields of business or science, such as software engineering.",0
Business Administration,F-Law,"An A-law algorithm is a standard companding algorithm, used in European 8-bit PCM digital communications systems to optimize, i.e. modify, the dynamic range of an analog signal for digitizing. It is one of two versions of the G.711 standard from ITU-T, the other version being the similar μ-law, used in North America and Japan.
For a given input 
  
    
      
        x
      
    
    {\displaystyle x}
  , the equation for A-law encoding is as follows,

where 
  
    
      
        A
      
    
    {\displaystyle A}
   is the compression parameter. In Europe, 
  
    
      
        A
        =
        87.6
      
    
    {\displaystyle A=87.6}
  .
A-law expansion is given by the inverse function,

The reason for this encoding is that the wide dynamic range of speech does not lend itself well to efficient linear digital encoding. A-law encoding effectively reduces the dynamic range of the signal, thereby increasing the coding efficiency and resulting in a signal-to-distortion ratio that is superior to that obtained by linear encoding for a given number of bits.

",0
Business Administration,Fayolism,"Henri Fayol (29 July 1841 – 19 November 1925) was a French mining engineer, mining executive, author and director of mines who developed a general theory of business administration that is often called Fayolism. He and his colleagues developed this theory independently of scientific management but roughly contemporaneously. Like his contemporary Frederick Winslow Taylor, he is widely acknowledged as a founder of modern management methods.

",0
Business Administration,Feedforward (management),"Feed forward in  management theory is an application of the cybernetic concept of feedforward first articulated by I. A. Richards in 1951. It reflects the impact of Management cybernetics in the general area of management studies.
It refers to the practice of giving a control impact in a downlink to a subordinate to a person or an organization from which you are expecting an output. A feed forward is not just a pre-feedback, as a feedback is always based on measuring an output and sending respective feedback. A pre-feedback given without measurement of output may be understood as a confirmation or just an acknowledgment of control command.
However, a feed forward is generally imposed before any willful change in output may occur. All other changes of output determined with feedback may for example result from distortion, noise or attenuation. It usually involves giving a document for review and giving an ex post information on that document which you have not already given.
However, social feedback is the response of the supreme hierarch to the subordinate as an acknowledgement of a subordinate's report on output, hence the subordinate's feedback to the supreme.",0
Business Administration,Feminine style of management,"The feminine style of management is a management style generally characterized by more feminine quality soft skills and behaviors such as empathy, effective communication, and a generally more democratic or team-styled work environment. The style is a growing trend within businesses and is characterized by a form of transformational leadership style. The feminine style of management, although characterized by traits commonly labeled as feminine, it is not a style of management that is only used by females; it is also a style which has been found beneficial for particular types of businesses and organizations.",0
Business Administration,Foresight (psychology),"Foresight is the ability to predict, or the action of predicting, what will happen or what is needed in the future. Studies suggest that much of human daily thought is directed towards potential future events. Because of this and its role in human control on the planet, the nature and evolution of foresight is an important topic in psychology.  Thinking about the future is also studied under the label prospection. Recent neuroscientific, developmental, and cognitive studies have identified many commonalities to the human ability to recall past episodes. Science magazine selected new evidence for such commonalities one of the top ten scientific breakthroughs of 2007. However, there are fundamental differences between mentally travelling through time into the future (i.e., foresight) versus mentally travelling through time into the past (i.e., episodic memory).

",0
Business Administration,Full Range Leadership Model,"The Full Range of Leadership Model (FRLM) is a general leadership theory focusing on the behavior of leaders towards the workforce in different work situations. The FRLM relates transactional and transformational leadership styles with laissez-faire leadership style.The concepts of three distinct leadership styles — transactional, transformational, and laissez-faire — were introduced in 1991 by Bruce Avolio and Bernard Bass

",0
Business Administration,Halil Umut Meler,"Halil Umut Meler (born 1 August 1986) is a Turkish football referee. He has been FIFA listed since 2017 and a member of the UEFA Elite since 2022. He has officiated in 2017–18 UEFA Europa League, beginning with the match between Vojvodina and Ružomberok on 29 June 2017.",0
Business Administration,Hands-on management,"Hands-on management is a particular style of management where the manager or person in charge is particularly active in day-to-day business and leadership. It is not to be confused with micromanagement and is seen as the opposite of Laissez-faire management style.
Hands-on is considered a great manager behavior and includes traits and actions such as:
Understanding of the business and shows interest
Informed but passive with ideas
Follows up on agreed decisionsThe opposite to hands-on is a hands-off manager or management style.",0
Business Administration,Incremental profit,"Incremental profit is the profit gain or loss associated with a given managerial decision. Total profit increases so long as incremental profit is positive. When incremental profit is negative, total profit declines. Similarly, incremental profit is positive (and total profit increases) if the incremental revenue associated with a decision exceeds the incremental cost. The incremental concept is so intuitively obvious that it is easy to overlook both its significance in managerial decision making and the potential for difficulty in correctly applying it.
For this reason, the incremental concept is sometimes violated in practice. For example, a firm may refuse to sublet excess warehouse space for $5000 per month because it figures its cost as $7500 per month -a price paid for a long-term lease on the facility. However, if the warehouse space represents excess capacity with no current value to the company, its historical cost of $7500 per month is irrelevant and should be disregarded. The firm would forego $5000 in profits by turning down the offer to sublet the excess warehouse space. Similarly, any firm that adds a standard allocated charge for fixed costs and overhead to the true incremental cost of production runs the risk of turning down profitable business.",0
Business Administration,Indian Ethos in Management,"Indian Ethos in Management refers to the values and practices that the culture of India (Bharatheeya Sanskriti) can contribute to service, leadership and management. These values and practices are rooted in Sanathana Dharma (the eternal essence), and have been influenced by various strands of Indian philosophy.",0
Business Administration,Innovation leadership,"Innovation leadership is a philosophy and technique that combines different leadership styles to influence employees to produce creative ideas, products, and services. The key role in the practice of innovation leadership is the innovation leader. Dr. David Gliddon (2006) developed the competency model of innovation leaders and established the concept of innovation leadership at Penn State University.
As an approach to organization development, innovation leadership can support achievement of the mission or the vision of an organization or group. With new technologies and processes, it is necessary for organizations to think innovatively to ensure continued success and stay competitive. to adapt to new changes, “The need for innovation in organizations has resulted in a new focus on the role of leaders in shaping the nature and success of creative efforts.” Without innovation leadership, organizations are likely to struggle. This new call for innovation represents the shift from the 20th century, traditional view of organizational practices, which discouraged employee innovative behaviors, to the 21st-century view of valuing innovative thinking as a “potentially powerful influence on organizational performance.”",0
Business Administration,Instruction creep,"Instruction creep or rule creep occurs when instructions or rules accumulate over time until they are unmanageable or inappropriate.  It is a type of scope creep.  The accumulation of bureaucratic requirements results in overly complex procedures that are often misunderstood, irritating, time-wasting, or ignored. 
Instruction creep is common in complex organizations, where rules and guidelines are created by changing groups of people over extended periods of time.  The constant state of flux in such groups often leads them to add or modify instructions, rather than simplifying, consolidating, or generalizing existing ones.  This can result in a loss of clarity, efficiency, and communication, or even of consistency.  Alternatives to instruction creep include applying the KISS principle, articulating general principles rather than specific rules, and trusting people to use their best judgment.
The fundamental fallacy of instruction creep is believing that people read instructions with the same level of attention and comprehension, regardless of the volume or complexity of those instructions. A byproduct is the advent of many new rules having the deliberate intent to control others via fiat, without considering consensus or collaboration. This tends to antagonize others, even when it appears to the instigators that they are acting with proper intent.

",0
Business Administration,Integrated Management Concept,"The Integrated Management Concept, or IMC is an approach to structure management challenges by applying a ""system-theoretical perspective that sees organisations as complex systems consisting of sub-systems, interrelations, and functions"".  The most characteristic aspect of the IMC is its distinction between three particular management dimensions: normative, strategic, and operational management, which are held together by different integration mechanisms.  The normative management dimension determines the general aim of the organization, the strategic dimension directs the plans, basic structures, systems, and the problem-solving behaviour of the staff for achieving it, and the operative level translates the normative missions and strategic programs into day-to-day organizational processes. 
The IMC was developed by Knut Bleicher and his colleagues originally as an element of the St. Gallen Management Model, introduced in the 1970s by Hans Ulrich and Walter Krieg at the Swiss University of St. Gallen. Thereafter, the IMC has been revised several times (e.g. with respect to its application within SMEs sectors ) and further developed by research institutions and management scholars, such as Johannes Rüegg-Stürm.",0
Business Administration,Japanese management culture,"Japanese management culture refers to working philosophies or methods in Japan. It included concepts and philosophies such as just in time, kaizen and total quality management.",0
Business Administration,Joy's law (management),"In management, Joy's law is the principle that ""no matter who you are, most of the smartest people work for someone else,” attributed to Sun Microsystems co-founder Bill Joy. Joy was prompted to state this observation through his dislike of Bill Gates' view of ""Microsoft as an IQ monopolist."" He argued that, instead, ""It's better to create an ecology that gets all the world’s smartest people toiling in your garden for your goals. If you rely solely on your own employees, you’ll never solve all your customers' needs."" Core to this principle is the definition of smart within the context of the quotation. Smart ""refers to capability but not willingness to work for someone."" Furthermore, ""the fact that you are smart for one company does not make you smart for another."" Richard Pettinger, Director of Information Management for Business, UCL   The law highlights an essential problem that is faced by many modern businesses, ""that in any given sphere of activity most of the pertinent knowledge will reside outside the boundaries of any one organization, and the central challenge [is] to find ways to access that knowledge.""In computing, the same Bill Joy devised a simple mathematical function regarding the increase in microprocessor speed over time which is also referred to as Joy's Law.",0
Business Administration,Kiss up kick down,"Kiss up kick down (or suck up kick down) is a neologism used to describe the situation where middle-level employees in an organization are polite and flattering to superiors but abusive to subordinates. The term is believed to have originated in the US, with the first documented use having occurred in 1993. A similar expression (lit. ""lick up, kick down"") was used by Swedish punk band Ebba Grön in one of their songs, on an album released in 1981. The concept can be applied to any social interaction where one person believes they have power over another person and believes that another person has power over them.

",0
Business Administration,Lean thinking,"Lean thinking is a transformational framework that aims to provide a new way to think about how to organize human activities to deliver more benefits to society and value to individuals while eliminating waste. The term “lean thinking” was coined by James P. Womack and Daniel T. Jones to capture the essence of their in-depth study of Toyota's fabled Toyota Production System. Lean thinking is a way of thinking about an activity and seeing the waste inadvertently generated by the way the process is organized. It uses the concepts of:

Value
Value streams
Flow
Pull
PerfectionThe aim of lean thinking is to create a lean culture, one that sustains growth by aligning customer satisfaction with employee satisfaction, and that offers innovative products or services profitably while minimizing unnecessary over-costs to customers, suppliers and the environment. The basic insight of lean thinking is that if you train every person to identify wasted time and effort in their own job and to better work together to improve processes by eliminating such waste, the resulting culture (basic thinking, mindset & assumptions) will deliver more value at less expense while developing every employee's confidence, competence and ability to work with others.
The idea of lean thinking gained popularity in the business world and has evolved in three different directions:

Lean thinking converts who keep seeking to understand how to seek dynamic gains rather than static efficiencies. For this group of thinkers, lean thinking continuously evolves as they seek to better understand the possibilities of the way opened up by Toyota and have grasped the fact that the aim of continuous improvement is continuous improvement. Lean thinking as such is a movement of practitioners and writers who experiment and learn in different industries and conditions, to lean think any new activity.
Lean production adepts who have interpreted the term “lean” as a form of operational excellence and have turned to company programs aimed at taking costs out of processes. Lean activities are used to improve processes without ever challenging the underlying thinking, with powerful low-hanging fruit results but little hope of transforming the enterprise as a whole. This “corporate lean” approach is fundamentally opposed to the ideals of lean thinking, but has been taken up by a great number of large businesses seeking to cut their costs without challenging their fundamental management assumptions.
Lean Services,  as an extent area of application to all the learnings gathered from the industry and adapted to a whole new set of scenarios, such as HR, Accounting,  Retail,  Health,  Education, Product Development, Startup/ Entrepreneurship, Digital... among so many other areas. Lean basic principles can be applied basically to all scopes of action,  disregarding its geography and culture.

",0
Business Administration,Likert's management systems,"Likert's management systems are management styles developed by Rensis Likert in the 1960s. He outlined four systems of management to describe the relationship, involvement, and roles of managers and subordinates in industrial settings. He based the systems on studies of highly productive supervisors and their team members of an American Insurance Company. Later, he and Jane G. Likert revised the systems to apply to educational settings. They initially intended to spell out the roles of principals, students, and teachers; eventually others such as superintendents, administrators, and parents were included. The management systems, established by Likert, include ""Exploitative Authoritative (System I), Benevolent Authoritative (System II), Consultative (System III), and Participative (System IV).""",0
Business Administration,Machiavellianism in the workplace,"Machiavellianism in the workplace is a concept studied by many organizational psychologists. Conceptualized originally by Richard Christie and Florence Geis, Machiavellianism refers to a psychological trait concept where individuals behave in a cold and duplicitous manner. It has in recent times been adapted and applied to the context of the workplace and organizations by many writers and academics. 
Oliver James wrote on the effects of Machiavellianism and other dark triad personality traits in the workplace, the others being narcissism and psychopathy.A new model of Machiavellianism based in organizational settings consists of three factors:
maintaining power
harsh management tactics
manipulative behaviors.Examples of what Machiavellianism could look like in the workplace:
Theft (tangible or intangible)
Lying/Deceit
Sabotage
Cheating (passive or active)High Machs can exhibit high levels of charisma, and their leadership can be beneficial in some areas.The presence of Machiavellianism in an organization has been positively correlated with counterproductive workplace behaviour and workplace deviance.The origin of Machiavellianism entering the workplace can be tied to multiple factors, such as distrust towards others, pessimism, survival/self-protection tactics, or even the gender of involved parties.

",0
Business Administration,Macromanagement,"Macromanagement is a management theory with two different approaches to the definition that both share a common idea; management from afar. 
Contrary to micromanagement where managers closely observe and control the works of their employees, macromanagement is a more independent style of organizational management. Managers step back and give employees the freedom to do their job how they think it is best done, so long as the desired result is reached. This is the most commonly applied understanding of macromanagement. 
Both styles of management are viewed as a negative when taken to an extreme, so it is important for organizations to develop a balance of micro- and macromanagement practices and understand when to apply which. 
The second interpretation of macromanagement is when an organization views itself as a social institution, orienting its goals and purpose toward serving society. To do this, they align the organization’s values, norms, ethics with those of the society they are immersed in. In 1971, Alan Wells defined a social institution as “patterns of rules, customs, norms, beliefs and roles that are instrumentally related to the needs and purposes of society.” Other examples of social institutions in this respect include government and religious organizations, some more in-line with serving society that others.  
This interpretation of macromanagement is less about managing employees, but rather managing the organization from a broader perspective that is oriented toward the future. An organization that practices macromanagement greatly considers the future of the organization, the future of society, and their impact on one another.

",0
Business Administration,Magang Constitution,"The Magang Constitution (simplified Chinese: 马钢宪法; traditional Chinese: 馬鋼憲法; pinyin: Mǎ gāng xiànfǎ), also known as Ma-steel Constitution, was a set of enterprise management system that was gradually formed in the Former Soviet Union in the 1950s and 1960s after decades of socialist industrial construction and development. Nowadays, it has been abandoned.
It is worth noting that ""Magang Constitution"" is a vivid metaphor, it is not really a constitution in the true sense of the term (like China's ""Angang Constitution"").  It is a complete set of rules and regulations for factory management, even rising to the height of the law.Magang Constitution is not the fundamental law of the FSU, nor the most basic law of the country. In addition, the name ""Magang Constitution"" was not named by the Soviets, but by the Chinese.",0
Business Administration,Management buy-in,"A management buy-in (MBI) occurs when a manager or a management team from outside the company raises the necessary finance, buys it, and becomes the company's new management. A management buy-in team often competes with other purchasers in the search for a suitable business. Usually, the team will be led by a manager with significant experience at managing director level.
The difference to a management buy-out is in the position of the purchaser: in the case of a buy-out, they are already working for the company. In the case of a buy-in, however, the manager or management team is from another source.

",0
Business Administration,Management buyout,"A management buyout (MBO) is a form of acquisition in which a company's existing managers acquire a large part, or all, of the company, whether from a parent company or non-artificial person(s). Management-, and/or leveraged buyout  became noted phenomena of 1980s business economics. These so-called MBOs originated in the US, spreading first to the UK and then throughout the rest of Europe. The venture capital industry has played a crucial role in the development of buyouts in Europe, especially in smaller deals in the UK, the Netherlands, and France.",0
Business Administration,Management due diligence,"Management due diligence is the process of appraising a company's senior management—evaluating each individual's effectiveness in contributing to the organization's strategic objectives.Assessing company management is crucial when closing business deals. It can mean the difference between long-term success or sudden failure. It also helps the organisation understand how the teams perform their roles in context with the company's future business plan. This helps clarify the structure of the organisation's work-force.
The management due diligence process can be identified as an informative tool for external stakeholders, and can also be referred to as Management Assessment as it addresses the team’s dynamics and highlight the risks.",0
Business Administration,Management entrenchment,"Management is a type of labor with a special role of coordinating the activities of inputs and carrying out the contracts agreed among inputs, all of which can be characterized as ""decision making"". Managers usually face disciplinary forces by making themselves irreplaceable in a way that the company would lose without them. A manager has an incentive to invest the firm's resources in assets whose value is higher under him than under the best alternative manager, even when such investments are not value-maximizing.",0
Business Administration,Management process,"Management process is a process of setting goals, planning and/or controlling the organising and leading the execution of any type of activity, such as: 

a project (project management process) or
a process (process management process, sometimes referred to as the process performance measurement and management system).An organization's senior management is responsible for carrying out its management process. However, this is not always the case for all management processes, for example, sometimes it is the responsibility of the project manager to carry out a project management process.

",0
Business Administration,Management science,"Management science or Managerial Science (MS) is the broad interdisciplinary study of problem solving and decision making in human organizations, with strong links to management, economics, business, engineering, management consulting, and other fields. It uses various scientific research-based principles, strategies, and analytical methods including  mathematical modeling, statistics and numerical algorithms to improve an organization's ability to enact rational and accurate management decisions by arriving at optimal or near optimal solutions to complex decision problems.  Management science helps businesses to achieve goals using various scientific methods.
The field was initially an outgrowth of applied mathematics, where early challenges were problems relating to the optimization of systems which could be modeled linearly, i.e., determining the optima (maximum value of profit, assembly line performance, crop yield, bandwidth, etc. or minimum of loss, risk, costs, etc.) of some objective function. Today, management science encompasses any organizational activity for which a problem is structured in mathematical form to generate managerially relevant insights.",0
Business Administration,Managerial economics,"Managerial economics is a branch of economics involving the application of economic methods in the managerial decision-making process. Economics is the study of the production, distribution and consumption of goods and services. Managerial economics involves the use of economic theories and principles to make decisions regarding the allocation of scarce resources.Managers use economic frameworks in order to optimise profits, resource allocation and the overall output of the firm, whilst improving efficiency and minimising unproductive activities. These frameworks assist organisations to make rational, progressive decisions, by analysing practical problems at both micro and macroeconomic levels. Managerial decisions involve forecasting (making decisions about the future), which involve levels of risk and uncertainty, however, the assistance of managerial economic techniques aid in informing managers in these decisions.The two main purposes of managerial economics are:

To optimize decision making when the firm is faced with problems or obstacles, with the consideration and application of macro and microeconomic theories and principles.
To analyze the possible effects and implications of both short and long-term planning decisions on the revenue and profitability of the Business.The core principles that managerial economist use to achieve the above purposes are; monitoring operations management and performance, target or goal setting and talent management and development.
In order to optimize economic decisions, the use of operations research, mathematical programming, strategic decision making, game theory and other computational methods are often involved. The methods listed above are typically used for making quantitate decisions by data analysis techniques.
The theory of Managerial Economics includes a focus on; incentives, business organization, biases, advertising, innovation, uncertainty, pricing, analytics, and competition. In other words, managerial economics is a combination of economics and managerial theory.  It helps the manager in decision-making and acts as a link between practice and theory.
Furthermore, managerial economics provides the device and techniques for managers to make the best possible decisions for any scenario.
Some examples of the types of problems that the tools provided by managerial economics can answer are:

The price and quantity of a good or service that a business should produce.
Whether to invest in training current staff or to look into the market.
When to purchase or retire fleet equipment.
Decisions regarding understanding the competition between two firms based on the motive of profit maximization.
The impacts of consumer and competitor incentives plan on business decisionsManagerial economics is sometimes referred to as business economics and is a branch of economics that applies microeconomic analysis to decision methods of businesses or other management units to assist managers to make a wide array of multifaceted decisions. The calculation and quantitative analysis draws heavily from techniques such as regression analysis, correlation and calculus.",0
Business Administration,Managerial finance,"Managerial finance is the branch of finance that concerns itself with the managerial application of finance techniques and theory, 

emphasizing the financial aspects of managerial decisions.
The techniques addressed are drawn in the main from managerial accounting and corporate finance; 
the former allow management to better understand, and hence act on, financial information relating to profitability and performance; the latter are about optimizing the overall financial-structure.
The discipline is somewhat academic in nature, and ""is concerned more with the assessment of financial techniques versus the financial techniques themselves""; 
its emphasis though, is managerial as opposed to technical.
Putting the techniques into practice -  i.e. performing financial management - entails strategic planning, organizing, directing, and controlling of the organization's financial undertakings; see Financial analyst § Corporate and other and Financial management § Role.
Correspondingly, the discipline will assess the various techniques from the perspectives of Planning, Directing, and Controlling.",0
Business Administration,Managerial hubris,"Managerial hubris is the unrealistic belief held by managers in bidding firms that they can manage the assets of a target firm more efficiently than the target firm's current management. 
Managerial hubris is one reason a manager may choose to invest in a merger that on average generates no profits.

",0
Business Administration,Managerial prerogative,"Managerial prerogatives are also referred to as the functions and rights of management,  is considered as the discretion of the employer or manager on how to manage its business, not bound by collective bargaining. It is a term that easily leads to widespread misunderstanding. Different circles have different interpretations of this term. When it is used in the trade unions circles, is perceived as a user's support for unilateral management power and can cause protests.When used by the management circle, It is considered as exclusive right and control right without interference.  Managerial prerogatives give employers or managers the power to control the direction in which their businesses are heading. Employees basically do not have this power. 

",0
Business Administration,Managerial psychology,"Managerial psychology is a sub-discipline of industrial and organizational psychology that focuses on the effectiveness of individuals and groups in the workplace, using behavioral science.
The purpose of managerial psychology is to aid managers in gaining a better managerial and personal understanding of the psychological patterns common among these individuals and groups.

Managers can use managerial psychology to predict and prevent harmful psychological patterns within the workplace and to control psychological patterns to benefit the organisation long term.
Managerial psychologists help managers, through research in theory, practice, methods and tools, to achieve better decision-making, leadership practices and development, problem solving and improve overall human relations.

",0
Business Administration,Managerialism,"Managerialism involves belief in the value of professional managers and of the concepts and methods they use. Contemporary writers on management such as Thomas Diefenbach associate managerialism with hierarchy. But scholars have also linked managerialism to control,
to accountability
and measurement, and to an ideologically determined belief in the importance of tightly-managed organizations,
as opposed to individuals or to groups that do not resemble an organization.
Following Enteman's 1993 classic on Managerialism: The Emergence of a New Ideology,
American management experts Robert R. Locke and J. C. Spender see managerialism as an expression of a special group – management – that entrenches itself ruthlessly and systemically in an organization.  It deprives owners of decision-making power and workers of their ability to resist managerialism. In fact the rise of managerialism may in itself be a response to people's resistance in society and more specifically to workers' opposition against managerial regimes. Enteman (1993), Locke and Spender (2011) and Klikauer (2013) explain Managerialism in three different ways:

Building on Enteman (1993) and Locke/Spender (2011), Thomas Klikauer in “Managerialism – Critique of an Ideology” (2013) defined managerialism thus:  ""[....] Managerialism combines management knowledge and ideology to establish itself systemically in organisations and society while depriving owners, employees (organisational-economical) and civil society (social-political) of all decision-making powers. Managerialism justifies the application of managerial techniques to all areas of society on the grounds of superior ideology, expert training, and the exclusive possession of managerial knowledge necessary to efficiently run corporations and societies.""As the simpler yet already highly organised management of Henri Fayol (1841-1925) and Frederick Winslow Taylor (1856-1915) mutated into managerialism, managerialism became a full-fledged ideology under the following formula:Management + Ideology + Expansion = Managerialism [1]
Two examples of the extension of management into the non-management domain – the not for profit sphere of human existence – are public schools and universities. [2] In both cases, managerialism occurs when public institutions are run “as if” these were for-profit organization even though they remain government institutions funded through state taxes. In these cases, the term new public management has been used. [3] But the ideology of managerialism can even extend into more distant institutions such as, for example, a college of physicians.[4]
Albert A. Anderson summarized  managerialism as the ideological principle that sees societies as equivalent to the sum of the decisions and transactions made by the managements of organizations.Compare what the historian James Hoopes wrote (2003):

""[...] the main genesis of managerialism lay in the human relations movement that took root at the Harvard Business School in the 1920s and 1930s under the guiding hand of Professor Elton Mayo. Mayo, an immigrant from Australia, saw democracy as divisive and lacking in community spirit. He looked to corporate managers to restore the social harmony that he believed the uprooting experiences of immigration and industrialization had destroyed and that democracy was incapable of repairing.""",0
Business Administration,Managing by wire,"Managing by wire is a management strategy in which managers rely on their company's ""information representation"" generated by computers such as databases and software instead of on detailed commands.
It was presented by Stephan H. Haeckel and Richard L. Nolan in a 1993 Harvard Business Review article. The authors chose the term ""managing by wire"" as an analogue to the fly-by-wire concept for jets. SAP SE, Aetna, Mrs. Fields Cookies, and Brooklyn Union Gas have done ""managing by wire"".",0
Business Administration,Managing up and managing down,"Managing Up and Managing Down is a part of management that details how middle managers or supervisors should effectively deal with their managers and subordinates. Promotion to management comes with additional responsibility of managing down. With the additional responsibility for managing their team while remaining accountable to their management teams, managers require additional skills and training to effectively influence up or down. Management levels within large organizations are structured from a hierarchal organization and include senior, middle, and lower management roles..",0
Business Administration,MECE principle,"The MECE principle, (mutually exclusive and collectively exhaustive) pronounced by many as ""ME-see"", and pronounced by the author as ""Meese"" like Greece or niece, is a grouping principle for separating a set of items into subsets that are mutually exclusive (ME) and collectively exhaustive (CE). It was developed in the late 1960s by Barbara Minto at McKinsey & Company and underlies her Minto Pyramid Principle, and while she takes credit for MECE, according to her interview with McKinsey, she says the idea for MECE goes back as far as to Aristotle.The MECE principle has been used in the business mapping process wherein the optimum arrangement of information is exhaustive and does not double count at any level of the hierarchy. Examples of MECE arrangements include categorizing people by year of birth (assuming all years are known), apartments by their building number, letters by postmark, and dice rolls. A non-MECE example would be categorization by nationality, because nationalities are neither mutually exclusive (some people have dual nationality) nor collectively exhaustive (some people have none).

",0
Business Administration,Micromanagement,"In business management, micromanagement is a management style whereby a manager closely observes and/or controls and/or reminds the work of their subordinates or employees.
Micromanagement is generally considered to have a negative connotation, mainly because it shows a lack of freedom and trust in the workplace.

",0
Business Administration,Multidimensional organization,"A multidimensional organization is an organization that pursues its objectives simultaneously through multiple dimensions (product, region, account, market segment).
The multidimensional organization was discussed as early as the 1970s. It required the combination of the fall of costs of information, the development of dynamic multidimensional markets, and a new generation of workers and managers, to create this paradigm shift in organization forms.",0
Business Administration,Mushroom management,"Mushroom management is the management of a company where the communication channels between the managers and the employees do not work effectively, and where employees are 'kept in the dark' by management in regards to business decisions that affect their work and employment. The term 'mushroom management' alludes to the stereotypical (and somewhat inaccurate) view of mushroom cultivation: kept in the dark and fed bullshit.

",0
Business Administration,Organizational conflict,"Organizational conflict, or workplace conflict, is a state of discord caused by the actual or perceived opposition of needs, values and interests between people working together. Conflict takes many forms in organizations. There is the inevitable clash between formal authority and power and those individuals and groups affected. There are disputes over how revenues should be divided, how the work should be done, and how long and hard people should work. There are jurisdictional disagreements among individuals, departments, and between unions and management. There are subtler forms of conflict involving rivalries, jealousies, personality clashes, role definitions, and struggles for power and favor. There is also conflict within individuals – between competing needs and demands – to which individuals respond in different ways.

",0
Business Administration,Organizing (management),"Organizating or organising is the establishment of effective authority relationships among selected works, persons and work places in order for the group to work together efficiently, or the  process of dividing work into sections and departments.",0
Business Administration,OSTO System Model,"The OSTO System Model is based on the OSTO System Theory, which comprehends complex systems and organizations as living systems and maps these by means of the OSTO System Model. The model is cybernetic in nature and is deduced from the theory of closed loops. The basics of this theory have been formulated by David P. Hanna in the 1980s and have been published initially in 1988. The model assumes that several central transformation processes take place on the inside of a complex organization. These are deeply influenced by mutual reactions between the inner life of the organization and the outside (environment). In terms of closed loop theory, the OSTO System Model depicts the essential elements of such a living system in its interconnectedness, dependencies, and reciprocal reactions. Thinking in network structures is, thus, a crucial part of the OSTO System Theory.
The acronym “OSTO” stands for open, sociotechnical, economic (German: “oekonomisch”)  aspects of a system. With regard to organizations and economically working companies, the model takes into consideration the openness of systems towards their environments as well as the fact that they are multidimensional, socio-techno-economic structures. Taking into consideration these four aspects, the model displays the complexity of such a system in its numerous dimensions.",0
Business Administration,Plan,"A plan is typically any diagram or list of steps with details of timing and resources, used to achieve an objective to do something. It is commonly understood as a temporal set of intended actions through which one expects to achieve a goal.
For spatial or planar topologic or topographic sets see map.
Plans can be formal or informal:

Structured and formal plans, used by multiple people, are more likely to occur in projects, diplomacy, careers, economic development, military campaigns, combat, sports, games, or in the conduct of other business. In most cases, the absence of a well-laid plan can have adverse effects: for example, a non-robust project plan can cost the organization time and money.
Informal or ad hoc plans are created by individuals in all of their pursuits.The most popular ways to describe plans are by their breadth, time frame, and specificity; however, these planning classifications are not independent of one another. For instance, there is a close relationship between the short- and long-term categories and the strategic and operational categories.
It is common for less formal plans to be created as abstract ideas, and remain in that form as they are maintained and put to use. More formal plans as used for business and military purposes, while initially created with and as an abstract thought, are likely to be written down, drawn up or otherwise stored in a form that is accessible to multiple people across time and space. This allows more reliable collaboration in the execution of the plan.",0
Business Administration,Power structure,"In political sociology, but also operative within the rest of the animal kingdom, a power structure is a hierarchy of competence or aggression (might) predicated on influence between an individual and other entities in a group. A power structure focuses on the way power and authority is related between people within groups such as a government, nation, institution, organization, or a society. Such structures are of interest to various fields, including sociology, government, economics, and business. A power structure may be formal and intentionally constructed to maximize values like fairness or efficiency, as in a hierarchical organization wherein every entity, except one, is subordinate to a single other entity. Conversely, a power structure may be an informal set of roles, such as those found in a dominance hierarchy in which members of a social group interact, often aggressively, to create a ranking system. A culture that is organised in a dominance hierarchy is a dominator culture, the opposite of an egalitarian culture of partnership. A visible, dominant group or elite that holds power or authority within a power structure is often referred to as being the Establishment. Power structures are fluid, with changes occurring constantly, either slowly or rapidly, evolving or revolutionary, peacefully or violently.",0
Business Administration,Power to the edge,"Power to the edge refers to the ability of an organization to dynamically synchronize its actions; achieve command and control (C2) agility; and increase the speed of command over a robust, networked grid. The term is most commonly used in relation to military organizations, but it can equally be used in a civilian context.
""Power to the edge"" is an information and organization management philosophy first articulated by the U.S. Department of Defense in a publication by Dr. David S. Alberts and Richard E. Hayes in 2003 titled: ""Power to the Edge: Command...Control...in the Information Age."" This book was published by the Command and Control Research Program and can be downloaded from the Program's website.",0
Business Administration,Scaling of innovations,"Scaling of innovations is a process that leads to widespread use of an innovation. It is regarded the last step after the discovery, proof of concept and piloting of an innovation. In business it is often used as maximizing operational scale of the product. This technology, or project-focused scaling takes products and services as the point of departure and wants to see those to go scale. In the public sector, and for example in development aid, the desired impact is the point of departure and whatever leads to more impact is scaled (usually in the form of a range of innovations). However, some authors recognize that the public sector often uses the business way of scaling to reach impact, leading to disillusionment and doing more harm than good. Sometimes, scaling is seen as a process towards sustainable systems change at scale, where sustainability, systems change and responsible scaling are just as important as “reaching many”.",0
Business Administration,Seagull management,"Seagull management is a management style wherein a manager only interacts with employees when they deem a problem has arisen. The perception is that such a management style involves hasty decisions about things of which they have little understanding, resulting in a messy situation with which others must deal.The term became popular through a joke in Ken Blanchard’s 1985 book Leadership and the One Minute Manager: “Seagull managers fly in, make a lot of noise, dump on everyone, then fly out.”",0
Business Administration,Sense and respond,"Sense and respond has been used in control theory for several decades, primarily in closed systems such as refineries where comparisons are made between measurements and desired values, and system settings are adjusted to narrow the gap between the two. Since  the early 1980s, sense and respond has also been used to describe the behavior of certain open systems. 
Sense and respond is based on lean principles and follows URSLIMM:

U - understand customer value
R - remove waste
S - standardize
L - learn by doing
I - involve everyone
M - measure what matters
M - manage performance visuallyThe term ""sense and respond"" as a business concept was used in a 1992 American Management Association Management Review article by Stephan H. Haeckel.  It was developed by Haeckel at IBM’s Advanced Business Institute.",0
Business Administration,Smiling curve,"In business management theory, the smiling curve is a graphical depiction of how value added varies across the different stages of bringing a product on to the market in an IT-related manufacturing industry. The concept was first proposed around 1992 by Stan Shih, the founder of Acer Inc., an IT company headquartered in Taiwan. According to Shih's observation, in the personal computer industry, the two ends of the value chain – conception and marketing – command higher values added to the product than the middle part of the value chain – manufacturing. If this phenomenon is presented in a graph with a Y-axis for value-added and an X-axis for value chain (stage of production), the resulting curve appears like a ""smile"".
Based on this model, the Acer company adopted a business strategy to reorient itself from manufacturing into global marketing of brand-name PC-related products and services. Acer accordingly invested heavily in R&D to develop innovative technology. The concept later became widely cited to describe the distribution of value-adding potentials in other types of industry to justify business strategies aimed at higher value-adding activities.",0
Business Administration,Social entrepreneurship,"Social entrepreneurship is an approach by individuals, groups, start-up companies or entrepreneurs, in which they develop, fund and implement solutions to social, cultural, or environmental issues. This concept may be applied to a wide range of organizations, which vary in size, aims, and beliefs. For-profit entrepreneurs typically measure performance using business metrics like profit, revenues and increases in stock prices. Social entrepreneurs, however, are either non-profits, or they blend for-profit goals with generating a positive ""return to society"". Therefore, they use different metrics. Social entrepreneurship typically attempts to further broad social, cultural and environmental goals often associated with the voluntary sector in areas such as poverty alleviation, health care and community development.
At times, profit-making social enterprises may be established to support the social or cultural goals of the organization but not as an end in themselves. For example, an organization that aims to provide housing and employment to the homeless may operate a restaurant, both to raise money and to provide employment for the homeless.
In 2010, social entrepreneurship was facilitated by the use of the Internet, particularly social networking and social media websites. These websites enable social entrepreneurs to reach numerous people who are not geographically close yet who share the same goals and encourage them to collaborate online, learn about the issues, disseminate information about the group's events and activities, and raise funds through crowdfunding.In recent years, researchers are calling for a better understanding of the ecosystem in which social entrepreneurship exists, and social ventures operate. This will help them formulate better strategy and help achieve their double bottom line objective.",0
Business Administration,Social work management,"Social work management is the management of organisations or enterprises in the social economy and non-profit sector, e.g., public service providers, charities, youth welfare offices, associations, etc. 
Social work management has been traditionally pursued by social workers, social pedagogues, pedagogues, psychologists without additional management skills and knowledge or legal practitioners and business economists – often without reference to the social economy.
Most scholars and practitioners agree that social work managers need to have a high degree of leadership skills to make considered managerial decisions, to empower social workers, to develop staff within and collaborate with partners outside the social and human service organisation. Social work management as a field of social work education and practice was established in many universities in Europe and North America  since the 1980s. Established qualifications in higher education first included diplomas in social economy. It originally focused on person-centred leadership, motivation and strategic issues. It combines management with social pedagogical, psychological, and sociological knowledge and skills.",0
Business Administration,Stakeholder approach,"In management, a stakeholder approach is the practice that managers formulate and implement processes that satisfy stakeholders' needs to ensure long-term success. According to the degree of participation of the different groups, the company can take advantage of market imperfections to create valuable opportunities. It emphasizes active management of the business environment, relationships and the promotion of shared interests. This approach is based on stakeholder theory, which arises as a counterpart to business practices and management that focus on shareholders satisfaction. The implementation of this approach can reinforce the firm values and create competitive advantage. However, it has been criticized for overvaluing stakeholders and its difficulty to reach consensus.",0
Business Administration,Stewardship theory,"Stewardship theory is a theory that managers, left on their own, will act as responsible stewards of the assets they control.Stewardship theorists assume that given a choice between self-serving behavior and pro-organizational behavior, a steward will place higher value on cooperation than defection. Stewards are assumed to be collectivists, pro-organizational, and trustworthy.In American politics, an example of the stewardship theory is where a president practices a governing style based on belief, they have the duty to do whatever is necessary in national interest, unless prohibited by the Constitution.  The Stewardship approach is often associated with Theodore Roosevelt, who viewed the Presidency as a ""Bully pulpit"" of moral and political leadership.",0
Business Administration,Success,"Success is the state or condition of meeting a defined range of expectations. It may be viewed as the opposite of failure. The criteria for success depend on context, and may be relative to a particular observer or belief system. One person might consider a success what another person considers a failure, particularly in cases of direct competition or a zero-sum game. Similarly, the degree of success or failure in a situation may be differently viewed by distinct observers or participants, such that a situation that one considers to be a success, another might consider to be a failure, a qualified success or a neutral situation. For example, a film that is a commercial failure or even a box-office bomb can go on to receive a cult following, with the initial lack of commercial success even lending a cachet of subcultural coolness.It may also be difficult or impossible to ascertain whether a situation meets criteria for success or failure due to ambiguous or ill-defined definition of those criteria. Finding useful and effective criteria, or heuristics, to judge the failure or success of a situation may itself be a significant task.",0
Business Administration,Supervision,"Supervision is an act or instance of directing, managing, or oversight.

",0
Business Administration,Target culture,"Target culture is a pejorative term used to refer to the perceived negative effects of rigid adherence to performance targets by businesses and organisations. The term is primarily used to refer to this kind of behaviour within the provision of public services in the United Kingdom. Target culture often stems from not being able to accurately measure a broad social good like health, education or crime prevention: instead, specific target like increasing the number of people passing an examination  or the number of arrests made by a police force is used.",0
Business Administration,Technostructure,"Technostructure is the group of technicians, analysts within an organisation (enterprise, administrative body) with considerable influence and control on its economy. The term was coined by the economist John Kenneth Galbraith in The New Industrial State (1967). It usually refers to managerial capitalism where the managers and other company leading administrators, scientists, or lawyers retain more power and influence than the shareholders in the decisional and directional process.",0
Business Administration,Theory of the firm,"The theory of Forms or theory of Ideas is a philosophical theory, concept, or world-view, attributed to Plato, that the physical world is not as real or true as timeless, absolute, unchangeable ideas. According to this theory, ideas in this sense, often capitalized and translated as ""Ideas"" or ""Forms"", are the non-physical essences of all things, of which objects and matter in the physical world are merely imitations. Plato speaks of these entities only through the characters (primarily Socrates) of his dialogues who sometimes suggests that these Forms are the only objects of study that can provide knowledge. The theory itself is contested from within Plato's dialogues, and it is a general point of controversy in philosophy.  Nonetheless, the theory is considered to be a classical solution to the problem of universals.The early Greek concept of form precedes attested philosophical usage and is represented by a number of words mainly having to do with vision, sight, and appearance. Plato uses these aspects of sight and appearance from the early Greek concept of the form in his dialogues to explain the Forms and the Good.",0
Business Administration,Turnaround management,"Turnaround management is a process dedicated to corporate renewal. It uses analysis and planning to save troubled companies and return them to solvency, and to identify the reasons for failing performance in the market, and rectify them. Turnaround management involves management review, root failure causes analysis, and SWOT analysis to determine why the company is failing. Once analysis is completed, a long term strategic plan and restructuring plan are created. These plans may or may not involve a bankruptcy filing. Once approved, turnaround professionals begin to implement the plan, continually reviewing its progress and make changes to the plan as needed to ensure the company returns to solvency.",0
Business Administration,Upper echelons theory,"The upper echelons theory is a management theory published by Donald C. Hambrick and P. Mason in 1984.
It states that organizational outcomes are partially predicted by managerial background characteristics of the top level management team.

",0
Business Administration,Workers' control,"Workers' control is participation in the management of factories and other commercial enterprises by the people who work there.  It has been variously advocated by anarchists, socialists, communists, social democrats, distributists and Christian democrats, and has been combined with various socialist and mixed economy systems.
Workers' councils are a form of workers' control. Council communism, such as in the early Soviet Union, advocates workers' control through workers' councils and factory committees. Syndicalism advocates workers' control through trade unions. Guild socialism advocates workers' control through a revival of the guild system. Participatory economics represents a recent variation on the idea of workers' control.
Workers' control can be contrasted to control of the economy via the state, such as nationalization and central planning (see state socialism) versus control of the means of production by owners, which workers can achieve through employer provided stock purchases, direct stock purchases, etc., as found in capitalism.",0
Business Administration,Association of Consulting Engineers New Zealand,"The Association of Consulting Engineers New Zealand (ACENZ) is New Zealand's main business association representing engineers providing consultancy services in a wide range of disciplines. It was founded in 1959 as the consulting division of IPENZ, though it has been a separate entity since 1970.It has 176 corporate members with a total of around 8,500 staff (2007 data), up from about 5,800 in 2001.Apart from its functions as a representative of the interests of its member companies, it annually judges engineering awards for the most innovative and exceptional engineering projects of New Zealand.",0
Business Administration,Body shopping,"Body shopping is the practice of consultancy firms recruiting workers (generally in the information technology sector) in order to contract their services out on a tactical short- to mid-term basis. IT services companies that practice body shopping assert that they provide real services (such as software development) rather than the ""sham"" of merely farming out professionals to overseas companies.",0
Business Administration,Chemonics,"Chemonics International, Inc. is a private international development firm based in Washington, D.C. It was established in 1975 by Thurston F. (Tony) Teele as a subsidiary of Erly Industries. The employee-owned company offers a variety of services globally and with more than $1.5 billion in USAID contracts in 2019 is the largest for-profit recipient of U.S. government foreign aid. As of 2019 the company has approximately 5,000 employees in 100 countries.

",0
Business Administration,Commercialista,"A tax advisor or tax consultant is a person with advanced training and knowledge of tax law. The services of a tax advisor are usually retained in order to minimize taxation while remaining compliant with the law in complicated financial situations. Tax Advisors are also retained to represent clients before tax authorities and tax courts to resolve tax issues.

",0
Business Administration,Cumming Corporation,"Cumming is a privately held international project management and cost consulting firm with more than 950 employees worldwide and a focus on serving the education, healthcare, themed entertainment, and hospitality sectors.  In 2017, the firm generated an estimated $117 million in professional fees on more than $4 billion in construction.

",0
Business Administration,Engineering consulting,"Engineering consulting is the practice of performing engineering as a consulting engineer. It assists public and private companies with process management, idea organization, product design, fabrication, MRO (Maintenance, Repair and Operations), servicing, tech advice, tech specifications, tech estimating, costing, budgeting, valuation, branding, and marketing.Engineering consulting firms may involve Civil, Structural, Mechanical, Electrical, Environmental, Chemical, Industrial, and Agricultural, Electronics and Telecom, Computer and Network, Instrumentation and Control, IT, Manufacturing and Production, Aerospace, Marine, Fire and Safety, etc. Consulting engineers may also assist in marketing.",0
Business Administration,Expert as a service,"Expert-as-a-Service (ExaaS) is an online delivery model of human consulting and/or automated knowledge transfer. Similarly to SaaS (Software as a service) the concept of ExaaS comes from the remote nature of processing data and/or providing service. The user of ExaaS  accesses the service through a webpage, VoiP, IM, etc. or any related mobile app. Typical ExaaS providers are consultants, lawyers, physicians, teachers etc. 
Biztech Magazine claims that  ""...Everything as a service...Clouding Will Disrupt the World..."" and according to Business2Customers magazine there are ""...3 Reasons Why Experts-as-a-Service is the Future of Consulting...""ExaaS is the general term for newer consulting or expertise based online services like CaaS (Consulting-as-a-Service) or JaaS (Justice-as-a-Service) CaaS companies or individuals deliver online consulting services (The Consulting-as-a-Service model (CONaaS) by Luke Marson, sales diagnostics by Miklos Kadar, 
Introducing Executive As A Service by GconnTec )
According to BusinessInsider JaaS companies ""...Find You Money You Didn't Know You Were Owed..."" by providing legal or consumer rights services. And according to Henrik Zillmer ""...There’s a new wave of customer empowerment coming...""  
He lists honourable mentions of JaaS services like Paribus, AirHelp, Compensair, Fixed, BillFixers, 71lbs )",0
Business Administration,Grade (consulting),"In information technology consulting and management consulting, a grade aims to explicitly recognize a certain professional level, both within the organization and to customer organizations. A grade is separated from a place in the line hierarchy of a company; it underlines the very possibility of recognizing a person (top) level without being necessarily in a management functions.
The most commonly used system of grading consultants is the following:

Junior consultant or associate consultant: In some fields, an associate consultant is at the beginning of their consulting career and will typically do work to support the consultants and senior consultants - data collection and analysis, workshop support, etc. An associate consultant can also refer to a day rate contractor at any level, differentiating them from an employee of the firm (e.g. associate managing consultant). However, in engineering, urban planning. and some environmental consulting fields the term associate consultant is used differently and is typically a higher grade than principal consultant.
Consultant: A consultant is ‘learning the trade' within a specific domain of expertise. A consultant is developing in most competency dimensions and work in different roles on different projects in a specific domain.
Senior consultant: A senior consultant has developed a specialisation within a specific domain of expertise. A senior consultant is capable of working independently as well as in teams. A senior consultant is often responsible for the completion of a part of a project or activities for which he/she leads a small team. A senior consultant is more client oriented and explores sales activities.
Managing consultant: A managing consultant has started to excel in some of the competency dimensions. A managing consultant is known for domain expertise and is capable of generating his or her own work and that of others. As such the managing consultant is often responsible for business volume, through (add-on) sales and delivering a project. A managing consultant can act as a team lead or counsellor for other team members.
Senior managing consultant: A senior managing consultant has developed excellence within some of the competency dimensions. A senior managing consultant is known for domain expertise and is capable of generating his or her own work and that of others. As such the senior managing consultant is often responsible for business volume, through (add-on) sales and delivering a project. A senior managing consultant leads a team or counsellor for other teams.
Principal consultant: A principal consultant has a strong business impact and is often part of the company's leadership. A principal consultant is capable of shaping a piece of business being the leader in a specific domain and in any other domains. A principal consultant develops high-level business relations and high-impact projects. A principal is capable of leading large teams and also generates new business ideas.
Executive Principal consultant: not a real thing.",0
Business Administration,Information Control Company,"g2o, formerly Information Control Company (ICC), formerly Information Control Corporation, is headquartered in Columbus, Ohio, United States. It is the largest Ohio-owned digital experience and technology consulting company.The company employs more than 500 analysts, designers, developers, system and data engineers, researchers, and strategists.
Their client list includes financial institutions, health care organizations, government agencies, insurance companies, retailers, and educators.",0
Business Administration,Oliver Wyman,"Oliver Wyman is an American management consulting firm. Founded in New York City in 1984 by former Booz Allen Hamilton partners Alex Oliver and Bill Wyman, the firm has more than 60 offices in Europe, North America, the Middle East, and Asia-Pacific, employing over 5,000 professionals. The firm is part of the Oliver Wyman Group, a business unit of Marsh McLennan.",0
Business Administration,Palladium International,"Palladium (also known as ""The Palladium Group"", ""Palladium Holdings"" or ""Palladium International"") is an international advisory and management company representing the combination of seven prior companies: GRM International, Futures Group, Palladium, the IDL Group, Development & Training Services, HK Logistics and CARANA Corporation. As of October 2016, Palladium employs over 2,500 persons operating in 90 countries. At the end of 2015, Palladium International was the fourth-largest private sector partner for the UK Government's Department for International Development (DFID). During 2011, Palladium International members Futures Group and Carana were USAID's fourteenth and sixteenth largest private sector partners, respectively. At the end of 2012, GRM International was the third largest private sector partner for AusAID.",0
Business Administration,Peopleware,"Peopleware is a term used to refer to one of the three core aspects of computer technology, the other two being hardware and software. Peopleware can refer to anything that has to do with the role of people in the development or use of computer software and hardware systems, including such issues as developer productivity, teamwork, group dynamics, the psychology of programming, project management, organizational factors, human interface design, and human–machine interaction.",0
Business Administration,Peopleware: Productive Projects and Teams,"Peopleware: Productive Projects and Teams is a 1987 book on the social side of software development, specifically managing project teams.  It was written by software consultants Tom DeMarco and Tim Lister, from their experience in the world of software development.  This book was revised in 2013.",0
Business Administration,Performance consulting,"Performance consulting is a practice that became popular in the  early 2000s. Performance consulting is a practice that evolved from the instructional design discipline. It is performed by performance consultants who use more of a systems-thinking approach to resolving workplace performance problems. Performance consulting acknowledges that there are other environmental factors that affect one's performance. While instructional design and the development of training or learning solutions helps to build knowledge and skills, performance consulting takes a more systems-thinking approach to investigate and identify other environmental factors that may degrade one's performance.",0
Business Administration,Professional services,"Professional services are occupations in the service sector requiring special training in the arts or sciences. Some professional services, such as architects, accountants, engineers, doctors, lawyers, and teachers, require the practitioner to hold professional degrees or licenses and possess specific skills. Other professional services involve providing specialist business support to businesses of all sizes and in all sectors; this can include tax advice, supporting a company with accounting, IT services, Public Relations services or providing management advice.

",0
Business Administration,Public consultation,"Public consultation (Commonwealth countries and European Union), public comment (US),  or simply consultation, is a regulatory process by which the public's input on matters affecting them is sought. Its main goals are in improving the efficiency, transparency and public involvement in large-scale projects or laws and policies. It usually involves notification (to publicise the matter to be consulted on), consultation (a two-way flow of information and opinion exchange) as well as participation (involving interest groups in the drafting of policy or legislation). A frequently used tool for understanding different levels of community participation in consultation is known as Arnstein's ladder, although some academics contest that Arnstein's ladder is contextually specific and was not intended to be a universal tool. Ineffective consultations are considered to be cosmetic consultations that were done due to obligation or show and not true participatory decision making.
Public comment (or ""vox populi"") is a public meeting of government bodies which set aside time for public comments, usually upon documents. Such documents may either be reports such as Draft Environmental Impact Reports (DEIR's) or new regulations. There is typically a notice which is posted on the web and mailed to lists of interested parties known to the government agencies. If there is to be a change of regulations, there will be a formal notice of proposed rulemaking.
The basis for public comment is found in general political theory of constitutional democracy as originated during and after the Enlightenment, particularly by Rousseau. This basis was elaborated in the American Revolution, and various thinkers such as Benjamin Franklin, Thomas Jefferson and Thomas Paine are associated with the rejection of tyrannical, closed government decision making in favor of open government. The tradition of the New England Town Hall is believed to be rooted in this early American movement, and the distillation of formal public comment in official proceedings in the United States is a direct application of this format in the workings of public administration itself.",0
Business Administration,Maria Ramberger,"Maria Ramberger is a 2-time Olympic Snowboarder and Engagement Manager at McKinsey & Company where she served a broad range of tech clients across Hardware, Software and Tech Services. Born in Vienna, Austria she initially joined as part of the German office in 2017 and later transferred to the United States in early 2019. Before joining McKinsey she was a member of the Austrian National Snowboard Team. Maria also holds a PhD in law and served in the Austrian Armed Forces (2007 - 2012).",0
Business Administration,Rapid Results,"Rapid Results is a structured process that mobilizes teams to achieve tangible results over a rapid time frame and accelerate organizational learning. Schaffer Consulting, a management consulting firm headquartered in Stamford, Connecticut, developed the Rapid Results approach based on their experience working with clients across industries to achieve breakthrough levels of performance. The approach and its practitioners have since been recognized in The New York Times, Harvard Business Review, and Foreign Policy, among other publications.",0
Business Administration,Research ethics consultation,"Analogous to clinical ethics consultation, Research Ethics Consultation (REC) describes a formal way for researchers to solicit and receive expert ethical guidance related to biomedical research. The first REC service was established at the National Institutes of Health (NIH) Clinical Center in 1997. Today, most REC services are found at academic institutions, and the majority of current services were originally launched in response to the 2006 NIH Clinical and Translational Science Award program, as applicants to that program were required to have procedures in place to address ethical concerns raised by their research.While still a young discipline with no explicit standards, individuals serving as research ethics consultants are expected to be familiar with research ethics and ethical analysis; knowledgeable about the applicable regulations, laws, and policies; and ideally also have some biomedical research experience and scientific expertise.REC is distinct from related services, such as those of Institutional Review Boards, in that it is typically available at any point during a study (planning, conducting, interpreting, or disseminating results), and can relate to any ethical question. While little is known about the range and distribution of topics put forth for REC, such services may be particularly important and useful for studies of known regulatory and ethical uncertainty (e.g. assessment of minimal risk in pediatric studies) and frontier research for which there is little if any regulation or expert consensus. The recommendations that result from the consultation are non-binding, meaning that the researcher may choose to follow the recommendation, or to pursue a different approach.",0
Business Administration,Research report,"A research report is a publication that reports on the findings of a research project or alternatively scientific observations on or about a subject.
Research reports are produced by many sectors including industry, education, government and non-government organizations and may be disseminated internally, or made public (i.e. published) however they are not usually available from booksellers or through standard commercial publishing channels. Research reports are also issued by governmental and international organizations, such as UNESCO.
There are various distribution models for research reports with the main ones being: public distribution for free or open access; limited distribution to clients and customers; or sold commercially. For example market research reports are often produced for sale by specialist market research companies, investment companies may provide research reports to clients while government agencies and civil society organizations such as UNESCO, the World Health Organization and many others often provide free access to organization research reports in the public interest or for a range of organization requirements and objectives.",0
Business Administration,Steuerberater,"Steuerberater (StB) is the professional license for tax advisors in Germany.The provision of tax advisory services is restricted and basically permissible for Steuerberater, Rechtsanwälte (attorneys-at-law) and Wirtschaftsprüfer (certified public accountants) only according to German law. In order to obtain this qualification, an individual must pass the Steuerberaterprüfung, a special uniform nationwide state examination. Merely being qualified as an attorney at law is not sufficient. Individuals may hold several of the aforementioned qualifications at the same time, e.g. be dual qualified and licensed as Rechtsanwalt (attorney-at-law) and Steuerberater (licensed tax advisor) at the same time. 
A similar license exists in Austria and Switzerland.

",0
Business Administration,Systel,"Systel, Inc. is a US-based multinational firm dealing in integrated technology and business services, mostly utilizing temporary H-1B visa workers. Headquartered in Alpharetta, its development centers are located in India and the United States. Systel is a certified minority business enterprise.

",0
Business Administration,List of university statistical consulting centers,This list of university statistical consulting centers (or centres)  is a simple list of universities in which there is a specifically designated team providing statistical consultancy services. Often this service will be available only to enquirers from within the same university.,0
Business Administration,Office,"An office is a space where an organization's employees perform administrative work in order to support and realize objects and goals of the organization. The word ""office"" may also denote a position within an organization with specific duties attached to it (see officer, office-holder, official); the latter is in fact an earlier usage, office as place originally referring to the location of one's duty. When used as an adjective, the term ""office"" may refer to business-related tasks. In law, a company or organization has offices in any place where it has an official presence, even if that presence consists of (for example) a storage silo rather than an establishment with desk-and-chair. An office is also an architectural and design phenomenon: ranging from a small office such as a bench in the corner of a small business of extremely small size (see small office/home office), through entire floors of buildings, up to and including massive buildings dedicated entirely to one company. In modern terms an office is usually the location where white-collar workers carry out their functions. According to James Stephenson, ""Office is that part of business enterprise which is devoted to the direction and co-ordination of its various activities.""
Offices in classical antiquity were often part of a palace complex or of a large temple. The High Middle Ages (1000–1300) saw the rise of the medieval chancery, which was usually the place where most government letters were written and where laws were copied in the administration of a kingdom. With the growth of large, complex organizations in the 18th century, the first purpose-built office spaces were constructed. As the Industrial Revolution intensified in the 18th and 19th centuries, the industries of banking, rail, insurance, retail, petroleum, and telegraphy grew dramatically, requiring many clerks, and as a result more office space was assigned to house their activities. The time-and-motion study, pioneered in manufacturing by F. W. Taylor (1856-1915) led to the ""Modern Efficiency Desk"" of 1915 with a flat top and drawers below, designed to allow managers an easy view of the workers.
However, by the middle of the 20th century, it became apparent that an efficient office required discretion in the control of privacy, and gradually the cubicle system evolved.The main purpose of an office environment is to support its occupants in performing their jobs. Work spaces in an office are typically used for conventional office activities such as reading, writing and computer work. There are nine generic types of work space, each supporting different activities. In addition to individual cubicles, one can find meeting rooms, lounges, and spaces for support activities, such as photocopying and filing. Some offices also have a kitchen area where workers can make their lunches. There are many different ways of arranging the space in an office and whilst these vary according to function, managerial fashions and the culture of specific companies can be even more important.
While offices can be built in almost any location and in almost any building, some modern requirements for offices make this more difficult, such as requirements for light, networking, and security. The major purpose of an office building is to provide a workplace and working environment - primarily for administrative and managerial workers. These workers usually occupy set areas within the office building, and usually are provided with desks, PCs and other equipment they may need within these areas. The chief operating officer (COO) is responsible for handling administration and maintenance of an office building.",0
Business Administration,Office administration,"Office administration (shortened as Office Ad and abbreviated as OA) is a set of day-to-day activities that are related to the maintenance of an office building, financial planning, record keeping and billing, personal development, physical distribution and logistics, within an organization. An employee that undertakes these activities is commonly called an office administrator or office manager, and plays a key role in any organization's infrastructure, regardless of the scale. Many administrative positions require the candidate to have an advanced skill set in the software applications Microsoft Word, Excel and Access.

",0
Business Administration,Activity management,"Project management is the process of leading the work of a team to achieve all project goals within the given constraints. This information is usually described in project documentation, created at the beginning of the development process. The primary constraints are scope, time, and budget. The secondary challenge is to optimize the allocation of necessary inputs and apply them to meet pre-defined objectives.
The objective of project management is to produce a complete project which complies with the client's objectives. In many cases, the objective of project management is also to shape or reform the client's brief to feasibly address the client's objectives. Once the client's objectives are clearly established they should influence all decisions made by other people involved in the project – for example, project managers, designers, contractors, and sub-contractors. Ill-defined or too tightly prescribed project management objectives are detrimental to decision making.
A project is a temporary and unique endeavor designed to produce a product, service, or result with a defined beginning and end (usually time-constrained, and often constrained by funding or staffing) undertaken to meet unique goals and objectives, typically to bring about beneficial change or added value.  The temporary nature of projects stands in contrast with business as usual (or operations), which are repetitive, permanent, or semi-permanent functional activities to produce products or services. In practice, the management of such distinct production approaches requires the development of distinct technical skills and management strategies.",0
Business Administration,Administrative assistant,A person responsible for providing various kinds of administrative assistance is called an administrative assistant (admin assistant) or sometimes an administrative support specialist.,0
Business Administration,Business workflow analysis,"Business Workflow Analysis (BWA), aka Business management systems p2p, is a management tool that streamlines, automates and improves the efficiency of business procedures.",0
Business Administration,Fish! Philosophy,"The Fish! Philosophy (styled FISH! Philosophy), modeled after the Pike Place Fish Market, is a business technique that is aimed at creating happy individuals in the workplace. John Christensen created this philosophy in 1998 to improve organizational culture. The central four ideas are: ""choose your attitude"", ""play"", ""make their day"" and the ""present moment"".",0
Business Administration,Hot desking,"Hot desking (sometimes called ""non-reservation-based hoteling"") is an office organization system that involves multiple workers using a single physical work station or surface during different time periods. The ""desk"" in the name refers to a table or other work space being shared by multiple workers on different shifts as opposed to every staff member having their own personal desk. A primary motivation for hot-desking is cost reduction through space savings—up to 30% in some cases. Hot desking is especially valuable in cities where real estate prices are high.",0
Business Administration,Hoteling,"Hoteling (also hotelling or office hoteling) is a method of office management in which workers dynamically schedule their use of workspaces such as desks, cubicles, and offices. It is an alternative approach to the more traditional method of permanently assigned seating. Hoteling is reservation-based unassigned seating; employees reserve a workspace before they come to work in an office. An alternate method of handling unassigned seating is hot desking, which does not involve reservations; with hot-desking, a worker chooses a workspace upon arrival, rather than reserving it in advance. The use of the term has declined in recent years.
With hoteling, workers are not assigned their own desks; instead, they reserve a desk for their temporary use for just the days they expect to work in the office. The benefits of hoteling over a more traditional, one-desk-per-employee scenario include saving costs on commercial real estate, as well as creating opportunities for staff to mingle and collaborate more.The practice of hoteling has resulted from increased worker mobility, enabled by advances in mobile technology. Organizations whose workers travel frequently, or with growing remote or mobile workforces, are best suited to hoteling. A Washington Post article cites the rising use of hoteling as reflecting a shift from the office being a ""home base"" to being a ""hospitality hub.""Companies started implementing hoteling in the 1990s, with consulting and accounting firms among the early adopters.",0
Business Administration,Interdepartmental communication,"Interdepartmental communication is largely a formal affair between different departments of an organization. Interdepartmental communication is effective when it is supported by good infrastructural facilities. There are various documents used in inter departmental communication, they are:
A memorandum is a note or record for future use. It is convenient and useful for informal communication. Most interdepartmental communication is done over phone, but when the information has to be communicated in writing then memorandums are used. Memos are also issued in the cases of disciplinary actions to be taken against employees. The format of a memo is almost the same.
Office circulars are used to convey the information to a large number of employees. It is used for internal communication, so it is brief and formal.
The format of office orders is similar to memorandum but the purpose for which it is issued will differ. It is usually issued in matters affecting rights and privileges of employees. Office orders carry a number since they will be in force until revoked.
Suggestions are given by employees. Sometimes they given by one department to another. It helps in developing new ideas and policies. But its effectiveness depends on the attitude of the management.
Complaints are a part of office routine. As the size of the organization increases, the number of complaints also increases. In many cases complaints may relate to lack of proper infrastructure, non-observance of rules, etc.",0
Business Administration,Office management,"Office management is a profession involving the design, implementation, evaluation, and maintenance of the process of work within an office or other organization, in order to sustain and improve efficiency and productivity.
Office management is thus a part of the overall administration of business and since the elements of management are forecasting and planning, organising, command, control and coordination, the office is a part of the total management function.
Office management is the technique of planning, organizing, coordinating and controlling office activities with a view to achieve business objectives and is concerned with efficient and effective performance of the office work. The success of a business depends upon the efficiency of its office. The volume of paper work in offices has increased manifold in these days due to industrialization, population explosion, government control and application of various tax and labour laws to any business enterprise. Efficiency and effectiveness which are key words in management are achieved only through proper planning and control of activities, reduction of office costs and coordination of all activities of business.
In simple words, office management can be defined as “a distinct process of planning, organizing, staffing, directing, coordinating and controlling office in order to facilitate achievement of objectives of any business enterprise’ the definition shows managerial functions of an administrative manager. Following diagram indicates various elements or functions in the process of office management.",0
